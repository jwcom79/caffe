I0408 13:17:01.306974 31727 caffe.cpp:184] Using GPUs 0
I0408 13:17:02.185763 31727 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mlp/mlp_test"
solver_mode: GPU
device_id: 0
net: "examples/mlp/mlp_train_test.prototxt"
I0408 13:17:02.186784 31727 solver.cpp:91] Creating training net from net file: examples/mlp/mlp_train_test.prototxt
I0408 13:17:02.187291 31727 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0408 13:17:02.187324 31727 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0408 13:17:02.187445 31727 net.cpp:49] Initializing net from parameters: 
name: "MLP"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "tanh1"
  type: "TanH"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip1-2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip1-2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "tanh5"
  type: "TanH"
  bottom: "ip1-2"
  top: "ip1-2"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1-2"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "tanh2"
  type: "TanH"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "tanh3"
  type: "TanH"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "ip4"
  type: "InnerProduct"
  bottom: "ip3"
  top: "ip4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip4"
  bottom: "label"
  top: "loss"
}
I0408 13:17:02.187557 31727 layer_factory.hpp:77] Creating layer mnist
I0408 13:17:02.214085 31727 net.cpp:106] Creating Layer mnist
I0408 13:17:02.214133 31727 net.cpp:411] mnist -> data
I0408 13:17:02.214190 31727 net.cpp:411] mnist -> label
I0408 13:17:02.235210 31734 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I0408 13:17:02.324555 31727 data_layer.cpp:41] output data size: 100,1,28,28
I0408 13:17:02.326793 31727 net.cpp:150] Setting up mnist
I0408 13:17:02.326844 31727 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0408 13:17:02.326851 31727 net.cpp:157] Top shape: 100 (100)
I0408 13:17:02.326855 31727 net.cpp:165] Memory required for data: 314000
I0408 13:17:02.326879 31727 layer_factory.hpp:77] Creating layer ip1
I0408 13:17:02.326913 31727 net.cpp:106] Creating Layer ip1
I0408 13:17:02.326933 31727 net.cpp:454] ip1 <- data
I0408 13:17:02.326959 31727 net.cpp:411] ip1 -> ip1
I0408 13:17:02.345713 31727 net.cpp:150] Setting up ip1
I0408 13:17:02.345775 31727 net.cpp:157] Top shape: 100 1024 (102400)
I0408 13:17:02.345793 31727 net.cpp:165] Memory required for data: 723600
I0408 13:17:02.345827 31727 layer_factory.hpp:77] Creating layer tanh1
I0408 13:17:02.345856 31727 net.cpp:106] Creating Layer tanh1
I0408 13:17:02.345875 31727 net.cpp:454] tanh1 <- ip1
I0408 13:17:02.345896 31727 net.cpp:397] tanh1 -> ip1 (in-place)
I0408 13:17:03.784442 31727 net.cpp:150] Setting up tanh1
I0408 13:17:03.784495 31727 net.cpp:157] Top shape: 100 1024 (102400)
I0408 13:17:03.784543 31727 net.cpp:165] Memory required for data: 1133200
I0408 13:17:03.784556 31727 layer_factory.hpp:77] Creating layer ip1-2
I0408 13:17:03.784590 31727 net.cpp:106] Creating Layer ip1-2
I0408 13:17:03.784606 31727 net.cpp:454] ip1-2 <- ip1
I0408 13:17:03.784632 31727 net.cpp:411] ip1-2 -> ip1-2
I0408 13:17:03.785748 31727 net.cpp:150] Setting up ip1-2
I0408 13:17:03.785779 31727 net.cpp:157] Top shape: 100 10 (1000)
I0408 13:17:03.785792 31727 net.cpp:165] Memory required for data: 1137200
I0408 13:17:03.785823 31727 layer_factory.hpp:77] Creating layer tanh5
I0408 13:17:03.785851 31727 net.cpp:106] Creating Layer tanh5
I0408 13:17:03.785869 31727 net.cpp:454] tanh5 <- ip1-2
I0408 13:17:03.785887 31727 net.cpp:397] tanh5 -> ip1-2 (in-place)
I0408 13:17:03.787051 31727 net.cpp:150] Setting up tanh5
I0408 13:17:03.787082 31727 net.cpp:157] Top shape: 100 10 (1000)
I0408 13:17:03.787094 31727 net.cpp:165] Memory required for data: 1141200
I0408 13:17:03.787107 31727 layer_factory.hpp:77] Creating layer ip2
I0408 13:17:03.787132 31727 net.cpp:106] Creating Layer ip2
I0408 13:17:03.787140 31727 net.cpp:454] ip2 <- ip1-2
I0408 13:17:03.787152 31727 net.cpp:411] ip2 -> ip2
I0408 13:17:03.787366 31727 net.cpp:150] Setting up ip2
I0408 13:17:03.787384 31727 net.cpp:157] Top shape: 100 512 (51200)
I0408 13:17:03.787390 31727 net.cpp:165] Memory required for data: 1346000
I0408 13:17:03.787405 31727 layer_factory.hpp:77] Creating layer tanh2
I0408 13:17:03.787418 31727 net.cpp:106] Creating Layer tanh2
I0408 13:17:03.787425 31727 net.cpp:454] tanh2 <- ip2
I0408 13:17:03.787441 31727 net.cpp:397] tanh2 -> ip2 (in-place)
I0408 13:17:03.788455 31727 net.cpp:150] Setting up tanh2
I0408 13:17:03.788486 31727 net.cpp:157] Top shape: 100 512 (51200)
I0408 13:17:03.788498 31727 net.cpp:165] Memory required for data: 1550800
I0408 13:17:03.788511 31727 layer_factory.hpp:77] Creating layer ip3
I0408 13:17:03.788533 31727 net.cpp:106] Creating Layer ip3
I0408 13:17:03.788542 31727 net.cpp:454] ip3 <- ip2
I0408 13:17:03.788553 31727 net.cpp:411] ip3 -> ip3
I0408 13:17:03.790956 31727 net.cpp:150] Setting up ip3
I0408 13:17:03.790982 31727 net.cpp:157] Top shape: 100 256 (25600)
I0408 13:17:03.790989 31727 net.cpp:165] Memory required for data: 1653200
I0408 13:17:03.791002 31727 layer_factory.hpp:77] Creating layer tanh3
I0408 13:17:03.791016 31727 net.cpp:106] Creating Layer tanh3
I0408 13:17:03.791023 31727 net.cpp:454] tanh3 <- ip3
I0408 13:17:03.791033 31727 net.cpp:397] tanh3 -> ip3 (in-place)
I0408 13:17:03.792075 31727 net.cpp:150] Setting up tanh3
I0408 13:17:03.792101 31727 net.cpp:157] Top shape: 100 256 (25600)
I0408 13:17:03.792109 31727 net.cpp:165] Memory required for data: 1755600
I0408 13:17:03.792115 31727 layer_factory.hpp:77] Creating layer ip4
I0408 13:17:03.792127 31727 net.cpp:106] Creating Layer ip4
I0408 13:17:03.792135 31727 net.cpp:454] ip4 <- ip3
I0408 13:17:03.792145 31727 net.cpp:411] ip4 -> ip4
I0408 13:17:03.793099 31727 net.cpp:150] Setting up ip4
I0408 13:17:03.793126 31727 net.cpp:157] Top shape: 100 10 (1000)
I0408 13:17:03.793133 31727 net.cpp:165] Memory required for data: 1759600
I0408 13:17:03.793150 31727 layer_factory.hpp:77] Creating layer loss
I0408 13:17:03.793167 31727 net.cpp:106] Creating Layer loss
I0408 13:17:03.793174 31727 net.cpp:454] loss <- ip4
I0408 13:17:03.793184 31727 net.cpp:454] loss <- label
I0408 13:17:03.793196 31727 net.cpp:411] loss -> loss
I0408 13:17:03.793218 31727 layer_factory.hpp:77] Creating layer loss
I0408 13:17:03.795330 31727 net.cpp:150] Setting up loss
I0408 13:17:03.795358 31727 net.cpp:157] Top shape: (1)
I0408 13:17:03.795366 31727 net.cpp:160]     with loss weight 1
I0408 13:17:03.795390 31727 net.cpp:165] Memory required for data: 1759604
I0408 13:17:03.795405 31727 net.cpp:226] loss needs backward computation.
I0408 13:17:03.795413 31727 net.cpp:226] ip4 needs backward computation.
I0408 13:17:03.795419 31727 net.cpp:226] tanh3 needs backward computation.
I0408 13:17:03.795425 31727 net.cpp:226] ip3 needs backward computation.
I0408 13:17:03.795465 31727 net.cpp:226] tanh2 needs backward computation.
I0408 13:17:03.795478 31727 net.cpp:226] ip2 needs backward computation.
I0408 13:17:03.795485 31727 net.cpp:226] tanh5 needs backward computation.
I0408 13:17:03.795490 31727 net.cpp:226] ip1-2 needs backward computation.
I0408 13:17:03.795496 31727 net.cpp:226] tanh1 needs backward computation.
I0408 13:17:03.795502 31727 net.cpp:226] ip1 needs backward computation.
I0408 13:17:03.795509 31727 net.cpp:228] mnist does not need backward computation.
I0408 13:17:03.795514 31727 net.cpp:270] This network produces output loss
I0408 13:17:03.795533 31727 net.cpp:283] Network initialization done.
I0408 13:17:03.796115 31727 solver.cpp:181] Creating test net (#0) specified by net file: examples/mlp/mlp_train_test.prototxt
I0408 13:17:03.796169 31727 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0408 13:17:03.796315 31727 net.cpp:49] Initializing net from parameters: 
name: "MLP"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "tanh1"
  type: "TanH"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip1-2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip1-2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "tanh5"
  type: "TanH"
  bottom: "ip1-2"
  top: "ip1-2"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1-2"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "tanh2"
  type: "TanH"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "tanh3"
  type: "TanH"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "ip4"
  type: "InnerProduct"
  bottom: "ip3"
  top: "ip4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip4"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip4"
  bottom: "label"
  top: "loss"
}
I0408 13:17:03.796463 31727 layer_factory.hpp:77] Creating layer mnist
I0408 13:17:03.796653 31727 net.cpp:106] Creating Layer mnist
I0408 13:17:03.796700 31727 net.cpp:411] mnist -> data
I0408 13:17:03.796721 31727 net.cpp:411] mnist -> label
I0408 13:17:03.828440 31736 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I0408 13:17:03.893733 31727 data_layer.cpp:41] output data size: 100,1,28,28
I0408 13:17:03.895623 31727 net.cpp:150] Setting up mnist
I0408 13:17:03.895659 31727 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0408 13:17:03.895673 31727 net.cpp:157] Top shape: 100 (100)
I0408 13:17:03.895681 31727 net.cpp:165] Memory required for data: 314000
I0408 13:17:03.895690 31727 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0408 13:17:03.895709 31727 net.cpp:106] Creating Layer label_mnist_1_split
I0408 13:17:03.895722 31727 net.cpp:454] label_mnist_1_split <- label
I0408 13:17:03.895738 31727 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I0408 13:17:03.895757 31727 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I0408 13:17:03.895970 31727 net.cpp:150] Setting up label_mnist_1_split
I0408 13:17:03.896034 31727 net.cpp:157] Top shape: 100 (100)
I0408 13:17:03.896052 31727 net.cpp:157] Top shape: 100 (100)
I0408 13:17:03.896060 31727 net.cpp:165] Memory required for data: 314800
I0408 13:17:03.896069 31727 layer_factory.hpp:77] Creating layer ip1
I0408 13:17:03.896088 31727 net.cpp:106] Creating Layer ip1
I0408 13:17:03.896096 31727 net.cpp:454] ip1 <- data
I0408 13:17:03.896112 31727 net.cpp:411] ip1 -> ip1
I0408 13:17:03.898809 31737 blocking_queue.cpp:50] Waiting for data
I0408 13:17:03.911057 31727 net.cpp:150] Setting up ip1
I0408 13:17:03.911090 31727 net.cpp:157] Top shape: 100 1024 (102400)
I0408 13:17:03.911098 31727 net.cpp:165] Memory required for data: 724400
I0408 13:17:03.911123 31727 layer_factory.hpp:77] Creating layer tanh1
I0408 13:17:03.911141 31727 net.cpp:106] Creating Layer tanh1
I0408 13:17:03.911156 31727 net.cpp:454] tanh1 <- ip1
I0408 13:17:03.911169 31727 net.cpp:397] tanh1 -> ip1 (in-place)
I0408 13:17:03.912657 31727 net.cpp:150] Setting up tanh1
I0408 13:17:03.912690 31727 net.cpp:157] Top shape: 100 1024 (102400)
I0408 13:17:03.912699 31727 net.cpp:165] Memory required for data: 1134000
I0408 13:17:03.912708 31727 layer_factory.hpp:77] Creating layer ip1-2
I0408 13:17:03.912726 31727 net.cpp:106] Creating Layer ip1-2
I0408 13:17:03.912736 31727 net.cpp:454] ip1-2 <- ip1
I0408 13:17:03.912755 31727 net.cpp:411] ip1-2 -> ip1-2
I0408 13:17:03.913193 31727 net.cpp:150] Setting up ip1-2
I0408 13:17:03.913215 31727 net.cpp:157] Top shape: 100 10 (1000)
I0408 13:17:03.913223 31727 net.cpp:165] Memory required for data: 1138000
I0408 13:17:03.913242 31727 layer_factory.hpp:77] Creating layer tanh5
I0408 13:17:03.913255 31727 net.cpp:106] Creating Layer tanh5
I0408 13:17:03.913264 31727 net.cpp:454] tanh5 <- ip1-2
I0408 13:17:03.913275 31727 net.cpp:397] tanh5 -> ip1-2 (in-place)
I0408 13:17:03.914860 31727 net.cpp:150] Setting up tanh5
I0408 13:17:03.914897 31727 net.cpp:157] Top shape: 100 10 (1000)
I0408 13:17:03.914907 31727 net.cpp:165] Memory required for data: 1142000
I0408 13:17:03.914916 31727 layer_factory.hpp:77] Creating layer ip2
I0408 13:17:03.914932 31727 net.cpp:106] Creating Layer ip2
I0408 13:17:03.914940 31727 net.cpp:454] ip2 <- ip1-2
I0408 13:17:03.914957 31727 net.cpp:411] ip2 -> ip2
I0408 13:17:03.915267 31727 net.cpp:150] Setting up ip2
I0408 13:17:03.915287 31727 net.cpp:157] Top shape: 100 512 (51200)
I0408 13:17:03.915294 31727 net.cpp:165] Memory required for data: 1346800
I0408 13:17:03.915314 31727 layer_factory.hpp:77] Creating layer tanh2
I0408 13:17:03.915336 31727 net.cpp:106] Creating Layer tanh2
I0408 13:17:03.915346 31727 net.cpp:454] tanh2 <- ip2
I0408 13:17:03.915357 31727 net.cpp:397] tanh2 -> ip2 (in-place)
I0408 13:17:03.917034 31727 net.cpp:150] Setting up tanh2
I0408 13:17:03.917068 31727 net.cpp:157] Top shape: 100 512 (51200)
I0408 13:17:03.917076 31727 net.cpp:165] Memory required for data: 1551600
I0408 13:17:03.917085 31727 layer_factory.hpp:77] Creating layer ip3
I0408 13:17:03.917107 31727 net.cpp:106] Creating Layer ip3
I0408 13:17:03.917124 31727 net.cpp:454] ip3 <- ip2
I0408 13:17:03.917152 31727 net.cpp:411] ip3 -> ip3
I0408 13:17:03.920143 31727 net.cpp:150] Setting up ip3
I0408 13:17:03.920173 31727 net.cpp:157] Top shape: 100 256 (25600)
I0408 13:17:03.920181 31727 net.cpp:165] Memory required for data: 1654000
I0408 13:17:03.920197 31727 layer_factory.hpp:77] Creating layer tanh3
I0408 13:17:03.920213 31727 net.cpp:106] Creating Layer tanh3
I0408 13:17:03.920225 31727 net.cpp:454] tanh3 <- ip3
I0408 13:17:03.920240 31727 net.cpp:397] tanh3 -> ip3 (in-place)
I0408 13:17:03.921816 31727 net.cpp:150] Setting up tanh3
I0408 13:17:03.921847 31727 net.cpp:157] Top shape: 100 256 (25600)
I0408 13:17:03.921859 31727 net.cpp:165] Memory required for data: 1756400
I0408 13:17:03.921869 31727 layer_factory.hpp:77] Creating layer ip4
I0408 13:17:03.921885 31727 net.cpp:106] Creating Layer ip4
I0408 13:17:03.921896 31727 net.cpp:454] ip4 <- ip3
I0408 13:17:03.921908 31727 net.cpp:411] ip4 -> ip4
I0408 13:17:03.922169 31727 net.cpp:150] Setting up ip4
I0408 13:17:03.922212 31727 net.cpp:157] Top shape: 100 10 (1000)
I0408 13:17:03.922224 31727 net.cpp:165] Memory required for data: 1760400
I0408 13:17:03.922243 31727 layer_factory.hpp:77] Creating layer ip4_ip4_0_split
I0408 13:17:03.922260 31727 net.cpp:106] Creating Layer ip4_ip4_0_split
I0408 13:17:03.922269 31727 net.cpp:454] ip4_ip4_0_split <- ip4
I0408 13:17:03.922284 31727 net.cpp:411] ip4_ip4_0_split -> ip4_ip4_0_split_0
I0408 13:17:03.922300 31727 net.cpp:411] ip4_ip4_0_split -> ip4_ip4_0_split_1
I0408 13:17:03.922379 31727 net.cpp:150] Setting up ip4_ip4_0_split
I0408 13:17:03.922394 31727 net.cpp:157] Top shape: 100 10 (1000)
I0408 13:17:03.922402 31727 net.cpp:157] Top shape: 100 10 (1000)
I0408 13:17:03.922408 31727 net.cpp:165] Memory required for data: 1768400
I0408 13:17:03.922415 31727 layer_factory.hpp:77] Creating layer accuracy
I0408 13:17:03.922431 31727 net.cpp:106] Creating Layer accuracy
I0408 13:17:03.922441 31727 net.cpp:454] accuracy <- ip4_ip4_0_split_0
I0408 13:17:03.922451 31727 net.cpp:454] accuracy <- label_mnist_1_split_0
I0408 13:17:03.922463 31727 net.cpp:411] accuracy -> accuracy
I0408 13:17:03.922482 31727 net.cpp:150] Setting up accuracy
I0408 13:17:03.922493 31727 net.cpp:157] Top shape: (1)
I0408 13:17:03.922499 31727 net.cpp:165] Memory required for data: 1768404
I0408 13:17:03.922507 31727 layer_factory.hpp:77] Creating layer loss
I0408 13:17:03.922516 31727 net.cpp:106] Creating Layer loss
I0408 13:17:03.922524 31727 net.cpp:454] loss <- ip4_ip4_0_split_1
I0408 13:17:03.922533 31727 net.cpp:454] loss <- label_mnist_1_split_1
I0408 13:17:03.922543 31727 net.cpp:411] loss -> loss
I0408 13:17:03.922557 31727 layer_factory.hpp:77] Creating layer loss
I0408 13:17:03.924075 31727 net.cpp:150] Setting up loss
I0408 13:17:03.924106 31727 net.cpp:157] Top shape: (1)
I0408 13:17:03.924114 31727 net.cpp:160]     with loss weight 1
I0408 13:17:03.924130 31727 net.cpp:165] Memory required for data: 1768408
I0408 13:17:03.924139 31727 net.cpp:226] loss needs backward computation.
I0408 13:17:03.924147 31727 net.cpp:228] accuracy does not need backward computation.
I0408 13:17:03.924155 31727 net.cpp:226] ip4_ip4_0_split needs backward computation.
I0408 13:17:03.924162 31727 net.cpp:226] ip4 needs backward computation.
I0408 13:17:03.924168 31727 net.cpp:226] tanh3 needs backward computation.
I0408 13:17:03.924175 31727 net.cpp:226] ip3 needs backward computation.
I0408 13:17:03.924181 31727 net.cpp:226] tanh2 needs backward computation.
I0408 13:17:03.924187 31727 net.cpp:226] ip2 needs backward computation.
I0408 13:17:03.924195 31727 net.cpp:226] tanh5 needs backward computation.
I0408 13:17:03.924201 31727 net.cpp:226] ip1-2 needs backward computation.
I0408 13:17:03.924207 31727 net.cpp:226] tanh1 needs backward computation.
I0408 13:17:03.924213 31727 net.cpp:226] ip1 needs backward computation.
I0408 13:17:03.924221 31727 net.cpp:228] label_mnist_1_split does not need backward computation.
I0408 13:17:03.924229 31727 net.cpp:228] mnist does not need backward computation.
I0408 13:17:03.924235 31727 net.cpp:270] This network produces output accuracy
I0408 13:17:03.924243 31727 net.cpp:270] This network produces output loss
I0408 13:17:03.924268 31727 net.cpp:283] Network initialization done.
I0408 13:17:03.924378 31727 solver.cpp:60] Solver scaffolding done.
I0408 13:17:03.925078 31727 caffe.cpp:212] Starting Optimization
I0408 13:17:03.925098 31727 solver.cpp:288] Solving MLP
I0408 13:17:03.925106 31727 solver.cpp:289] Learning Rate Policy: inv
I0408 13:17:03.926221 31727 solver.cpp:341] Iteration 0, Testing net (#0)
I0408 13:17:03.932129 31727 blocking_queue.cpp:50] Data layer prefetch queue empty
I0408 13:17:05.334161 31727 solver.cpp:409]     Test net output #0: accuracy = 0.0967
I0408 13:17:05.334224 31727 solver.cpp:409]     Test net output #1: loss = 2.32664 (* 1 = 2.32664 loss)
I0408 13:17:05.336776 31727 solver.cpp:237] Iteration 0, loss = 2.31966
I0408 13:17:05.336843 31727 solver.cpp:253]     Train net output #0: loss = 2.31966 (* 1 = 2.31966 loss)
I0408 13:17:05.336964 31727 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0408 13:17:07.337175 31727 solver.cpp:237] Iteration 100, loss = 0.502789
I0408 13:17:07.337276 31727 solver.cpp:253]     Train net output #0: loss = 0.502789 (* 1 = 0.502789 loss)
I0408 13:17:07.337291 31727 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0408 13:17:07.504338 31727 solver.cpp:237] Iteration 200, loss = 0.498831
I0408 13:17:07.504374 31727 solver.cpp:253]     Train net output #0: loss = 0.498831 (* 1 = 0.498831 loss)
I0408 13:17:07.504384 31727 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0408 13:17:07.655233 31727 solver.cpp:237] Iteration 300, loss = 0.394643
I0408 13:17:07.655272 31727 solver.cpp:253]     Train net output #0: loss = 0.394643 (* 1 = 0.394643 loss)
I0408 13:17:07.655280 31727 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0408 13:17:07.805976 31727 solver.cpp:237] Iteration 400, loss = 0.411568
I0408 13:17:07.806016 31727 solver.cpp:253]     Train net output #0: loss = 0.411568 (* 1 = 0.411568 loss)
I0408 13:17:07.806025 31727 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0408 13:17:07.956035 31727 solver.cpp:341] Iteration 500, Testing net (#0)
I0408 13:17:08.043401 31727 solver.cpp:409]     Test net output #0: accuracy = 0.9157
I0408 13:17:08.043458 31727 solver.cpp:409]     Test net output #1: loss = 0.27959 (* 1 = 0.27959 loss)
I0408 13:17:08.044443 31727 solver.cpp:237] Iteration 500, loss = 0.31925
I0408 13:17:08.044538 31727 solver.cpp:253]     Train net output #0: loss = 0.31925 (* 1 = 0.31925 loss)
I0408 13:17:08.044556 31727 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I0408 13:17:08.195210 31727 solver.cpp:237] Iteration 600, loss = 0.220876
I0408 13:17:08.195257 31727 solver.cpp:253]     Train net output #0: loss = 0.220876 (* 1 = 0.220876 loss)
I0408 13:17:08.195272 31727 sgd_solver.cpp:106] Iteration 600, lr = 0.0095724
I0408 13:17:08.346536 31727 solver.cpp:237] Iteration 700, loss = 0.343919
I0408 13:17:08.346585 31727 solver.cpp:253]     Train net output #0: loss = 0.343918 (* 1 = 0.343918 loss)
I0408 13:17:08.346598 31727 sgd_solver.cpp:106] Iteration 700, lr = 0.00950522
I0408 13:17:08.497932 31727 solver.cpp:237] Iteration 800, loss = 0.385803
I0408 13:17:08.497964 31727 solver.cpp:253]     Train net output #0: loss = 0.385803 (* 1 = 0.385803 loss)
I0408 13:17:08.497970 31727 sgd_solver.cpp:106] Iteration 800, lr = 0.00943913
I0408 13:17:08.649335 31727 solver.cpp:237] Iteration 900, loss = 0.31515
I0408 13:17:08.649369 31727 solver.cpp:253]     Train net output #0: loss = 0.31515 (* 1 = 0.31515 loss)
I0408 13:17:08.649376 31727 sgd_solver.cpp:106] Iteration 900, lr = 0.00937411
I0408 13:17:08.799430 31727 solver.cpp:341] Iteration 1000, Testing net (#0)
I0408 13:17:08.868774 31727 solver.cpp:409]     Test net output #0: accuracy = 0.9218
I0408 13:17:08.868813 31727 solver.cpp:409]     Test net output #1: loss = 0.266621 (* 1 = 0.266621 loss)
I0408 13:17:08.869626 31727 solver.cpp:237] Iteration 1000, loss = 0.327043
I0408 13:17:08.869657 31727 solver.cpp:253]     Train net output #0: loss = 0.327043 (* 1 = 0.327043 loss)
I0408 13:17:08.869668 31727 sgd_solver.cpp:106] Iteration 1000, lr = 0.00931012
I0408 13:17:09.020064 31727 solver.cpp:237] Iteration 1100, loss = 0.304397
I0408 13:17:09.020105 31727 solver.cpp:253]     Train net output #0: loss = 0.304397 (* 1 = 0.304397 loss)
I0408 13:17:09.020117 31727 sgd_solver.cpp:106] Iteration 1100, lr = 0.00924715
I0408 13:17:09.171131 31727 solver.cpp:237] Iteration 1200, loss = 0.187829
I0408 13:17:09.171170 31727 solver.cpp:253]     Train net output #0: loss = 0.187829 (* 1 = 0.187829 loss)
I0408 13:17:09.171181 31727 sgd_solver.cpp:106] Iteration 1200, lr = 0.00918515
I0408 13:17:09.322157 31727 solver.cpp:237] Iteration 1300, loss = 0.250893
I0408 13:17:09.322198 31727 solver.cpp:253]     Train net output #0: loss = 0.250893 (* 1 = 0.250893 loss)
I0408 13:17:09.322211 31727 sgd_solver.cpp:106] Iteration 1300, lr = 0.00912412
I0408 13:17:09.472743 31727 solver.cpp:237] Iteration 1400, loss = 0.329842
I0408 13:17:09.472784 31727 solver.cpp:253]     Train net output #0: loss = 0.329842 (* 1 = 0.329842 loss)
I0408 13:17:09.472828 31727 sgd_solver.cpp:106] Iteration 1400, lr = 0.00906403
I0408 13:17:09.621817 31727 solver.cpp:341] Iteration 1500, Testing net (#0)
I0408 13:17:09.692713 31727 solver.cpp:409]     Test net output #0: accuracy = 0.9365
I0408 13:17:09.692754 31727 solver.cpp:409]     Test net output #1: loss = 0.21393 (* 1 = 0.21393 loss)
I0408 13:17:09.693585 31727 solver.cpp:237] Iteration 1500, loss = 0.280845
I0408 13:17:09.693617 31727 solver.cpp:253]     Train net output #0: loss = 0.280845 (* 1 = 0.280845 loss)
I0408 13:17:09.693629 31727 sgd_solver.cpp:106] Iteration 1500, lr = 0.00900485
I0408 13:17:09.844066 31727 solver.cpp:237] Iteration 1600, loss = 0.265785
I0408 13:17:09.844161 31727 solver.cpp:253]     Train net output #0: loss = 0.265785 (* 1 = 0.265785 loss)
I0408 13:17:09.844187 31727 sgd_solver.cpp:106] Iteration 1600, lr = 0.00894657
I0408 13:17:09.994521 31727 solver.cpp:237] Iteration 1700, loss = 0.265177
I0408 13:17:09.994555 31727 solver.cpp:253]     Train net output #0: loss = 0.265177 (* 1 = 0.265177 loss)
I0408 13:17:09.994562 31727 sgd_solver.cpp:106] Iteration 1700, lr = 0.00888916
I0408 13:17:10.144526 31727 solver.cpp:237] Iteration 1800, loss = 0.168534
I0408 13:17:10.144564 31727 solver.cpp:253]     Train net output #0: loss = 0.168534 (* 1 = 0.168534 loss)
I0408 13:17:10.144572 31727 sgd_solver.cpp:106] Iteration 1800, lr = 0.0088326
I0408 13:17:10.294885 31727 solver.cpp:237] Iteration 1900, loss = 0.19248
I0408 13:17:10.294926 31727 solver.cpp:253]     Train net output #0: loss = 0.19248 (* 1 = 0.19248 loss)
I0408 13:17:10.294934 31727 sgd_solver.cpp:106] Iteration 1900, lr = 0.00877687
I0408 13:17:10.443883 31727 solver.cpp:341] Iteration 2000, Testing net (#0)
I0408 13:17:10.515846 31727 solver.cpp:409]     Test net output #0: accuracy = 0.9381
I0408 13:17:10.515939 31727 solver.cpp:409]     Test net output #1: loss = 0.209922 (* 1 = 0.209922 loss)
I0408 13:17:10.516862 31727 solver.cpp:237] Iteration 2000, loss = 0.281611
I0408 13:17:10.516911 31727 solver.cpp:253]     Train net output #0: loss = 0.281611 (* 1 = 0.281611 loss)
I0408 13:17:10.516954 31727 sgd_solver.cpp:106] Iteration 2000, lr = 0.00872196
I0408 13:17:10.671373 31727 solver.cpp:237] Iteration 2100, loss = 0.245414
I0408 13:17:10.671406 31727 solver.cpp:253]     Train net output #0: loss = 0.245414 (* 1 = 0.245414 loss)
I0408 13:17:10.671412 31727 sgd_solver.cpp:106] Iteration 2100, lr = 0.00866784
I0408 13:17:10.819221 31727 solver.cpp:237] Iteration 2200, loss = 0.216665
I0408 13:17:10.819247 31727 solver.cpp:253]     Train net output #0: loss = 0.216665 (* 1 = 0.216665 loss)
I0408 13:17:10.819253 31727 sgd_solver.cpp:106] Iteration 2200, lr = 0.0086145
I0408 13:17:10.966557 31727 solver.cpp:237] Iteration 2300, loss = 0.229407
I0408 13:17:10.966581 31727 solver.cpp:253]     Train net output #0: loss = 0.229408 (* 1 = 0.229408 loss)
I0408 13:17:10.966588 31727 sgd_solver.cpp:106] Iteration 2300, lr = 0.00856192
I0408 13:17:11.112107 31727 solver.cpp:237] Iteration 2400, loss = 0.155722
I0408 13:17:11.112130 31727 solver.cpp:253]     Train net output #0: loss = 0.155722 (* 1 = 0.155722 loss)
I0408 13:17:11.112136 31727 sgd_solver.cpp:106] Iteration 2400, lr = 0.00851008
I0408 13:17:11.257175 31727 solver.cpp:341] Iteration 2500, Testing net (#0)
I0408 13:17:11.341961 31727 solver.cpp:409]     Test net output #0: accuracy = 0.9366
I0408 13:17:11.341989 31727 solver.cpp:409]     Test net output #1: loss = 0.209493 (* 1 = 0.209493 loss)
I0408 13:17:11.342743 31727 solver.cpp:237] Iteration 2500, loss = 0.171906
I0408 13:17:11.342763 31727 solver.cpp:253]     Train net output #0: loss = 0.171906 (* 1 = 0.171906 loss)
I0408 13:17:11.342785 31727 sgd_solver.cpp:106] Iteration 2500, lr = 0.00845897
I0408 13:17:11.489879 31727 solver.cpp:237] Iteration 2600, loss = 0.247453
I0408 13:17:11.489907 31727 solver.cpp:253]     Train net output #0: loss = 0.247453 (* 1 = 0.247453 loss)
I0408 13:17:11.489917 31727 sgd_solver.cpp:106] Iteration 2600, lr = 0.00840857
I0408 13:17:11.642215 31727 solver.cpp:237] Iteration 2700, loss = 0.221127
I0408 13:17:11.642256 31727 solver.cpp:253]     Train net output #0: loss = 0.221127 (* 1 = 0.221127 loss)
I0408 13:17:11.642264 31727 sgd_solver.cpp:106] Iteration 2700, lr = 0.00835886
I0408 13:17:11.792243 31727 solver.cpp:237] Iteration 2800, loss = 0.188795
I0408 13:17:11.792275 31727 solver.cpp:253]     Train net output #0: loss = 0.188796 (* 1 = 0.188796 loss)
I0408 13:17:11.792282 31727 sgd_solver.cpp:106] Iteration 2800, lr = 0.00830984
I0408 13:17:11.940788 31727 solver.cpp:237] Iteration 2900, loss = 0.197867
I0408 13:17:11.940815 31727 solver.cpp:253]     Train net output #0: loss = 0.197867 (* 1 = 0.197867 loss)
I0408 13:17:11.940821 31727 sgd_solver.cpp:106] Iteration 2900, lr = 0.00826148
I0408 13:17:12.087416 31727 solver.cpp:341] Iteration 3000, Testing net (#0)
I0408 13:17:12.155359 31727 solver.cpp:409]     Test net output #0: accuracy = 0.9447
I0408 13:17:12.155385 31727 solver.cpp:409]     Test net output #1: loss = 0.180724 (* 1 = 0.180724 loss)
I0408 13:17:12.156143 31727 solver.cpp:237] Iteration 3000, loss = 0.14722
I0408 13:17:12.156158 31727 solver.cpp:253]     Train net output #0: loss = 0.14722 (* 1 = 0.14722 loss)
I0408 13:17:12.156165 31727 sgd_solver.cpp:106] Iteration 3000, lr = 0.00821377
I0408 13:17:12.741494 31727 solver.cpp:237] Iteration 3100, loss = 0.160075
I0408 13:17:12.741534 31727 solver.cpp:253]     Train net output #0: loss = 0.160075 (* 1 = 0.160075 loss)
I0408 13:17:12.741544 31727 sgd_solver.cpp:106] Iteration 3100, lr = 0.0081667
I0408 13:17:13.518230 31727 solver.cpp:237] Iteration 3200, loss = 0.224503
I0408 13:17:13.518322 31727 solver.cpp:253]     Train net output #0: loss = 0.224503 (* 1 = 0.224503 loss)
I0408 13:17:13.518334 31727 sgd_solver.cpp:106] Iteration 3200, lr = 0.00812025
I0408 13:17:13.672771 31727 solver.cpp:237] Iteration 3300, loss = 0.205619
I0408 13:17:13.672808 31727 solver.cpp:253]     Train net output #0: loss = 0.205619 (* 1 = 0.205619 loss)
I0408 13:17:13.672817 31727 sgd_solver.cpp:106] Iteration 3300, lr = 0.00807442
I0408 13:17:13.826169 31727 solver.cpp:237] Iteration 3400, loss = 0.170587
I0408 13:17:13.826205 31727 solver.cpp:253]     Train net output #0: loss = 0.170588 (* 1 = 0.170588 loss)
I0408 13:17:13.826215 31727 sgd_solver.cpp:106] Iteration 3400, lr = 0.00802918
I0408 13:17:13.978705 31727 solver.cpp:341] Iteration 3500, Testing net (#0)
I0408 13:17:14.091579 31727 solver.cpp:409]     Test net output #0: accuracy = 0.9451
I0408 13:17:14.091663 31727 solver.cpp:409]     Test net output #1: loss = 0.179343 (* 1 = 0.179343 loss)
I0408 13:17:14.092530 31727 solver.cpp:237] Iteration 3500, loss = 0.161429
I0408 13:17:14.092555 31727 solver.cpp:253]     Train net output #0: loss = 0.161429 (* 1 = 0.161429 loss)
I0408 13:17:14.093080 31727 sgd_solver.cpp:106] Iteration 3500, lr = 0.00798454
I0408 13:17:14.250488 31727 solver.cpp:237] Iteration 3600, loss = 0.14039
I0408 13:17:14.250533 31727 solver.cpp:253]     Train net output #0: loss = 0.14039 (* 1 = 0.14039 loss)
I0408 13:17:14.250541 31727 sgd_solver.cpp:106] Iteration 3600, lr = 0.00794046
I0408 13:17:14.407686 31727 solver.cpp:237] Iteration 3700, loss = 0.147227
I0408 13:17:14.407718 31727 solver.cpp:253]     Train net output #0: loss = 0.147227 (* 1 = 0.147227 loss)
I0408 13:17:14.407724 31727 sgd_solver.cpp:106] Iteration 3700, lr = 0.00789695
I0408 13:17:14.566185 31727 solver.cpp:237] Iteration 3800, loss = 0.210543
I0408 13:17:14.566220 31727 solver.cpp:253]     Train net output #0: loss = 0.210543 (* 1 = 0.210543 loss)
I0408 13:17:14.566228 31727 sgd_solver.cpp:106] Iteration 3800, lr = 0.007854
I0408 13:17:14.723574 31727 solver.cpp:237] Iteration 3900, loss = 0.192489
I0408 13:17:14.723611 31727 solver.cpp:253]     Train net output #0: loss = 0.192489 (* 1 = 0.192489 loss)
I0408 13:17:14.723623 31727 sgd_solver.cpp:106] Iteration 3900, lr = 0.00781158
I0408 13:17:14.880918 31727 solver.cpp:341] Iteration 4000, Testing net (#0)
I0408 13:17:14.959410 31727 solver.cpp:409]     Test net output #0: accuracy = 0.9472
I0408 13:17:14.959486 31727 solver.cpp:409]     Test net output #1: loss = 0.173246 (* 1 = 0.173246 loss)
I0408 13:17:14.960439 31727 solver.cpp:237] Iteration 4000, loss = 0.159079
I0408 13:17:14.960460 31727 solver.cpp:253]     Train net output #0: loss = 0.159079 (* 1 = 0.159079 loss)
I0408 13:17:14.960472 31727 sgd_solver.cpp:106] Iteration 4000, lr = 0.0077697
I0408 13:17:15.125401 31727 solver.cpp:237] Iteration 4100, loss = 0.130313
I0408 13:17:15.125437 31727 solver.cpp:253]     Train net output #0: loss = 0.130313 (* 1 = 0.130313 loss)
I0408 13:17:15.125447 31727 sgd_solver.cpp:106] Iteration 4100, lr = 0.00772833
I0408 13:17:15.285117 31727 solver.cpp:237] Iteration 4200, loss = 0.131416
I0408 13:17:15.285146 31727 solver.cpp:253]     Train net output #0: loss = 0.131416 (* 1 = 0.131416 loss)
I0408 13:17:15.285154 31727 sgd_solver.cpp:106] Iteration 4200, lr = 0.00768748
I0408 13:17:15.444663 31727 solver.cpp:237] Iteration 4300, loss = 0.139923
I0408 13:17:15.444690 31727 solver.cpp:253]     Train net output #0: loss = 0.139924 (* 1 = 0.139924 loss)
I0408 13:17:15.444699 31727 sgd_solver.cpp:106] Iteration 4300, lr = 0.00764712
I0408 13:17:15.604076 31727 solver.cpp:237] Iteration 4400, loss = 0.201419
I0408 13:17:15.604105 31727 solver.cpp:253]     Train net output #0: loss = 0.201419 (* 1 = 0.201419 loss)
I0408 13:17:15.604115 31727 sgd_solver.cpp:106] Iteration 4400, lr = 0.00760726
I0408 13:17:15.761744 31727 solver.cpp:341] Iteration 4500, Testing net (#0)
I0408 13:17:15.836076 31727 solver.cpp:409]     Test net output #0: accuracy = 0.95
I0408 13:17:15.836103 31727 solver.cpp:409]     Test net output #1: loss = 0.16402 (* 1 = 0.16402 loss)
I0408 13:17:15.836941 31727 solver.cpp:237] Iteration 4500, loss = 0.182698
I0408 13:17:15.836959 31727 solver.cpp:253]     Train net output #0: loss = 0.182698 (* 1 = 0.182698 loss)
I0408 13:17:15.836971 31727 sgd_solver.cpp:106] Iteration 4500, lr = 0.00756788
I0408 13:17:15.997253 31727 solver.cpp:237] Iteration 4600, loss = 0.14852
I0408 13:17:15.997277 31727 solver.cpp:253]     Train net output #0: loss = 0.14852 (* 1 = 0.14852 loss)
I0408 13:17:15.997284 31727 sgd_solver.cpp:106] Iteration 4600, lr = 0.00752897
I0408 13:17:16.157150 31727 solver.cpp:237] Iteration 4700, loss = 0.114802
I0408 13:17:16.157177 31727 solver.cpp:253]     Train net output #0: loss = 0.114802 (* 1 = 0.114802 loss)
I0408 13:17:16.157183 31727 sgd_solver.cpp:106] Iteration 4700, lr = 0.00749052
I0408 13:17:16.317142 31727 solver.cpp:237] Iteration 4800, loss = 0.124069
I0408 13:17:16.317167 31727 solver.cpp:253]     Train net output #0: loss = 0.124069 (* 1 = 0.124069 loss)
I0408 13:17:16.317173 31727 sgd_solver.cpp:106] Iteration 4800, lr = 0.00745253
I0408 13:17:16.477272 31727 solver.cpp:237] Iteration 4900, loss = 0.135093
I0408 13:17:16.477295 31727 solver.cpp:253]     Train net output #0: loss = 0.135093 (* 1 = 0.135093 loss)
I0408 13:17:16.477301 31727 sgd_solver.cpp:106] Iteration 4900, lr = 0.00741498
I0408 13:17:17.296064 31727 solver.cpp:459] Snapshotting to binary proto file examples/mlp/mlp_test_iter_5000.caffemodel
I0408 13:17:17.388162 31727 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mlp/mlp_test_iter_5000.solverstate
I0408 13:17:17.393108 31727 solver.cpp:341] Iteration 5000, Testing net (#0)
I0408 13:17:17.911736 31727 solver.cpp:409]     Test net output #0: accuracy = 0.9522
I0408 13:17:17.911784 31727 solver.cpp:409]     Test net output #1: loss = 0.157088 (* 1 = 0.157088 loss)
I0408 13:17:17.912721 31727 solver.cpp:237] Iteration 5000, loss = 0.191441
I0408 13:17:17.912756 31727 solver.cpp:253]     Train net output #0: loss = 0.191441 (* 1 = 0.191441 loss)
I0408 13:17:17.912767 31727 sgd_solver.cpp:106] Iteration 5000, lr = 0.00737788
I0408 13:17:18.076498 31727 solver.cpp:237] Iteration 5100, loss = 0.178728
I0408 13:17:18.076539 31727 solver.cpp:253]     Train net output #0: loss = 0.178728 (* 1 = 0.178728 loss)
I0408 13:17:18.076547 31727 sgd_solver.cpp:106] Iteration 5100, lr = 0.0073412
I0408 13:17:18.240506 31727 solver.cpp:237] Iteration 5200, loss = 0.136701
I0408 13:17:18.240542 31727 solver.cpp:253]     Train net output #0: loss = 0.136701 (* 1 = 0.136701 loss)
I0408 13:17:18.240550 31727 sgd_solver.cpp:106] Iteration 5200, lr = 0.00730495
I0408 13:17:18.402217 31727 solver.cpp:237] Iteration 5300, loss = 0.102042
I0408 13:17:18.402256 31727 solver.cpp:253]     Train net output #0: loss = 0.102042 (* 1 = 0.102042 loss)
I0408 13:17:18.402263 31727 sgd_solver.cpp:106] Iteration 5300, lr = 0.00726911
I0408 13:17:18.567553 31727 solver.cpp:237] Iteration 5400, loss = 0.117788
I0408 13:17:18.567589 31727 solver.cpp:253]     Train net output #0: loss = 0.117788 (* 1 = 0.117788 loss)
I0408 13:17:18.567596 31727 sgd_solver.cpp:106] Iteration 5400, lr = 0.00723368
I0408 13:17:18.731362 31727 solver.cpp:341] Iteration 5500, Testing net (#0)
I0408 13:17:18.809636 31727 solver.cpp:409]     Test net output #0: accuracy = 0.95
I0408 13:17:18.809674 31727 solver.cpp:409]     Test net output #1: loss = 0.158194 (* 1 = 0.158194 loss)
I0408 13:17:18.810617 31727 solver.cpp:237] Iteration 5500, loss = 0.129458
I0408 13:17:18.810647 31727 solver.cpp:253]     Train net output #0: loss = 0.129457 (* 1 = 0.129457 loss)
I0408 13:17:18.810655 31727 sgd_solver.cpp:106] Iteration 5500, lr = 0.00719865
I0408 13:17:18.976943 31727 solver.cpp:237] Iteration 5600, loss = 0.180688
I0408 13:17:18.976975 31727 solver.cpp:253]     Train net output #0: loss = 0.180688 (* 1 = 0.180688 loss)
I0408 13:17:18.976984 31727 sgd_solver.cpp:106] Iteration 5600, lr = 0.00716402
I0408 13:17:19.142719 31727 solver.cpp:237] Iteration 5700, loss = 0.176981
I0408 13:17:19.142755 31727 solver.cpp:253]     Train net output #0: loss = 0.176981 (* 1 = 0.176981 loss)
I0408 13:17:19.142763 31727 sgd_solver.cpp:106] Iteration 5700, lr = 0.00712977
I0408 13:17:19.309962 31727 solver.cpp:237] Iteration 5800, loss = 0.12457
I0408 13:17:19.310000 31727 solver.cpp:253]     Train net output #0: loss = 0.12457 (* 1 = 0.12457 loss)
I0408 13:17:19.310009 31727 sgd_solver.cpp:106] Iteration 5800, lr = 0.0070959
I0408 13:17:19.476430 31727 solver.cpp:237] Iteration 5900, loss = 0.092391
I0408 13:17:19.476469 31727 solver.cpp:253]     Train net output #0: loss = 0.092391 (* 1 = 0.092391 loss)
I0408 13:17:19.476476 31727 sgd_solver.cpp:106] Iteration 5900, lr = 0.0070624
I0408 13:17:19.641868 31727 solver.cpp:341] Iteration 6000, Testing net (#0)
I0408 13:17:19.732502 31727 solver.cpp:409]     Test net output #0: accuracy = 0.9551
I0408 13:17:19.732547 31727 solver.cpp:409]     Test net output #1: loss = 0.147466 (* 1 = 0.147466 loss)
I0408 13:17:19.733391 31727 solver.cpp:237] Iteration 6000, loss = 0.111257
I0408 13:17:19.733418 31727 solver.cpp:253]     Train net output #0: loss = 0.111257 (* 1 = 0.111257 loss)
I0408 13:17:19.733433 31727 sgd_solver.cpp:106] Iteration 6000, lr = 0.00702927
I0408 13:17:19.897568 31727 solver.cpp:237] Iteration 6100, loss = 0.124347
I0408 13:17:19.897611 31727 solver.cpp:253]     Train net output #0: loss = 0.124347 (* 1 = 0.124347 loss)
I0408 13:17:19.897622 31727 sgd_solver.cpp:106] Iteration 6100, lr = 0.0069965
I0408 13:17:20.060840 31727 solver.cpp:237] Iteration 6200, loss = 0.171899
I0408 13:17:20.060880 31727 solver.cpp:253]     Train net output #0: loss = 0.171899 (* 1 = 0.171899 loss)
I0408 13:17:20.060890 31727 sgd_solver.cpp:106] Iteration 6200, lr = 0.00696408
I0408 13:17:20.225237 31727 solver.cpp:237] Iteration 6300, loss = 0.176256
I0408 13:17:20.225282 31727 solver.cpp:253]     Train net output #0: loss = 0.176256 (* 1 = 0.176256 loss)
I0408 13:17:20.225292 31727 sgd_solver.cpp:106] Iteration 6300, lr = 0.00693201
I0408 13:17:20.388164 31727 solver.cpp:237] Iteration 6400, loss = 0.115142
I0408 13:17:20.388201 31727 solver.cpp:253]     Train net output #0: loss = 0.115141 (* 1 = 0.115141 loss)
I0408 13:17:20.388209 31727 sgd_solver.cpp:106] Iteration 6400, lr = 0.00690029
I0408 13:17:20.546566 31727 solver.cpp:341] Iteration 6500, Testing net (#0)
I0408 13:17:20.629674 31727 solver.cpp:409]     Test net output #0: accuracy = 0.9541
I0408 13:17:20.629746 31727 solver.cpp:409]     Test net output #1: loss = 0.145603 (* 1 = 0.145603 loss)
I0408 13:17:20.630672 31727 solver.cpp:237] Iteration 6500, loss = 0.0883671
I0408 13:17:20.630697 31727 solver.cpp:253]     Train net output #0: loss = 0.0883671 (* 1 = 0.0883671 loss)
I0408 13:17:20.630718 31727 sgd_solver.cpp:106] Iteration 6500, lr = 0.0068689
I0408 13:17:20.792806 31727 solver.cpp:237] Iteration 6600, loss = 0.103624
I0408 13:17:20.792845 31727 solver.cpp:253]     Train net output #0: loss = 0.103624 (* 1 = 0.103624 loss)
I0408 13:17:20.792853 31727 sgd_solver.cpp:106] Iteration 6600, lr = 0.00683784
I0408 13:17:20.955826 31727 solver.cpp:237] Iteration 6700, loss = 0.120391
I0408 13:17:20.955862 31727 solver.cpp:253]     Train net output #0: loss = 0.120391 (* 1 = 0.120391 loss)
I0408 13:17:20.955870 31727 sgd_solver.cpp:106] Iteration 6700, lr = 0.00680711
I0408 13:17:21.119551 31727 solver.cpp:237] Iteration 6800, loss = 0.166123
I0408 13:17:21.119592 31727 solver.cpp:253]     Train net output #0: loss = 0.166124 (* 1 = 0.166124 loss)
I0408 13:17:21.119604 31727 sgd_solver.cpp:106] Iteration 6800, lr = 0.0067767
I0408 13:17:21.285197 31727 solver.cpp:237] Iteration 6900, loss = 0.176204
I0408 13:17:21.285236 31727 solver.cpp:253]     Train net output #0: loss = 0.176204 (* 1 = 0.176204 loss)
I0408 13:17:21.285246 31727 sgd_solver.cpp:106] Iteration 6900, lr = 0.0067466
I0408 13:17:21.446530 31727 solver.cpp:341] Iteration 7000, Testing net (#0)
I0408 13:17:21.525578 31727 solver.cpp:409]     Test net output #0: accuracy = 0.9557
I0408 13:17:21.525619 31727 solver.cpp:409]     Test net output #1: loss = 0.145826 (* 1 = 0.145826 loss)
I0408 13:17:21.526486 31727 solver.cpp:237] Iteration 7000, loss = 0.107719
I0408 13:17:21.526520 31727 solver.cpp:253]     Train net output #0: loss = 0.107719 (* 1 = 0.107719 loss)
I0408 13:17:21.526533 31727 sgd_solver.cpp:106] Iteration 7000, lr = 0.00671681
I0408 13:17:21.689685 31727 solver.cpp:237] Iteration 7100, loss = 0.0883815
I0408 13:17:21.689729 31727 solver.cpp:253]     Train net output #0: loss = 0.0883815 (* 1 = 0.0883815 loss)
I0408 13:17:21.689741 31727 sgd_solver.cpp:106] Iteration 7100, lr = 0.00668733
I0408 13:17:21.852968 31727 solver.cpp:237] Iteration 7200, loss = 0.0956538
I0408 13:17:21.853004 31727 solver.cpp:253]     Train net output #0: loss = 0.0956539 (* 1 = 0.0956539 loss)
I0408 13:17:21.853013 31727 sgd_solver.cpp:106] Iteration 7200, lr = 0.00665815
I0408 13:17:22.016814 31727 solver.cpp:237] Iteration 7300, loss = 0.115137
I0408 13:17:22.016856 31727 solver.cpp:253]     Train net output #0: loss = 0.115137 (* 1 = 0.115137 loss)
I0408 13:17:22.016867 31727 sgd_solver.cpp:106] Iteration 7300, lr = 0.00662927
I0408 13:17:22.181185 31727 solver.cpp:237] Iteration 7400, loss = 0.162934
I0408 13:17:22.181227 31727 solver.cpp:253]     Train net output #0: loss = 0.162934 (* 1 = 0.162934 loss)
I0408 13:17:22.181239 31727 sgd_solver.cpp:106] Iteration 7400, lr = 0.00660067
I0408 13:17:22.343082 31727 solver.cpp:341] Iteration 7500, Testing net (#0)
I0408 13:17:22.422433 31727 solver.cpp:409]     Test net output #0: accuracy = 0.9549
I0408 13:17:22.422473 31727 solver.cpp:409]     Test net output #1: loss = 0.14492 (* 1 = 0.14492 loss)
I0408 13:17:22.423424 31727 solver.cpp:237] Iteration 7500, loss = 0.175445
I0408 13:17:22.423447 31727 solver.cpp:253]     Train net output #0: loss = 0.175445 (* 1 = 0.175445 loss)
I0408 13:17:22.423459 31727 sgd_solver.cpp:106] Iteration 7500, lr = 0.00657236
I0408 13:17:22.584693 31727 solver.cpp:237] Iteration 7600, loss = 0.100957
I0408 13:17:22.584722 31727 solver.cpp:253]     Train net output #0: loss = 0.100957 (* 1 = 0.100957 loss)
I0408 13:17:22.584728 31727 sgd_solver.cpp:106] Iteration 7600, lr = 0.00654433
I0408 13:17:22.745120 31727 solver.cpp:237] Iteration 7700, loss = 0.0887372
I0408 13:17:22.745148 31727 solver.cpp:253]     Train net output #0: loss = 0.0887372 (* 1 = 0.0887372 loss)
I0408 13:17:22.745154 31727 sgd_solver.cpp:106] Iteration 7700, lr = 0.00651658
I0408 13:17:22.905072 31727 solver.cpp:237] Iteration 7800, loss = 0.088472
I0408 13:17:22.905098 31727 solver.cpp:253]     Train net output #0: loss = 0.0884721 (* 1 = 0.0884721 loss)
I0408 13:17:22.905103 31727 sgd_solver.cpp:106] Iteration 7800, lr = 0.00648911
I0408 13:17:23.065254 31727 solver.cpp:237] Iteration 7900, loss = 0.108393
I0408 13:17:23.065281 31727 solver.cpp:253]     Train net output #0: loss = 0.108393 (* 1 = 0.108393 loss)
I0408 13:17:23.065286 31727 sgd_solver.cpp:106] Iteration 7900, lr = 0.0064619
I0408 13:17:23.223979 31727 solver.cpp:341] Iteration 8000, Testing net (#0)
I0408 13:17:23.327927 31727 solver.cpp:409]     Test net output #0: accuracy = 0.9621
I0408 13:17:23.327955 31727 solver.cpp:409]     Test net output #1: loss = 0.127673 (* 1 = 0.127673 loss)
I0408 13:17:23.328779 31727 solver.cpp:237] Iteration 8000, loss = 0.161734
I0408 13:17:23.328794 31727 solver.cpp:253]     Train net output #0: loss = 0.161734 (* 1 = 0.161734 loss)
I0408 13:17:23.328801 31727 sgd_solver.cpp:106] Iteration 8000, lr = 0.00643496
I0408 13:17:23.488924 31727 solver.cpp:237] Iteration 8100, loss = 0.174441
I0408 13:17:23.488950 31727 solver.cpp:253]     Train net output #0: loss = 0.174441 (* 1 = 0.174441 loss)
I0408 13:17:23.488960 31727 sgd_solver.cpp:106] Iteration 8100, lr = 0.00640827
I0408 13:17:23.649116 31727 solver.cpp:237] Iteration 8200, loss = 0.0944742
I0408 13:17:23.649145 31727 solver.cpp:253]     Train net output #0: loss = 0.0944742 (* 1 = 0.0944742 loss)
I0408 13:17:23.649157 31727 sgd_solver.cpp:106] Iteration 8200, lr = 0.00638185
I0408 13:17:23.809028 31727 solver.cpp:237] Iteration 8300, loss = 0.0869794
I0408 13:17:23.809052 31727 solver.cpp:253]     Train net output #0: loss = 0.0869794 (* 1 = 0.0869794 loss)
I0408 13:17:23.809061 31727 sgd_solver.cpp:106] Iteration 8300, lr = 0.00635567
I0408 13:17:23.968834 31727 solver.cpp:237] Iteration 8400, loss = 0.0815172
I0408 13:17:23.968859 31727 solver.cpp:253]     Train net output #0: loss = 0.0815172 (* 1 = 0.0815172 loss)
I0408 13:17:23.968864 31727 sgd_solver.cpp:106] Iteration 8400, lr = 0.00632975
I0408 13:17:24.128415 31727 solver.cpp:341] Iteration 8500, Testing net (#0)
I0408 13:17:24.204051 31727 solver.cpp:409]     Test net output #0: accuracy = 0.9625
I0408 13:17:24.204078 31727 solver.cpp:409]     Test net output #1: loss = 0.125436 (* 1 = 0.125436 loss)
I0408 13:17:24.204881 31727 solver.cpp:237] Iteration 8500, loss = 0.101601
I0408 13:17:24.204900 31727 solver.cpp:253]     Train net output #0: loss = 0.101601 (* 1 = 0.101601 loss)
I0408 13:17:24.204917 31727 sgd_solver.cpp:106] Iteration 8500, lr = 0.00630407
I0408 13:17:24.364029 31727 solver.cpp:237] Iteration 8600, loss = 0.161496
I0408 13:17:24.364058 31727 solver.cpp:253]     Train net output #0: loss = 0.161496 (* 1 = 0.161496 loss)
I0408 13:17:24.364066 31727 sgd_solver.cpp:106] Iteration 8600, lr = 0.00627864
I0408 13:17:24.524044 31727 solver.cpp:237] Iteration 8700, loss = 0.173271
I0408 13:17:24.524073 31727 solver.cpp:253]     Train net output #0: loss = 0.173271 (* 1 = 0.173271 loss)
I0408 13:17:24.524083 31727 sgd_solver.cpp:106] Iteration 8700, lr = 0.00625344
I0408 13:17:24.683969 31727 solver.cpp:237] Iteration 8800, loss = 0.0871128
I0408 13:17:24.683997 31727 solver.cpp:253]     Train net output #0: loss = 0.0871129 (* 1 = 0.0871129 loss)
I0408 13:17:24.684008 31727 sgd_solver.cpp:106] Iteration 8800, lr = 0.00622847
I0408 13:17:24.844321 31727 solver.cpp:237] Iteration 8900, loss = 0.083596
I0408 13:17:24.844348 31727 solver.cpp:253]     Train net output #0: loss = 0.083596 (* 1 = 0.083596 loss)
I0408 13:17:24.844359 31727 sgd_solver.cpp:106] Iteration 8900, lr = 0.00620374
I0408 13:17:25.003087 31727 solver.cpp:341] Iteration 9000, Testing net (#0)
I0408 13:17:25.090703 31727 solver.cpp:409]     Test net output #0: accuracy = 0.9604
I0408 13:17:25.090734 31727 solver.cpp:409]     Test net output #1: loss = 0.134738 (* 1 = 0.134738 loss)
I0408 13:17:25.091625 31727 solver.cpp:237] Iteration 9000, loss = 0.0749834
I0408 13:17:25.091642 31727 solver.cpp:253]     Train net output #0: loss = 0.0749835 (* 1 = 0.0749835 loss)
I0408 13:17:25.091678 31727 sgd_solver.cpp:106] Iteration 9000, lr = 0.00617924
I0408 13:17:26.171622 31727 solver.cpp:237] Iteration 9100, loss = 0.0957275
I0408 13:17:26.171666 31727 solver.cpp:253]     Train net output #0: loss = 0.0957275 (* 1 = 0.0957275 loss)
I0408 13:17:26.171675 31727 sgd_solver.cpp:106] Iteration 9100, lr = 0.00615496
I0408 13:17:26.600421 31727 solver.cpp:237] Iteration 9200, loss = 0.160761
I0408 13:17:26.600461 31727 solver.cpp:253]     Train net output #0: loss = 0.160761 (* 1 = 0.160761 loss)
I0408 13:17:26.600468 31727 sgd_solver.cpp:106] Iteration 9200, lr = 0.0061309
I0408 13:17:26.765418 31727 solver.cpp:237] Iteration 9300, loss = 0.171151
I0408 13:17:26.765470 31727 solver.cpp:253]     Train net output #0: loss = 0.171151 (* 1 = 0.171151 loss)
I0408 13:17:26.765480 31727 sgd_solver.cpp:106] Iteration 9300, lr = 0.00610706
I0408 13:17:26.930838 31727 solver.cpp:237] Iteration 9400, loss = 0.0791094
I0408 13:17:26.930881 31727 solver.cpp:253]     Train net output #0: loss = 0.0791095 (* 1 = 0.0791095 loss)
I0408 13:17:26.930889 31727 sgd_solver.cpp:106] Iteration 9400, lr = 0.00608343
I0408 13:17:27.094033 31727 solver.cpp:341] Iteration 9500, Testing net (#0)
I0408 13:17:27.169541 31727 solver.cpp:409]     Test net output #0: accuracy = 0.9623
I0408 13:17:27.169580 31727 solver.cpp:409]     Test net output #1: loss = 0.124664 (* 1 = 0.124664 loss)
I0408 13:17:27.170557 31727 solver.cpp:237] Iteration 9500, loss = 0.078804
I0408 13:17:27.170589 31727 solver.cpp:253]     Train net output #0: loss = 0.078804 (* 1 = 0.078804 loss)
I0408 13:17:27.170601 31727 sgd_solver.cpp:106] Iteration 9500, lr = 0.00606002
I0408 13:17:27.337818 31727 solver.cpp:237] Iteration 9600, loss = 0.0698419
I0408 13:17:27.337855 31727 solver.cpp:253]     Train net output #0: loss = 0.0698419 (* 1 = 0.0698419 loss)
I0408 13:17:27.337868 31727 sgd_solver.cpp:106] Iteration 9600, lr = 0.00603682
I0408 13:17:27.503552 31727 solver.cpp:237] Iteration 9700, loss = 0.0906657
I0408 13:17:27.503590 31727 solver.cpp:253]     Train net output #0: loss = 0.0906657 (* 1 = 0.0906657 loss)
I0408 13:17:27.503602 31727 sgd_solver.cpp:106] Iteration 9700, lr = 0.00601382
I0408 13:17:27.669679 31727 solver.cpp:237] Iteration 9800, loss = 0.158845
I0408 13:17:27.669718 31727 solver.cpp:253]     Train net output #0: loss = 0.158845 (* 1 = 0.158845 loss)
I0408 13:17:27.669728 31727 sgd_solver.cpp:106] Iteration 9800, lr = 0.00599102
I0408 13:17:27.836454 31727 solver.cpp:237] Iteration 9900, loss = 0.167988
I0408 13:17:27.836486 31727 solver.cpp:253]     Train net output #0: loss = 0.167988 (* 1 = 0.167988 loss)
I0408 13:17:27.836496 31727 sgd_solver.cpp:106] Iteration 9900, lr = 0.00596843
I0408 13:17:28.002177 31727 solver.cpp:459] Snapshotting to binary proto file examples/mlp/mlp_test_iter_10000.caffemodel
I0408 13:17:28.012145 31727 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mlp/mlp_test_iter_10000.solverstate
I0408 13:17:28.017920 31727 solver.cpp:321] Iteration 10000, loss = 0.0715797
I0408 13:17:28.017966 31727 solver.cpp:341] Iteration 10000, Testing net (#0)
I0408 13:17:28.126097 31727 solver.cpp:409]     Test net output #0: accuracy = 0.9628
I0408 13:17:28.126132 31727 solver.cpp:409]     Test net output #1: loss = 0.121054 (* 1 = 0.121054 loss)
I0408 13:17:28.126138 31727 solver.cpp:326] Optimization Done.
I0408 13:17:28.126142 31727 caffe.cpp:215] Optimization Done.
