I0328 17:16:19.427417  1546 caffe.cpp:184] Using GPUs 0
I0328 17:16:19.654170  1546 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mlp/mlp"
solver_mode: GPU
device_id: 0
net: "examples/mlp/mlp_train_test.prototxt"
I0328 17:16:19.654280  1546 solver.cpp:91] Creating training net from net file: examples/mlp/mlp_train_test.prototxt
I0328 17:16:19.654554  1546 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0328 17:16:19.654569  1546 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0328 17:16:19.654629  1546 net.cpp:49] Initializing net from parameters: 
name: "MLP"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "ip4"
  type: "InnerProduct"
  bottom: "ip3"
  top: "ip4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip4"
  bottom: "label"
  top: "loss"
}
I0328 17:16:19.654685  1546 layer_factory.hpp:77] Creating layer mnist
I0328 17:16:19.655148  1546 net.cpp:106] Creating Layer mnist
I0328 17:16:19.655161  1546 net.cpp:411] mnist -> data
I0328 17:16:19.655182  1546 net.cpp:411] mnist -> label
I0328 17:16:19.656286  1550 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I0328 17:16:19.662524  1546 data_layer.cpp:41] output data size: 64,1,28,28
I0328 17:16:19.663288  1546 net.cpp:150] Setting up mnist
I0328 17:16:19.663311  1546 net.cpp:157] Top shape: 64 1 28 28 (50176)
I0328 17:16:19.663316  1546 net.cpp:157] Top shape: 64 (64)
I0328 17:16:19.663321  1546 net.cpp:165] Memory required for data: 200960
I0328 17:16:19.663329  1546 layer_factory.hpp:77] Creating layer ip1
I0328 17:16:19.663343  1546 net.cpp:106] Creating Layer ip1
I0328 17:16:19.663352  1546 net.cpp:454] ip1 <- data
I0328 17:16:19.663369  1546 net.cpp:411] ip1 -> ip1
I0328 17:16:19.663936  1546 net.cpp:150] Setting up ip1
I0328 17:16:19.663950  1546 net.cpp:157] Top shape: 64 10 (640)
I0328 17:16:19.663957  1546 net.cpp:165] Memory required for data: 203520
I0328 17:16:19.663976  1546 layer_factory.hpp:77] Creating layer relu1
I0328 17:16:19.663990  1546 net.cpp:106] Creating Layer relu1
I0328 17:16:19.664000  1546 net.cpp:454] relu1 <- ip1
I0328 17:16:19.664008  1546 net.cpp:397] relu1 -> ip1 (in-place)
I0328 17:16:19.777968  1546 net.cpp:150] Setting up relu1
I0328 17:16:19.778000  1546 net.cpp:157] Top shape: 64 10 (640)
I0328 17:16:19.778007  1546 net.cpp:165] Memory required for data: 206080
I0328 17:16:19.778013  1546 layer_factory.hpp:77] Creating layer ip2
I0328 17:16:19.778056  1546 net.cpp:106] Creating Layer ip2
I0328 17:16:19.778067  1546 net.cpp:454] ip2 <- ip1
I0328 17:16:19.778079  1546 net.cpp:411] ip2 -> ip2
I0328 17:16:19.778188  1546 net.cpp:150] Setting up ip2
I0328 17:16:19.778199  1546 net.cpp:157] Top shape: 64 10 (640)
I0328 17:16:19.778205  1546 net.cpp:165] Memory required for data: 208640
I0328 17:16:19.778220  1546 layer_factory.hpp:77] Creating layer relu2
I0328 17:16:19.778234  1546 net.cpp:106] Creating Layer relu2
I0328 17:16:19.778241  1546 net.cpp:454] relu2 <- ip2
I0328 17:16:19.778250  1546 net.cpp:397] relu2 -> ip2 (in-place)
I0328 17:16:19.778856  1546 net.cpp:150] Setting up relu2
I0328 17:16:19.778869  1546 net.cpp:157] Top shape: 64 10 (640)
I0328 17:16:19.778877  1546 net.cpp:165] Memory required for data: 211200
I0328 17:16:19.778882  1546 layer_factory.hpp:77] Creating layer ip3
I0328 17:16:19.778893  1546 net.cpp:106] Creating Layer ip3
I0328 17:16:19.778899  1546 net.cpp:454] ip3 <- ip2
I0328 17:16:19.778908  1546 net.cpp:411] ip3 -> ip3
I0328 17:16:19.779388  1546 net.cpp:150] Setting up ip3
I0328 17:16:19.779403  1546 net.cpp:157] Top shape: 64 256 (16384)
I0328 17:16:19.779409  1546 net.cpp:165] Memory required for data: 276736
I0328 17:16:19.779423  1546 layer_factory.hpp:77] Creating layer relu3
I0328 17:16:19.779436  1546 net.cpp:106] Creating Layer relu3
I0328 17:16:19.779443  1546 net.cpp:454] relu3 <- ip3
I0328 17:16:19.779451  1546 net.cpp:397] relu3 -> ip3 (in-place)
I0328 17:16:19.779969  1546 net.cpp:150] Setting up relu3
I0328 17:16:19.779983  1546 net.cpp:157] Top shape: 64 256 (16384)
I0328 17:16:19.779989  1546 net.cpp:165] Memory required for data: 342272
I0328 17:16:19.779994  1546 layer_factory.hpp:77] Creating layer ip4
I0328 17:16:19.780005  1546 net.cpp:106] Creating Layer ip4
I0328 17:16:19.780011  1546 net.cpp:454] ip4 <- ip3
I0328 17:16:19.780020  1546 net.cpp:411] ip4 -> ip4
I0328 17:16:19.780122  1546 net.cpp:150] Setting up ip4
I0328 17:16:19.780131  1546 net.cpp:157] Top shape: 64 10 (640)
I0328 17:16:19.780138  1546 net.cpp:165] Memory required for data: 344832
I0328 17:16:19.780148  1546 layer_factory.hpp:77] Creating layer loss
I0328 17:16:19.780158  1546 net.cpp:106] Creating Layer loss
I0328 17:16:19.780165  1546 net.cpp:454] loss <- ip4
I0328 17:16:19.780174  1546 net.cpp:454] loss <- label
I0328 17:16:19.780184  1546 net.cpp:411] loss -> loss
I0328 17:16:19.780205  1546 layer_factory.hpp:77] Creating layer loss
I0328 17:16:19.781174  1546 net.cpp:150] Setting up loss
I0328 17:16:19.781190  1546 net.cpp:157] Top shape: (1)
I0328 17:16:19.781196  1546 net.cpp:160]     with loss weight 1
I0328 17:16:19.781220  1546 net.cpp:165] Memory required for data: 344836
I0328 17:16:19.781227  1546 net.cpp:226] loss needs backward computation.
I0328 17:16:19.781234  1546 net.cpp:226] ip4 needs backward computation.
I0328 17:16:19.781239  1546 net.cpp:226] relu3 needs backward computation.
I0328 17:16:19.781244  1546 net.cpp:226] ip3 needs backward computation.
I0328 17:16:19.781250  1546 net.cpp:226] relu2 needs backward computation.
I0328 17:16:19.781255  1546 net.cpp:226] ip2 needs backward computation.
I0328 17:16:19.781260  1546 net.cpp:226] relu1 needs backward computation.
I0328 17:16:19.781265  1546 net.cpp:226] ip1 needs backward computation.
I0328 17:16:19.781271  1546 net.cpp:228] mnist does not need backward computation.
I0328 17:16:19.781276  1546 net.cpp:270] This network produces output loss
I0328 17:16:19.781288  1546 net.cpp:283] Network initialization done.
I0328 17:16:19.781559  1546 solver.cpp:181] Creating test net (#0) specified by net file: examples/mlp/mlp_train_test.prototxt
I0328 17:16:19.781591  1546 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0328 17:16:19.781669  1546 net.cpp:49] Initializing net from parameters: 
name: "MLP"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "ip4"
  type: "InnerProduct"
  bottom: "ip3"
  top: "ip4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip4"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip4"
  bottom: "label"
  top: "loss"
}
I0328 17:16:19.781774  1546 layer_factory.hpp:77] Creating layer mnist
I0328 17:16:19.781919  1546 net.cpp:106] Creating Layer mnist
I0328 17:16:19.781929  1546 net.cpp:411] mnist -> data
I0328 17:16:19.781944  1546 net.cpp:411] mnist -> label
I0328 17:16:19.783090  1552 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I0328 17:16:19.783198  1546 data_layer.cpp:41] output data size: 100,1,28,28
I0328 17:16:19.784060  1546 net.cpp:150] Setting up mnist
I0328 17:16:19.784077  1546 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0328 17:16:19.784090  1546 net.cpp:157] Top shape: 100 (100)
I0328 17:16:19.784095  1546 net.cpp:165] Memory required for data: 314000
I0328 17:16:19.784101  1546 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0328 17:16:19.784116  1546 net.cpp:106] Creating Layer label_mnist_1_split
I0328 17:16:19.784123  1546 net.cpp:454] label_mnist_1_split <- label
I0328 17:16:19.784133  1546 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I0328 17:16:19.784147  1546 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I0328 17:16:19.784193  1546 net.cpp:150] Setting up label_mnist_1_split
I0328 17:16:19.784204  1546 net.cpp:157] Top shape: 100 (100)
I0328 17:16:19.784210  1546 net.cpp:157] Top shape: 100 (100)
I0328 17:16:19.784215  1546 net.cpp:165] Memory required for data: 314800
I0328 17:16:19.784221  1546 layer_factory.hpp:77] Creating layer ip1
I0328 17:16:19.784234  1546 net.cpp:106] Creating Layer ip1
I0328 17:16:19.784240  1546 net.cpp:454] ip1 <- data
I0328 17:16:19.784251  1546 net.cpp:411] ip1 -> ip1
I0328 17:16:19.784394  1546 net.cpp:150] Setting up ip1
I0328 17:16:19.784405  1546 net.cpp:157] Top shape: 100 10 (1000)
I0328 17:16:19.784410  1546 net.cpp:165] Memory required for data: 318800
I0328 17:16:19.784425  1546 layer_factory.hpp:77] Creating layer relu1
I0328 17:16:19.784437  1546 net.cpp:106] Creating Layer relu1
I0328 17:16:19.784445  1546 net.cpp:454] relu1 <- ip1
I0328 17:16:19.784452  1546 net.cpp:397] relu1 -> ip1 (in-place)
I0328 17:16:19.785195  1546 net.cpp:150] Setting up relu1
I0328 17:16:19.785214  1546 net.cpp:157] Top shape: 100 10 (1000)
I0328 17:16:19.785223  1546 net.cpp:165] Memory required for data: 322800
I0328 17:16:19.785235  1546 layer_factory.hpp:77] Creating layer ip2
I0328 17:16:19.785248  1546 net.cpp:106] Creating Layer ip2
I0328 17:16:19.785257  1546 net.cpp:454] ip2 <- ip1
I0328 17:16:19.785270  1546 net.cpp:411] ip2 -> ip2
I0328 17:16:19.785429  1546 net.cpp:150] Setting up ip2
I0328 17:16:19.785442  1546 net.cpp:157] Top shape: 100 10 (1000)
I0328 17:16:19.785450  1546 net.cpp:165] Memory required for data: 326800
I0328 17:16:19.785465  1546 layer_factory.hpp:77] Creating layer relu2
I0328 17:16:19.785476  1546 net.cpp:106] Creating Layer relu2
I0328 17:16:19.785485  1546 net.cpp:454] relu2 <- ip2
I0328 17:16:19.785493  1546 net.cpp:397] relu2 -> ip2 (in-place)
I0328 17:16:19.786149  1546 net.cpp:150] Setting up relu2
I0328 17:16:19.786164  1546 net.cpp:157] Top shape: 100 10 (1000)
I0328 17:16:19.786170  1546 net.cpp:165] Memory required for data: 330800
I0328 17:16:19.786176  1546 layer_factory.hpp:77] Creating layer ip3
I0328 17:16:19.786188  1546 net.cpp:106] Creating Layer ip3
I0328 17:16:19.786195  1546 net.cpp:454] ip3 <- ip2
I0328 17:16:19.786206  1546 net.cpp:411] ip3 -> ip3
I0328 17:16:19.786360  1546 net.cpp:150] Setting up ip3
I0328 17:16:19.786373  1546 net.cpp:157] Top shape: 100 256 (25600)
I0328 17:16:19.786380  1546 net.cpp:165] Memory required for data: 433200
I0328 17:16:19.786393  1546 layer_factory.hpp:77] Creating layer relu3
I0328 17:16:19.786412  1546 net.cpp:106] Creating Layer relu3
I0328 17:16:19.786428  1546 net.cpp:454] relu3 <- ip3
I0328 17:16:19.786440  1546 net.cpp:397] relu3 -> ip3 (in-place)
I0328 17:16:19.787111  1546 net.cpp:150] Setting up relu3
I0328 17:16:19.787127  1546 net.cpp:157] Top shape: 100 256 (25600)
I0328 17:16:19.787134  1546 net.cpp:165] Memory required for data: 535600
I0328 17:16:19.787140  1546 layer_factory.hpp:77] Creating layer ip4
I0328 17:16:19.787153  1546 net.cpp:106] Creating Layer ip4
I0328 17:16:19.787160  1546 net.cpp:454] ip4 <- ip3
I0328 17:16:19.787173  1546 net.cpp:411] ip4 -> ip4
I0328 17:16:19.787286  1546 net.cpp:150] Setting up ip4
I0328 17:16:19.787298  1546 net.cpp:157] Top shape: 100 10 (1000)
I0328 17:16:19.787307  1546 net.cpp:165] Memory required for data: 539600
I0328 17:16:19.787318  1546 layer_factory.hpp:77] Creating layer ip4_ip4_0_split
I0328 17:16:19.787333  1546 net.cpp:106] Creating Layer ip4_ip4_0_split
I0328 17:16:19.787343  1546 net.cpp:454] ip4_ip4_0_split <- ip4
I0328 17:16:19.787353  1546 net.cpp:411] ip4_ip4_0_split -> ip4_ip4_0_split_0
I0328 17:16:19.787366  1546 net.cpp:411] ip4_ip4_0_split -> ip4_ip4_0_split_1
I0328 17:16:19.787408  1546 net.cpp:150] Setting up ip4_ip4_0_split
I0328 17:16:19.787420  1546 net.cpp:157] Top shape: 100 10 (1000)
I0328 17:16:19.787430  1546 net.cpp:157] Top shape: 100 10 (1000)
I0328 17:16:19.787436  1546 net.cpp:165] Memory required for data: 547600
I0328 17:16:19.787443  1546 layer_factory.hpp:77] Creating layer accuracy
I0328 17:16:19.787454  1546 net.cpp:106] Creating Layer accuracy
I0328 17:16:19.787462  1546 net.cpp:454] accuracy <- ip4_ip4_0_split_0
I0328 17:16:19.787469  1546 net.cpp:454] accuracy <- label_mnist_1_split_0
I0328 17:16:19.787478  1546 net.cpp:411] accuracy -> accuracy
I0328 17:16:19.787492  1546 net.cpp:150] Setting up accuracy
I0328 17:16:19.787500  1546 net.cpp:157] Top shape: (1)
I0328 17:16:19.787508  1546 net.cpp:165] Memory required for data: 547604
I0328 17:16:19.787514  1546 layer_factory.hpp:77] Creating layer loss
I0328 17:16:19.787523  1546 net.cpp:106] Creating Layer loss
I0328 17:16:19.787530  1546 net.cpp:454] loss <- ip4_ip4_0_split_1
I0328 17:16:19.787538  1546 net.cpp:454] loss <- label_mnist_1_split_1
I0328 17:16:19.787547  1546 net.cpp:411] loss -> loss
I0328 17:16:19.787561  1546 layer_factory.hpp:77] Creating layer loss
I0328 17:16:19.788254  1546 net.cpp:150] Setting up loss
I0328 17:16:19.788267  1546 net.cpp:157] Top shape: (1)
I0328 17:16:19.788274  1546 net.cpp:160]     with loss weight 1
I0328 17:16:19.788286  1546 net.cpp:165] Memory required for data: 547608
I0328 17:16:19.788293  1546 net.cpp:226] loss needs backward computation.
I0328 17:16:19.788300  1546 net.cpp:228] accuracy does not need backward computation.
I0328 17:16:19.788305  1546 net.cpp:226] ip4_ip4_0_split needs backward computation.
I0328 17:16:19.788311  1546 net.cpp:226] ip4 needs backward computation.
I0328 17:16:19.788329  1546 net.cpp:226] relu3 needs backward computation.
I0328 17:16:19.788336  1546 net.cpp:226] ip3 needs backward computation.
I0328 17:16:19.788341  1546 net.cpp:226] relu2 needs backward computation.
I0328 17:16:19.788347  1546 net.cpp:226] ip2 needs backward computation.
I0328 17:16:19.788352  1546 net.cpp:226] relu1 needs backward computation.
I0328 17:16:19.788357  1546 net.cpp:226] ip1 needs backward computation.
I0328 17:16:19.788362  1546 net.cpp:228] label_mnist_1_split does not need backward computation.
I0328 17:16:19.788368  1546 net.cpp:228] mnist does not need backward computation.
I0328 17:16:19.788373  1546 net.cpp:270] This network produces output accuracy
I0328 17:16:19.788378  1546 net.cpp:270] This network produces output loss
I0328 17:16:19.788394  1546 net.cpp:283] Network initialization done.
I0328 17:16:19.788447  1546 solver.cpp:60] Solver scaffolding done.
I0328 17:16:19.788668  1546 caffe.cpp:212] Starting Optimization
I0328 17:16:19.788679  1546 solver.cpp:288] Solving MLP
I0328 17:16:19.788684  1546 solver.cpp:289] Learning Rate Policy: inv
I0328 17:16:19.788938  1546 solver.cpp:341] Iteration 0, Testing net (#0)
I0328 17:16:19.791831  1546 blocking_queue.cpp:50] Data layer prefetch queue empty
I0328 17:16:19.860752  1546 solver.cpp:409]     Test net output #0: accuracy = 0.083
I0328 17:16:19.860797  1546 solver.cpp:409]     Test net output #1: loss = 2.30073 (* 1 = 2.30073 loss)
I0328 17:16:19.861649  1546 solver.cpp:237] Iteration 0, loss = 2.30303
I0328 17:16:19.861675  1546 solver.cpp:253]     Train net output #0: loss = 2.30303 (* 1 = 2.30303 loss)
I0328 17:16:19.861718  1546 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0328 17:16:19.930241  1546 solver.cpp:237] Iteration 100, loss = 0.988103
I0328 17:16:19.930279  1546 solver.cpp:253]     Train net output #0: loss = 0.988103 (* 1 = 0.988103 loss)
I0328 17:16:19.930289  1546 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0328 17:16:19.998477  1546 solver.cpp:237] Iteration 200, loss = 0.418767
I0328 17:16:19.998514  1546 solver.cpp:253]     Train net output #0: loss = 0.418767 (* 1 = 0.418767 loss)
I0328 17:16:19.998523  1546 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0328 17:16:20.066892  1546 solver.cpp:237] Iteration 300, loss = 0.464127
I0328 17:16:20.066929  1546 solver.cpp:253]     Train net output #0: loss = 0.464127 (* 1 = 0.464127 loss)
I0328 17:16:20.066938  1546 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0328 17:16:20.135200  1546 solver.cpp:237] Iteration 400, loss = 0.37997
I0328 17:16:20.135237  1546 solver.cpp:253]     Train net output #0: loss = 0.37997 (* 1 = 0.37997 loss)
I0328 17:16:20.135260  1546 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0328 17:16:20.203040  1546 solver.cpp:341] Iteration 500, Testing net (#0)
I0328 17:16:20.254410  1546 solver.cpp:409]     Test net output #0: accuracy = 0.905
I0328 17:16:20.254446  1546 solver.cpp:409]     Test net output #1: loss = 0.323331 (* 1 = 0.323331 loss)
I0328 17:16:20.255012  1546 solver.cpp:237] Iteration 500, loss = 0.337898
I0328 17:16:20.255034  1546 solver.cpp:253]     Train net output #0: loss = 0.337898 (* 1 = 0.337898 loss)
I0328 17:16:20.255094  1546 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I0328 17:16:20.323173  1546 solver.cpp:237] Iteration 600, loss = 0.258928
I0328 17:16:20.323210  1546 solver.cpp:253]     Train net output #0: loss = 0.258928 (* 1 = 0.258928 loss)
I0328 17:16:20.323220  1546 sgd_solver.cpp:106] Iteration 600, lr = 0.0095724
I0328 17:16:20.391403  1546 solver.cpp:237] Iteration 700, loss = 0.522228
I0328 17:16:20.391441  1546 solver.cpp:253]     Train net output #0: loss = 0.522228 (* 1 = 0.522228 loss)
I0328 17:16:20.391449  1546 sgd_solver.cpp:106] Iteration 700, lr = 0.00950522
I0328 17:16:20.459523  1546 solver.cpp:237] Iteration 800, loss = 0.436131
I0328 17:16:20.459558  1546 solver.cpp:253]     Train net output #0: loss = 0.436131 (* 1 = 0.436131 loss)
I0328 17:16:20.459569  1546 sgd_solver.cpp:106] Iteration 800, lr = 0.00943913
I0328 17:16:20.534271  1546 solver.cpp:237] Iteration 900, loss = 0.283241
I0328 17:16:20.534318  1546 solver.cpp:253]     Train net output #0: loss = 0.283241 (* 1 = 0.283241 loss)
I0328 17:16:20.534328  1546 sgd_solver.cpp:106] Iteration 900, lr = 0.00937411
I0328 17:16:20.601529  1546 solver.cpp:341] Iteration 1000, Testing net (#0)
I0328 17:16:20.651798  1546 solver.cpp:409]     Test net output #0: accuracy = 0.9149
I0328 17:16:20.651823  1546 solver.cpp:409]     Test net output #1: loss = 0.276169 (* 1 = 0.276169 loss)
I0328 17:16:20.652242  1546 solver.cpp:237] Iteration 1000, loss = 0.250486
I0328 17:16:20.652261  1546 solver.cpp:253]     Train net output #0: loss = 0.250486 (* 1 = 0.250486 loss)
I0328 17:16:20.652272  1546 sgd_solver.cpp:106] Iteration 1000, lr = 0.00931012
I0328 17:16:20.720006  1546 solver.cpp:237] Iteration 1100, loss = 0.14897
I0328 17:16:20.720029  1546 solver.cpp:253]     Train net output #0: loss = 0.14897 (* 1 = 0.14897 loss)
I0328 17:16:20.720041  1546 sgd_solver.cpp:106] Iteration 1100, lr = 0.00924715
I0328 17:16:20.791941  1546 solver.cpp:237] Iteration 1200, loss = 0.250981
I0328 17:16:20.791962  1546 solver.cpp:253]     Train net output #0: loss = 0.250981 (* 1 = 0.250981 loss)
I0328 17:16:20.791976  1546 sgd_solver.cpp:106] Iteration 1200, lr = 0.00918515
I0328 17:16:20.860352  1546 solver.cpp:237] Iteration 1300, loss = 0.189204
I0328 17:16:20.860374  1546 solver.cpp:253]     Train net output #0: loss = 0.189204 (* 1 = 0.189204 loss)
I0328 17:16:20.860380  1546 sgd_solver.cpp:106] Iteration 1300, lr = 0.00912412
I0328 17:16:20.929229  1546 solver.cpp:237] Iteration 1400, loss = 0.175143
I0328 17:16:20.929251  1546 solver.cpp:253]     Train net output #0: loss = 0.175143 (* 1 = 0.175143 loss)
I0328 17:16:20.929265  1546 sgd_solver.cpp:106] Iteration 1400, lr = 0.00906403
I0328 17:16:20.997301  1546 solver.cpp:341] Iteration 1500, Testing net (#0)
I0328 17:16:21.066861  1546 solver.cpp:409]     Test net output #0: accuracy = 0.9233
I0328 17:16:21.066890  1546 solver.cpp:409]     Test net output #1: loss = 0.252711 (* 1 = 0.252711 loss)
I0328 17:16:21.067314  1546 solver.cpp:237] Iteration 1500, loss = 0.254451
I0328 17:16:21.067330  1546 solver.cpp:253]     Train net output #0: loss = 0.25445 (* 1 = 0.25445 loss)
I0328 17:16:21.067337  1546 sgd_solver.cpp:106] Iteration 1500, lr = 0.00900485
I0328 17:16:21.477249  1546 solver.cpp:237] Iteration 1600, loss = 0.568206
I0328 17:16:21.477283  1546 solver.cpp:253]     Train net output #0: loss = 0.568206 (* 1 = 0.568206 loss)
I0328 17:16:21.477290  1546 sgd_solver.cpp:106] Iteration 1600, lr = 0.00894657
I0328 17:16:22.387199  1546 solver.cpp:237] Iteration 1700, loss = 0.128677
I0328 17:16:22.387236  1546 solver.cpp:253]     Train net output #0: loss = 0.128677 (* 1 = 0.128677 loss)
I0328 17:16:22.387244  1546 sgd_solver.cpp:106] Iteration 1700, lr = 0.00888916
I0328 17:16:22.485055  1546 solver.cpp:237] Iteration 1800, loss = 0.182772
I0328 17:16:22.485095  1546 solver.cpp:253]     Train net output #0: loss = 0.182772 (* 1 = 0.182772 loss)
I0328 17:16:22.485119  1546 sgd_solver.cpp:106] Iteration 1800, lr = 0.0088326
I0328 17:16:22.558543  1546 solver.cpp:237] Iteration 1900, loss = 0.245364
I0328 17:16:22.558578  1546 solver.cpp:253]     Train net output #0: loss = 0.245364 (* 1 = 0.245364 loss)
I0328 17:16:22.558593  1546 sgd_solver.cpp:106] Iteration 1900, lr = 0.00877687
I0328 17:16:22.630210  1546 solver.cpp:341] Iteration 2000, Testing net (#0)
I0328 17:16:22.711019  1546 solver.cpp:409]     Test net output #0: accuracy = 0.9056
I0328 17:16:22.711112  1546 solver.cpp:409]     Test net output #1: loss = 0.305014 (* 1 = 0.305014 loss)
I0328 17:16:22.711678  1546 solver.cpp:237] Iteration 2000, loss = 0.256539
I0328 17:16:22.711743  1546 solver.cpp:253]     Train net output #0: loss = 0.256539 (* 1 = 0.256539 loss)
I0328 17:16:22.711781  1546 sgd_solver.cpp:106] Iteration 2000, lr = 0.00872196
I0328 17:16:22.783920  1546 solver.cpp:237] Iteration 2100, loss = 0.135086
I0328 17:16:22.784008  1546 solver.cpp:253]     Train net output #0: loss = 0.135086 (* 1 = 0.135086 loss)
I0328 17:16:22.784070  1546 sgd_solver.cpp:106] Iteration 2100, lr = 0.00866784
I0328 17:16:22.855432  1546 solver.cpp:237] Iteration 2200, loss = 0.233005
I0328 17:16:22.855466  1546 solver.cpp:253]     Train net output #0: loss = 0.233005 (* 1 = 0.233005 loss)
I0328 17:16:22.855473  1546 sgd_solver.cpp:106] Iteration 2200, lr = 0.0086145
I0328 17:16:22.924105  1546 solver.cpp:237] Iteration 2300, loss = 0.382437
I0328 17:16:22.924142  1546 solver.cpp:253]     Train net output #0: loss = 0.382437 (* 1 = 0.382437 loss)
I0328 17:16:22.924152  1546 sgd_solver.cpp:106] Iteration 2300, lr = 0.00856192
I0328 17:16:22.992804  1546 solver.cpp:237] Iteration 2400, loss = 0.0951937
I0328 17:16:22.992838  1546 solver.cpp:253]     Train net output #0: loss = 0.0951934 (* 1 = 0.0951934 loss)
I0328 17:16:22.992849  1546 sgd_solver.cpp:106] Iteration 2400, lr = 0.00851008
I0328 17:16:23.060926  1546 solver.cpp:341] Iteration 2500, Testing net (#0)
I0328 17:16:23.133328  1546 solver.cpp:409]     Test net output #0: accuracy = 0.9285
I0328 17:16:23.133369  1546 solver.cpp:409]     Test net output #1: loss = 0.235293 (* 1 = 0.235293 loss)
I0328 17:16:23.133838  1546 solver.cpp:237] Iteration 2500, loss = 0.241855
I0328 17:16:23.133858  1546 solver.cpp:253]     Train net output #0: loss = 0.241855 (* 1 = 0.241855 loss)
I0328 17:16:23.133913  1546 sgd_solver.cpp:106] Iteration 2500, lr = 0.00845897
I0328 17:16:23.202445  1546 solver.cpp:237] Iteration 2600, loss = 0.307972
I0328 17:16:23.202481  1546 solver.cpp:253]     Train net output #0: loss = 0.307972 (* 1 = 0.307972 loss)
I0328 17:16:23.202497  1546 sgd_solver.cpp:106] Iteration 2600, lr = 0.00840857
I0328 17:16:23.276828  1546 solver.cpp:237] Iteration 2700, loss = 0.417786
I0328 17:16:23.276860  1546 solver.cpp:253]     Train net output #0: loss = 0.417786 (* 1 = 0.417786 loss)
I0328 17:16:23.276877  1546 sgd_solver.cpp:106] Iteration 2700, lr = 0.00835886
I0328 17:16:23.344586  1546 solver.cpp:237] Iteration 2800, loss = 0.08866
I0328 17:16:23.344610  1546 solver.cpp:253]     Train net output #0: loss = 0.0886599 (* 1 = 0.0886599 loss)
I0328 17:16:23.344621  1546 sgd_solver.cpp:106] Iteration 2800, lr = 0.00830984
I0328 17:16:23.412518  1546 solver.cpp:237] Iteration 2900, loss = 0.247751
I0328 17:16:23.412538  1546 solver.cpp:253]     Train net output #0: loss = 0.24775 (* 1 = 0.24775 loss)
I0328 17:16:23.412544  1546 sgd_solver.cpp:106] Iteration 2900, lr = 0.00826148
I0328 17:16:23.479734  1546 solver.cpp:341] Iteration 3000, Testing net (#0)
I0328 17:16:23.572902  1546 blocking_queue.cpp:50] Data layer prefetch queue empty
I0328 17:16:23.580513  1546 solver.cpp:409]     Test net output #0: accuracy = 0.915
I0328 17:16:23.580541  1546 solver.cpp:409]     Test net output #1: loss = 0.28694 (* 1 = 0.28694 loss)
I0328 17:16:23.581024  1546 solver.cpp:237] Iteration 3000, loss = 0.265393
I0328 17:16:23.581056  1546 solver.cpp:253]     Train net output #0: loss = 0.265393 (* 1 = 0.265393 loss)
I0328 17:16:23.581071  1546 sgd_solver.cpp:106] Iteration 3000, lr = 0.00821377
I0328 17:16:23.650744  1546 solver.cpp:237] Iteration 3100, loss = 0.174144
I0328 17:16:23.650769  1546 solver.cpp:253]     Train net output #0: loss = 0.174144 (* 1 = 0.174144 loss)
I0328 17:16:23.650782  1546 sgd_solver.cpp:106] Iteration 3100, lr = 0.0081667
I0328 17:16:23.720676  1546 solver.cpp:237] Iteration 3200, loss = 0.184481
I0328 17:16:23.720700  1546 solver.cpp:253]     Train net output #0: loss = 0.184481 (* 1 = 0.184481 loss)
I0328 17:16:23.720706  1546 sgd_solver.cpp:106] Iteration 3200, lr = 0.00812025
I0328 17:16:23.792309  1546 solver.cpp:237] Iteration 3300, loss = 0.124572
I0328 17:16:23.792336  1546 solver.cpp:253]     Train net output #0: loss = 0.124571 (* 1 = 0.124571 loss)
I0328 17:16:23.792342  1546 sgd_solver.cpp:106] Iteration 3300, lr = 0.00807442
I0328 17:16:24.773962  1546 solver.cpp:237] Iteration 3400, loss = 0.170004
I0328 17:16:24.773993  1546 solver.cpp:253]     Train net output #0: loss = 0.170004 (* 1 = 0.170004 loss)
I0328 17:16:24.773998  1546 sgd_solver.cpp:106] Iteration 3400, lr = 0.00802918
I0328 17:16:25.120262  1546 solver.cpp:341] Iteration 3500, Testing net (#0)
I0328 17:16:25.226021  1546 solver.cpp:409]     Test net output #0: accuracy = 0.9303
I0328 17:16:25.226075  1546 solver.cpp:409]     Test net output #1: loss = 0.229456 (* 1 = 0.229456 loss)
I0328 17:16:25.226856  1546 solver.cpp:237] Iteration 3500, loss = 0.0819112
I0328 17:16:25.226903  1546 solver.cpp:253]     Train net output #0: loss = 0.081911 (* 1 = 0.081911 loss)
I0328 17:16:25.226935  1546 sgd_solver.cpp:106] Iteration 3500, lr = 0.00798454
I0328 17:16:25.334202  1546 solver.cpp:237] Iteration 3600, loss = 0.557807
I0328 17:16:25.334255  1546 solver.cpp:253]     Train net output #0: loss = 0.557807 (* 1 = 0.557807 loss)
I0328 17:16:25.334270  1546 sgd_solver.cpp:106] Iteration 3600, lr = 0.00794046
I0328 17:16:25.416120  1546 solver.cpp:237] Iteration 3700, loss = 0.172254
I0328 17:16:25.416165  1546 solver.cpp:253]     Train net output #0: loss = 0.172254 (* 1 = 0.172254 loss)
I0328 17:16:25.416177  1546 sgd_solver.cpp:106] Iteration 3700, lr = 0.00789695
I0328 17:16:25.488981  1546 solver.cpp:237] Iteration 3800, loss = 0.130637
I0328 17:16:25.489017  1546 solver.cpp:253]     Train net output #0: loss = 0.130637 (* 1 = 0.130637 loss)
I0328 17:16:25.489027  1546 sgd_solver.cpp:106] Iteration 3800, lr = 0.007854
I0328 17:16:25.561266  1546 solver.cpp:237] Iteration 3900, loss = 0.135744
I0328 17:16:25.561300  1546 solver.cpp:253]     Train net output #0: loss = 0.135744 (* 1 = 0.135744 loss)
I0328 17:16:25.561305  1546 sgd_solver.cpp:106] Iteration 3900, lr = 0.00781158
I0328 17:16:25.632977  1546 solver.cpp:341] Iteration 4000, Testing net (#0)
I0328 17:16:25.700495  1546 solver.cpp:409]     Test net output #0: accuracy = 0.9351
I0328 17:16:25.700552  1546 solver.cpp:409]     Test net output #1: loss = 0.224281 (* 1 = 0.224281 loss)
I0328 17:16:25.701079  1546 solver.cpp:237] Iteration 4000, loss = 0.293653
I0328 17:16:25.701100  1546 solver.cpp:253]     Train net output #0: loss = 0.293653 (* 1 = 0.293653 loss)
I0328 17:16:25.701113  1546 sgd_solver.cpp:106] Iteration 4000, lr = 0.0077697
I0328 17:16:25.773396  1546 solver.cpp:237] Iteration 4100, loss = 0.0829233
I0328 17:16:25.773483  1546 solver.cpp:253]     Train net output #0: loss = 0.082923 (* 1 = 0.082923 loss)
I0328 17:16:25.773512  1546 sgd_solver.cpp:106] Iteration 4100, lr = 0.00772833
I0328 17:16:25.845662  1546 solver.cpp:237] Iteration 4200, loss = 0.0947402
I0328 17:16:25.845746  1546 solver.cpp:253]     Train net output #0: loss = 0.0947399 (* 1 = 0.0947399 loss)
I0328 17:16:25.845783  1546 sgd_solver.cpp:106] Iteration 4200, lr = 0.00768748
I0328 17:16:25.917958  1546 solver.cpp:237] Iteration 4300, loss = 0.2943
I0328 17:16:25.918000  1546 solver.cpp:253]     Train net output #0: loss = 0.294299 (* 1 = 0.294299 loss)
I0328 17:16:25.918010  1546 sgd_solver.cpp:106] Iteration 4300, lr = 0.00764712
I0328 17:16:25.996493  1546 solver.cpp:237] Iteration 4400, loss = 0.103251
I0328 17:16:25.996522  1546 solver.cpp:253]     Train net output #0: loss = 0.103251 (* 1 = 0.103251 loss)
I0328 17:16:25.996528  1546 sgd_solver.cpp:106] Iteration 4400, lr = 0.00760726
I0328 17:16:26.063567  1546 solver.cpp:341] Iteration 4500, Testing net (#0)
I0328 17:16:26.130528  1546 solver.cpp:409]     Test net output #0: accuracy = 0.9242
I0328 17:16:26.130558  1546 solver.cpp:409]     Test net output #1: loss = 0.249809 (* 1 = 0.249809 loss)
I0328 17:16:26.130987  1546 solver.cpp:237] Iteration 4500, loss = 0.208729
I0328 17:16:26.131006  1546 solver.cpp:253]     Train net output #0: loss = 0.208729 (* 1 = 0.208729 loss)
I0328 17:16:26.131019  1546 sgd_solver.cpp:106] Iteration 4500, lr = 0.00756788
I0328 17:16:26.198910  1546 solver.cpp:237] Iteration 4600, loss = 0.139338
I0328 17:16:26.198936  1546 solver.cpp:253]     Train net output #0: loss = 0.139338 (* 1 = 0.139338 loss)
I0328 17:16:26.198947  1546 sgd_solver.cpp:106] Iteration 4600, lr = 0.00752897
I0328 17:16:26.266512  1546 solver.cpp:237] Iteration 4700, loss = 0.245488
I0328 17:16:26.266532  1546 solver.cpp:253]     Train net output #0: loss = 0.245487 (* 1 = 0.245487 loss)
I0328 17:16:26.266566  1546 sgd_solver.cpp:106] Iteration 4700, lr = 0.00749052
I0328 17:16:26.334280  1546 solver.cpp:237] Iteration 4800, loss = 0.409705
I0328 17:16:26.334300  1546 solver.cpp:253]     Train net output #0: loss = 0.409704 (* 1 = 0.409704 loss)
I0328 17:16:26.334311  1546 sgd_solver.cpp:106] Iteration 4800, lr = 0.00745253
I0328 17:16:26.402173  1546 solver.cpp:237] Iteration 4900, loss = 0.169541
I0328 17:16:26.402192  1546 solver.cpp:253]     Train net output #0: loss = 0.16954 (* 1 = 0.16954 loss)
I0328 17:16:26.402201  1546 sgd_solver.cpp:106] Iteration 4900, lr = 0.00741498
I0328 17:16:26.469871  1546 solver.cpp:459] Snapshotting to binary proto file examples/mlp/mlp_iter_5000.caffemodel
I0328 17:16:26.470649  1546 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mlp/mlp_iter_5000.solverstate
I0328 17:16:26.470890  1546 solver.cpp:341] Iteration 5000, Testing net (#0)
I0328 17:16:26.543462  1546 solver.cpp:409]     Test net output #0: accuracy = 0.9432
I0328 17:16:26.543494  1546 solver.cpp:409]     Test net output #1: loss = 0.188015 (* 1 = 0.188015 loss)
I0328 17:16:26.543939  1546 solver.cpp:237] Iteration 5000, loss = 0.252839
I0328 17:16:26.543972  1546 solver.cpp:253]     Train net output #0: loss = 0.252839 (* 1 = 0.252839 loss)
I0328 17:16:26.543992  1546 sgd_solver.cpp:106] Iteration 5000, lr = 0.00737788
I0328 17:16:26.612529  1546 solver.cpp:237] Iteration 5100, loss = 0.23587
I0328 17:16:26.612557  1546 solver.cpp:253]     Train net output #0: loss = 0.23587 (* 1 = 0.23587 loss)
I0328 17:16:26.612566  1546 sgd_solver.cpp:106] Iteration 5100, lr = 0.0073412
I0328 17:16:27.441407  1546 solver.cpp:237] Iteration 5200, loss = 0.0973599
I0328 17:16:27.441443  1546 solver.cpp:253]     Train net output #0: loss = 0.0973596 (* 1 = 0.0973596 loss)
I0328 17:16:27.441452  1546 sgd_solver.cpp:106] Iteration 5200, lr = 0.00730495
I0328 17:16:27.931069  1546 solver.cpp:237] Iteration 5300, loss = 0.223632
I0328 17:16:27.931107  1546 solver.cpp:253]     Train net output #0: loss = 0.223632 (* 1 = 0.223632 loss)
I0328 17:16:27.931116  1546 sgd_solver.cpp:106] Iteration 5300, lr = 0.00726911
I0328 17:16:27.999168  1546 solver.cpp:237] Iteration 5400, loss = 0.290328
I0328 17:16:27.999199  1546 solver.cpp:253]     Train net output #0: loss = 0.290327 (* 1 = 0.290327 loss)
I0328 17:16:27.999213  1546 sgd_solver.cpp:106] Iteration 5400, lr = 0.00723368
I0328 17:16:28.066324  1546 solver.cpp:341] Iteration 5500, Testing net (#0)
I0328 17:16:28.079799  1546 blocking_queue.cpp:50] Data layer prefetch queue empty
I0328 17:16:28.139819  1546 solver.cpp:409]     Test net output #0: accuracy = 0.9371
I0328 17:16:28.139852  1546 solver.cpp:409]     Test net output #1: loss = 0.204462 (* 1 = 0.204462 loss)
I0328 17:16:28.140462  1546 solver.cpp:237] Iteration 5500, loss = 0.155611
I0328 17:16:28.140496  1546 solver.cpp:253]     Train net output #0: loss = 0.155611 (* 1 = 0.155611 loss)
I0328 17:16:28.140519  1546 sgd_solver.cpp:106] Iteration 5500, lr = 0.00719865
I0328 17:16:28.213470  1546 solver.cpp:237] Iteration 5600, loss = 0.0382203
I0328 17:16:28.213510  1546 solver.cpp:253]     Train net output #0: loss = 0.03822 (* 1 = 0.03822 loss)
I0328 17:16:28.213517  1546 sgd_solver.cpp:106] Iteration 5600, lr = 0.00716402
I0328 17:16:28.283032  1546 solver.cpp:237] Iteration 5700, loss = 0.0861406
I0328 17:16:28.283066  1546 solver.cpp:253]     Train net output #0: loss = 0.0861403 (* 1 = 0.0861403 loss)
I0328 17:16:28.283076  1546 sgd_solver.cpp:106] Iteration 5700, lr = 0.00712977
I0328 17:16:28.351557  1546 solver.cpp:237] Iteration 5800, loss = 0.193229
I0328 17:16:28.351588  1546 solver.cpp:253]     Train net output #0: loss = 0.193228 (* 1 = 0.193228 loss)
I0328 17:16:28.351596  1546 sgd_solver.cpp:106] Iteration 5800, lr = 0.0070959
I0328 17:16:28.420011  1546 solver.cpp:237] Iteration 5900, loss = 0.107974
I0328 17:16:28.420042  1546 solver.cpp:253]     Train net output #0: loss = 0.107974 (* 1 = 0.107974 loss)
I0328 17:16:28.420070  1546 sgd_solver.cpp:106] Iteration 5900, lr = 0.0070624
I0328 17:16:28.487975  1546 solver.cpp:341] Iteration 6000, Testing net (#0)
I0328 17:16:28.563021  1546 solver.cpp:409]     Test net output #0: accuracy = 0.9413
I0328 17:16:28.563051  1546 solver.cpp:409]     Test net output #1: loss = 0.187283 (* 1 = 0.187283 loss)
I0328 17:16:28.563446  1546 solver.cpp:237] Iteration 6000, loss = 0.144094
I0328 17:16:28.563463  1546 solver.cpp:253]     Train net output #0: loss = 0.144094 (* 1 = 0.144094 loss)
I0328 17:16:28.563472  1546 sgd_solver.cpp:106] Iteration 6000, lr = 0.00702927
I0328 17:16:28.632064  1546 solver.cpp:237] Iteration 6100, loss = 0.134899
I0328 17:16:28.632097  1546 solver.cpp:253]     Train net output #0: loss = 0.134899 (* 1 = 0.134899 loss)
I0328 17:16:28.632102  1546 sgd_solver.cpp:106] Iteration 6100, lr = 0.0069965
I0328 17:16:28.700435  1546 solver.cpp:237] Iteration 6200, loss = 0.169557
I0328 17:16:28.700466  1546 solver.cpp:253]     Train net output #0: loss = 0.169557 (* 1 = 0.169557 loss)
I0328 17:16:28.700474  1546 sgd_solver.cpp:106] Iteration 6200, lr = 0.00696408
I0328 17:16:28.768961  1546 solver.cpp:237] Iteration 6300, loss = 0.164336
I0328 17:16:28.768993  1546 solver.cpp:253]     Train net output #0: loss = 0.164336 (* 1 = 0.164336 loss)
I0328 17:16:28.769001  1546 sgd_solver.cpp:106] Iteration 6300, lr = 0.00693201
I0328 17:16:28.837205  1546 solver.cpp:237] Iteration 6400, loss = 0.2389
I0328 17:16:28.837236  1546 solver.cpp:253]     Train net output #0: loss = 0.2389 (* 1 = 0.2389 loss)
I0328 17:16:28.837242  1546 sgd_solver.cpp:106] Iteration 6400, lr = 0.00690029
I0328 17:16:28.911872  1546 solver.cpp:341] Iteration 6500, Testing net (#0)
I0328 17:16:28.963568  1546 solver.cpp:409]     Test net output #0: accuracy = 0.9309
I0328 17:16:28.963595  1546 solver.cpp:409]     Test net output #1: loss = 0.223852 (* 1 = 0.223852 loss)
I0328 17:16:28.964087  1546 solver.cpp:237] Iteration 6500, loss = 0.115672
I0328 17:16:28.964107  1546 solver.cpp:253]     Train net output #0: loss = 0.115672 (* 1 = 0.115672 loss)
I0328 17:16:28.964120  1546 sgd_solver.cpp:106] Iteration 6500, lr = 0.0068689
I0328 17:16:29.036661  1546 solver.cpp:237] Iteration 6600, loss = 0.302162
I0328 17:16:29.036686  1546 solver.cpp:253]     Train net output #0: loss = 0.302161 (* 1 = 0.302161 loss)
I0328 17:16:29.036697  1546 sgd_solver.cpp:106] Iteration 6600, lr = 0.00683784
I0328 17:16:29.109287  1546 solver.cpp:237] Iteration 6700, loss = 0.288262
I0328 17:16:29.109309  1546 solver.cpp:253]     Train net output #0: loss = 0.288262 (* 1 = 0.288262 loss)
I0328 17:16:29.109318  1546 sgd_solver.cpp:106] Iteration 6700, lr = 0.00680711
I0328 17:16:29.181797  1546 solver.cpp:237] Iteration 6800, loss = 0.276896
I0328 17:16:29.181823  1546 solver.cpp:253]     Train net output #0: loss = 0.276896 (* 1 = 0.276896 loss)
I0328 17:16:29.181833  1546 sgd_solver.cpp:106] Iteration 6800, lr = 0.0067767
I0328 17:16:29.250447  1546 solver.cpp:237] Iteration 6900, loss = 0.225502
I0328 17:16:29.250465  1546 solver.cpp:253]     Train net output #0: loss = 0.225501 (* 1 = 0.225501 loss)
I0328 17:16:29.250471  1546 sgd_solver.cpp:106] Iteration 6900, lr = 0.0067466
I0328 17:16:29.317600  1546 solver.cpp:341] Iteration 7000, Testing net (#0)
I0328 17:16:29.390332  1546 solver.cpp:409]     Test net output #0: accuracy = 0.9376
I0328 17:16:29.390362  1546 solver.cpp:409]     Test net output #1: loss = 0.199945 (* 1 = 0.199945 loss)
I0328 17:16:29.390857  1546 solver.cpp:237] Iteration 7000, loss = 0.0218196
I0328 17:16:29.390877  1546 solver.cpp:253]     Train net output #0: loss = 0.0218192 (* 1 = 0.0218192 loss)
I0328 17:16:29.390887  1546 sgd_solver.cpp:106] Iteration 7000, lr = 0.00671681
I0328 17:16:29.463063  1546 solver.cpp:237] Iteration 7100, loss = 0.27565
I0328 17:16:29.463083  1546 solver.cpp:253]     Train net output #0: loss = 0.27565 (* 1 = 0.27565 loss)
I0328 17:16:29.463089  1546 sgd_solver.cpp:106] Iteration 7100, lr = 0.00668733
I0328 17:16:29.536378  1546 solver.cpp:237] Iteration 7200, loss = 0.118176
I0328 17:16:29.536425  1546 solver.cpp:253]     Train net output #0: loss = 0.118176 (* 1 = 0.118176 loss)
I0328 17:16:29.536437  1546 sgd_solver.cpp:106] Iteration 7200, lr = 0.00665815
I0328 17:16:29.605160  1546 solver.cpp:237] Iteration 7300, loss = 0.510169
I0328 17:16:29.605182  1546 solver.cpp:253]     Train net output #0: loss = 0.510168 (* 1 = 0.510168 loss)
I0328 17:16:29.605187  1546 sgd_solver.cpp:106] Iteration 7300, lr = 0.00662927
I0328 17:16:29.672955  1546 solver.cpp:237] Iteration 7400, loss = 0.318172
I0328 17:16:29.672974  1546 solver.cpp:253]     Train net output #0: loss = 0.318172 (* 1 = 0.318172 loss)
I0328 17:16:29.672981  1546 sgd_solver.cpp:106] Iteration 7400, lr = 0.00660067
I0328 17:16:29.740059  1546 solver.cpp:341] Iteration 7500, Testing net (#0)
I0328 17:16:29.807027  1546 solver.cpp:409]     Test net output #0: accuracy = 0.926
I0328 17:16:29.807055  1546 solver.cpp:409]     Test net output #1: loss = 0.242 (* 1 = 0.242 loss)
I0328 17:16:29.807462  1546 solver.cpp:237] Iteration 7500, loss = 0.105555
I0328 17:16:29.807478  1546 solver.cpp:253]     Train net output #0: loss = 0.105555 (* 1 = 0.105555 loss)
I0328 17:16:29.807487  1546 sgd_solver.cpp:106] Iteration 7500, lr = 0.00657236
I0328 17:16:29.875360  1546 solver.cpp:237] Iteration 7600, loss = 0.213149
I0328 17:16:29.875382  1546 solver.cpp:253]     Train net output #0: loss = 0.213148 (* 1 = 0.213148 loss)
I0328 17:16:29.875388  1546 sgd_solver.cpp:106] Iteration 7600, lr = 0.00654433
I0328 17:16:29.943159  1546 solver.cpp:237] Iteration 7700, loss = 0.0953238
I0328 17:16:29.943186  1546 solver.cpp:253]     Train net output #0: loss = 0.0953235 (* 1 = 0.0953235 loss)
I0328 17:16:29.943192  1546 sgd_solver.cpp:106] Iteration 7700, lr = 0.00651658
I0328 17:16:30.010825  1546 solver.cpp:237] Iteration 7800, loss = 0.23455
I0328 17:16:30.010846  1546 solver.cpp:253]     Train net output #0: loss = 0.234549 (* 1 = 0.234549 loss)
I0328 17:16:30.010851  1546 sgd_solver.cpp:106] Iteration 7800, lr = 0.00648911
I0328 17:16:30.078218  1546 solver.cpp:237] Iteration 7900, loss = 0.232213
I0328 17:16:30.078238  1546 solver.cpp:253]     Train net output #0: loss = 0.232213 (* 1 = 0.232213 loss)
I0328 17:16:30.078243  1546 sgd_solver.cpp:106] Iteration 7900, lr = 0.0064619
I0328 17:16:30.145326  1546 solver.cpp:341] Iteration 8000, Testing net (#0)
I0328 17:16:30.687399  1546 solver.cpp:409]     Test net output #0: accuracy = 0.9452
I0328 17:16:30.687434  1546 solver.cpp:409]     Test net output #1: loss = 0.18128 (* 1 = 0.18128 loss)
I0328 17:16:30.687873  1546 solver.cpp:237] Iteration 8000, loss = 0.163705
I0328 17:16:30.687891  1546 solver.cpp:253]     Train net output #0: loss = 0.163705 (* 1 = 0.163705 loss)
I0328 17:16:30.687913  1546 sgd_solver.cpp:106] Iteration 8000, lr = 0.00643496
I0328 17:16:31.459238  1546 solver.cpp:237] Iteration 8100, loss = 0.0642411
I0328 17:16:31.459270  1546 solver.cpp:253]     Train net output #0: loss = 0.0642407 (* 1 = 0.0642407 loss)
I0328 17:16:31.459280  1546 sgd_solver.cpp:106] Iteration 8100, lr = 0.00640827
I0328 17:16:31.527065  1546 solver.cpp:237] Iteration 8200, loss = 0.174382
I0328 17:16:31.527099  1546 solver.cpp:253]     Train net output #0: loss = 0.174382 (* 1 = 0.174382 loss)
I0328 17:16:31.527114  1546 sgd_solver.cpp:106] Iteration 8200, lr = 0.00638185
I0328 17:16:31.596071  1546 solver.cpp:237] Iteration 8300, loss = 0.245978
I0328 17:16:31.596113  1546 solver.cpp:253]     Train net output #0: loss = 0.245978 (* 1 = 0.245978 loss)
I0328 17:16:31.596124  1546 sgd_solver.cpp:106] Iteration 8300, lr = 0.00635567
I0328 17:16:31.665638  1546 solver.cpp:237] Iteration 8400, loss = 0.178883
I0328 17:16:31.665678  1546 solver.cpp:253]     Train net output #0: loss = 0.178883 (* 1 = 0.178883 loss)
I0328 17:16:31.665693  1546 sgd_solver.cpp:106] Iteration 8400, lr = 0.00632975
I0328 17:16:31.732964  1546 solver.cpp:341] Iteration 8500, Testing net (#0)
I0328 17:16:31.807351  1546 solver.cpp:409]     Test net output #0: accuracy = 0.9445
I0328 17:16:31.807466  1546 solver.cpp:409]     Test net output #1: loss = 0.177128 (* 1 = 0.177128 loss)
I0328 17:16:31.808018  1546 solver.cpp:237] Iteration 8500, loss = 0.0952057
I0328 17:16:31.808061  1546 solver.cpp:253]     Train net output #0: loss = 0.0952054 (* 1 = 0.0952054 loss)
I0328 17:16:31.808091  1546 sgd_solver.cpp:106] Iteration 8500, lr = 0.00630407
I0328 17:16:31.879827  1546 solver.cpp:237] Iteration 8600, loss = 0.0308048
I0328 17:16:31.879910  1546 solver.cpp:253]     Train net output #0: loss = 0.0308044 (* 1 = 0.0308044 loss)
I0328 17:16:31.879933  1546 sgd_solver.cpp:106] Iteration 8600, lr = 0.00627864
I0328 17:16:31.951887  1546 solver.cpp:237] Iteration 8700, loss = 0.154671
I0328 17:16:31.951932  1546 solver.cpp:253]     Train net output #0: loss = 0.154671 (* 1 = 0.154671 loss)
I0328 17:16:31.951942  1546 sgd_solver.cpp:106] Iteration 8700, lr = 0.00625344
I0328 17:16:32.023759  1546 solver.cpp:237] Iteration 8800, loss = 0.145631
I0328 17:16:32.023794  1546 solver.cpp:253]     Train net output #0: loss = 0.145631 (* 1 = 0.145631 loss)
I0328 17:16:32.023800  1546 sgd_solver.cpp:106] Iteration 8800, lr = 0.00622847
I0328 17:16:32.095625  1546 solver.cpp:237] Iteration 8900, loss = 0.094443
I0328 17:16:32.095713  1546 solver.cpp:253]     Train net output #0: loss = 0.0944427 (* 1 = 0.0944427 loss)
I0328 17:16:32.095737  1546 sgd_solver.cpp:106] Iteration 8900, lr = 0.00620374
I0328 17:16:32.166965  1546 solver.cpp:341] Iteration 9000, Testing net (#0)
I0328 17:16:32.175732  1546 blocking_queue.cpp:50] Data layer prefetch queue empty
I0328 17:16:32.262662  1546 solver.cpp:409]     Test net output #0: accuracy = 0.9496
I0328 17:16:32.262699  1546 solver.cpp:409]     Test net output #1: loss = 0.170727 (* 1 = 0.170727 loss)
I0328 17:16:32.263134  1546 solver.cpp:237] Iteration 9000, loss = 0.204422
I0328 17:16:32.263164  1546 solver.cpp:253]     Train net output #0: loss = 0.204422 (* 1 = 0.204422 loss)
I0328 17:16:32.263178  1546 sgd_solver.cpp:106] Iteration 9000, lr = 0.00617924
I0328 17:16:32.334698  1546 solver.cpp:237] Iteration 9100, loss = 0.355325
I0328 17:16:32.334733  1546 solver.cpp:253]     Train net output #0: loss = 0.355325 (* 1 = 0.355325 loss)
I0328 17:16:32.334739  1546 sgd_solver.cpp:106] Iteration 9100, lr = 0.00615496
I0328 17:16:32.406361  1546 solver.cpp:237] Iteration 9200, loss = 0.0441621
I0328 17:16:32.406404  1546 solver.cpp:253]     Train net output #0: loss = 0.0441617 (* 1 = 0.0441617 loss)
I0328 17:16:32.406421  1546 sgd_solver.cpp:106] Iteration 9200, lr = 0.0061309
I0328 17:16:32.477957  1546 solver.cpp:237] Iteration 9300, loss = 0.140848
I0328 17:16:32.477998  1546 solver.cpp:253]     Train net output #0: loss = 0.140848 (* 1 = 0.140848 loss)
I0328 17:16:32.478006  1546 sgd_solver.cpp:106] Iteration 9300, lr = 0.00610706
I0328 17:16:32.553692  1546 solver.cpp:237] Iteration 9400, loss = 0.219135
I0328 17:16:32.553722  1546 solver.cpp:253]     Train net output #0: loss = 0.219135 (* 1 = 0.219135 loss)
I0328 17:16:32.553727  1546 sgd_solver.cpp:106] Iteration 9400, lr = 0.00608343
I0328 17:16:32.620915  1546 solver.cpp:341] Iteration 9500, Testing net (#0)
I0328 17:16:32.672351  1546 solver.cpp:409]     Test net output #0: accuracy = 0.9455
I0328 17:16:32.672376  1546 solver.cpp:409]     Test net output #1: loss = 0.177637 (* 1 = 0.177637 loss)
I0328 17:16:32.672780  1546 solver.cpp:237] Iteration 9500, loss = 0.132044
I0328 17:16:32.672796  1546 solver.cpp:253]     Train net output #0: loss = 0.132044 (* 1 = 0.132044 loss)
I0328 17:16:32.672804  1546 sgd_solver.cpp:106] Iteration 9500, lr = 0.00606002
I0328 17:16:32.740409  1546 solver.cpp:237] Iteration 9600, loss = 0.0750558
I0328 17:16:32.740428  1546 solver.cpp:253]     Train net output #0: loss = 0.0750554 (* 1 = 0.0750554 loss)
I0328 17:16:32.740434  1546 sgd_solver.cpp:106] Iteration 9600, lr = 0.00603682
I0328 17:16:32.807852  1546 solver.cpp:237] Iteration 9700, loss = 0.141793
I0328 17:16:32.807873  1546 solver.cpp:253]     Train net output #0: loss = 0.141793 (* 1 = 0.141793 loss)
I0328 17:16:32.807879  1546 sgd_solver.cpp:106] Iteration 9700, lr = 0.00601382
I0328 17:16:32.875468  1546 solver.cpp:237] Iteration 9800, loss = 0.344679
I0328 17:16:32.875488  1546 solver.cpp:253]     Train net output #0: loss = 0.344679 (* 1 = 0.344679 loss)
I0328 17:16:32.875494  1546 sgd_solver.cpp:106] Iteration 9800, lr = 0.00599102
I0328 17:16:32.943058  1546 solver.cpp:237] Iteration 9900, loss = 0.0395065
I0328 17:16:32.943078  1546 solver.cpp:253]     Train net output #0: loss = 0.0395062 (* 1 = 0.0395062 loss)
I0328 17:16:32.943084  1546 sgd_solver.cpp:106] Iteration 9900, lr = 0.00596843
I0328 17:16:33.010452  1546 solver.cpp:459] Snapshotting to binary proto file examples/mlp/mlp_iter_10000.caffemodel
I0328 17:16:33.011029  1546 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mlp/mlp_iter_10000.solverstate
I0328 17:16:33.011539  1546 solver.cpp:321] Iteration 10000, loss = 0.142482
I0328 17:16:33.011556  1546 solver.cpp:341] Iteration 10000, Testing net (#0)
I0328 17:16:33.061072  1546 solver.cpp:409]     Test net output #0: accuracy = 0.945
I0328 17:16:33.061103  1546 solver.cpp:409]     Test net output #1: loss = 0.184885 (* 1 = 0.184885 loss)
I0328 17:16:33.061108  1546 solver.cpp:326] Optimization Done.
I0328 17:16:33.061115  1546 caffe.cpp:215] Optimization Done.
