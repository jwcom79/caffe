I0428 19:41:01.597327 30618 caffe.cpp:184] Using GPUs 0
I0428 19:41:01.850019 30618 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 5000
base_lr: 0.01
display: 100
max_iter: 50000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mlp/mlp_test"
solver_mode: GPU
device_id: 0
net: "examples/mlp/mlp_train_test.prototxt"
I0428 19:41:01.850244 30618 solver.cpp:91] Creating training net from net file: examples/mlp/mlp_train_test.prototxt
I0428 19:41:01.850545 30618 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:41:01.850579 30618 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:41:01.850653 30618 net.cpp:49] Initializing net from parameters: 
name: "MLP"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "ip4"
  type: "InnerProduct"
  bottom: "data"
  top: "ip4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip4"
  bottom: "label"
  top: "loss"
}
I0428 19:41:01.850874 30618 layer_factory.hpp:77] Creating layer mnist
I0428 19:41:01.851529 30618 net.cpp:106] Creating Layer mnist
I0428 19:41:01.851567 30618 net.cpp:411] mnist -> data
I0428 19:41:01.851625 30618 net.cpp:411] mnist -> label
I0428 19:41:01.869560 30623 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:41:01.878473 30618 data_layer.cpp:41] output data size: 100,1,28,28
I0428 19:41:01.879587 30618 net.cpp:150] Setting up mnist
I0428 19:41:01.881289 30618 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0428 19:41:01.881345 30618 net.cpp:157] Top shape: 100 (100)
I0428 19:41:01.881363 30618 net.cpp:165] Memory required for data: 314000
I0428 19:41:01.881391 30618 layer_factory.hpp:77] Creating layer ip4
I0428 19:41:01.881420 30618 net.cpp:106] Creating Layer ip4
I0428 19:41:01.881441 30618 net.cpp:454] ip4 <- data
I0428 19:41:01.881474 30618 net.cpp:411] ip4 -> ip4
I0428 19:41:01.882398 30618 net.cpp:150] Setting up ip4
I0428 19:41:01.882441 30618 net.cpp:157] Top shape: 100 10 (1000)
I0428 19:41:01.882463 30618 net.cpp:165] Memory required for data: 318000
I0428 19:41:01.882501 30618 layer_factory.hpp:77] Creating layer loss
I0428 19:41:01.882534 30618 net.cpp:106] Creating Layer loss
I0428 19:41:01.882556 30618 net.cpp:454] loss <- ip4
I0428 19:41:01.882575 30618 net.cpp:454] loss <- label
I0428 19:41:01.882599 30618 net.cpp:411] loss -> loss
I0428 19:41:01.882637 30618 layer_factory.hpp:77] Creating layer loss
I0428 19:41:02.053012 30618 net.cpp:150] Setting up loss
I0428 19:41:02.053102 30618 net.cpp:157] Top shape: (1)
I0428 19:41:02.053122 30618 net.cpp:160]     with loss weight 1
I0428 19:41:02.053160 30618 net.cpp:165] Memory required for data: 318004
I0428 19:41:02.053181 30618 net.cpp:226] loss needs backward computation.
I0428 19:41:02.053201 30618 net.cpp:226] ip4 needs backward computation.
I0428 19:41:02.053220 30618 net.cpp:228] mnist does not need backward computation.
I0428 19:41:02.053238 30618 net.cpp:270] This network produces output loss
I0428 19:41:02.053263 30618 net.cpp:283] Network initialization done.
I0428 19:41:02.053526 30618 solver.cpp:181] Creating test net (#0) specified by net file: examples/mlp/mlp_train_test.prototxt
I0428 19:41:02.053571 30618 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:41:02.053650 30618 net.cpp:49] Initializing net from parameters: 
name: "MLP"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "ip4"
  type: "InnerProduct"
  bottom: "data"
  top: "ip4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip4"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip4"
  bottom: "label"
  top: "loss"
}
I0428 19:41:02.053912 30618 layer_factory.hpp:77] Creating layer mnist
I0428 19:41:02.054067 30618 net.cpp:106] Creating Layer mnist
I0428 19:41:02.054100 30618 net.cpp:411] mnist -> data
I0428 19:41:02.054141 30618 net.cpp:411] mnist -> label
I0428 19:41:02.055004 30625 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:41:02.055291 30618 data_layer.cpp:41] output data size: 100,1,28,28
I0428 19:41:02.056427 30618 net.cpp:150] Setting up mnist
I0428 19:41:02.056473 30618 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0428 19:41:02.056504 30618 net.cpp:157] Top shape: 100 (100)
I0428 19:41:02.056527 30618 net.cpp:165] Memory required for data: 314000
I0428 19:41:02.056552 30618 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:41:02.056584 30618 net.cpp:106] Creating Layer label_mnist_1_split
I0428 19:41:02.056612 30618 net.cpp:454] label_mnist_1_split <- label
I0428 19:41:02.056639 30618 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:41:02.056671 30618 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:41:02.056736 30618 net.cpp:150] Setting up label_mnist_1_split
I0428 19:41:02.056762 30618 net.cpp:157] Top shape: 100 (100)
I0428 19:41:02.056783 30618 net.cpp:157] Top shape: 100 (100)
I0428 19:41:02.056804 30618 net.cpp:165] Memory required for data: 314800
I0428 19:41:02.056825 30618 layer_factory.hpp:77] Creating layer ip4
I0428 19:41:02.056850 30618 net.cpp:106] Creating Layer ip4
I0428 19:41:02.056871 30618 net.cpp:454] ip4 <- data
I0428 19:41:02.056907 30618 net.cpp:411] ip4 -> ip4
I0428 19:41:02.057118 30618 net.cpp:150] Setting up ip4
I0428 19:41:02.057152 30618 net.cpp:157] Top shape: 100 10 (1000)
I0428 19:41:02.057176 30618 net.cpp:165] Memory required for data: 318800
I0428 19:41:02.057207 30618 layer_factory.hpp:77] Creating layer ip4_ip4_0_split
I0428 19:41:02.057235 30618 net.cpp:106] Creating Layer ip4_ip4_0_split
I0428 19:41:02.057257 30618 net.cpp:454] ip4_ip4_0_split <- ip4
I0428 19:41:02.057282 30618 net.cpp:411] ip4_ip4_0_split -> ip4_ip4_0_split_0
I0428 19:41:02.057318 30618 net.cpp:411] ip4_ip4_0_split -> ip4_ip4_0_split_1
I0428 19:41:02.057384 30618 net.cpp:150] Setting up ip4_ip4_0_split
I0428 19:41:02.057410 30618 net.cpp:157] Top shape: 100 10 (1000)
I0428 19:41:02.057433 30618 net.cpp:157] Top shape: 100 10 (1000)
I0428 19:41:02.057453 30618 net.cpp:165] Memory required for data: 326800
I0428 19:41:02.057472 30618 layer_factory.hpp:77] Creating layer accuracy
I0428 19:41:02.057498 30618 net.cpp:106] Creating Layer accuracy
I0428 19:41:02.057523 30618 net.cpp:454] accuracy <- ip4_ip4_0_split_0
I0428 19:41:02.057549 30618 net.cpp:454] accuracy <- label_mnist_1_split_0
I0428 19:41:02.057579 30618 net.cpp:411] accuracy -> accuracy
I0428 19:41:02.057613 30618 net.cpp:150] Setting up accuracy
I0428 19:41:02.057642 30618 net.cpp:157] Top shape: (1)
I0428 19:41:02.057947 30618 net.cpp:165] Memory required for data: 326804
I0428 19:41:02.057974 30618 layer_factory.hpp:77] Creating layer loss
I0428 19:41:02.058001 30618 net.cpp:106] Creating Layer loss
I0428 19:41:02.058020 30618 net.cpp:454] loss <- ip4_ip4_0_split_1
I0428 19:41:02.058050 30618 net.cpp:454] loss <- label_mnist_1_split_1
I0428 19:41:02.058071 30618 net.cpp:411] loss -> loss
I0428 19:41:02.058100 30618 layer_factory.hpp:77] Creating layer loss
I0428 19:41:02.059171 30618 net.cpp:150] Setting up loss
I0428 19:41:02.059213 30618 net.cpp:157] Top shape: (1)
I0428 19:41:02.059237 30618 net.cpp:160]     with loss weight 1
I0428 19:41:02.059264 30618 net.cpp:165] Memory required for data: 326808
I0428 19:41:02.059310 30618 net.cpp:226] loss needs backward computation.
I0428 19:41:02.059331 30618 net.cpp:228] accuracy does not need backward computation.
I0428 19:41:02.059357 30618 net.cpp:226] ip4_ip4_0_split needs backward computation.
I0428 19:41:02.059378 30618 net.cpp:226] ip4 needs backward computation.
I0428 19:41:02.059397 30618 net.cpp:228] label_mnist_1_split does not need backward computation.
I0428 19:41:02.059428 30618 net.cpp:228] mnist does not need backward computation.
I0428 19:41:02.059447 30618 net.cpp:270] This network produces output accuracy
I0428 19:41:02.059466 30618 net.cpp:270] This network produces output loss
I0428 19:41:02.059490 30618 net.cpp:283] Network initialization done.
I0428 19:41:02.059554 30618 solver.cpp:60] Solver scaffolding done.
I0428 19:41:02.059680 30618 caffe.cpp:212] Starting Optimization
I0428 19:41:02.059710 30618 solver.cpp:288] Solving MLP
I0428 19:41:02.059728 30618 solver.cpp:289] Learning Rate Policy: inv
I0428 19:41:02.059834 30618 solver.cpp:341] Iteration 0, Testing net (#0)
I0428 19:41:02.061288 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:02.174486 30618 solver.cpp:409]     Test net output #0: accuracy = 0.0924
I0428 19:41:02.174562 30618 solver.cpp:409]     Test net output #1: loss = 2.3741 (* 1 = 2.3741 loss)
I0428 19:41:02.176350 30618 solver.cpp:237] Iteration 0, loss = 2.40982
I0428 19:41:02.176395 30618 solver.cpp:253]     Train net output #0: loss = 2.40982 (* 1 = 2.40982 loss)
I0428 19:41:02.176432 30618 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0428 19:41:02.280431 30618 solver.cpp:237] Iteration 100, loss = 0.563389
I0428 19:41:02.280475 30618 solver.cpp:253]     Train net output #0: loss = 0.563389 (* 1 = 0.563389 loss)
I0428 19:41:02.280485 30618 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0428 19:41:02.387584 30618 solver.cpp:237] Iteration 200, loss = 0.665799
I0428 19:41:02.387625 30618 solver.cpp:253]     Train net output #0: loss = 0.665799 (* 1 = 0.665799 loss)
I0428 19:41:02.387636 30618 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0428 19:41:02.494714 30618 solver.cpp:237] Iteration 300, loss = 0.53117
I0428 19:41:02.494757 30618 solver.cpp:253]     Train net output #0: loss = 0.53117 (* 1 = 0.53117 loss)
I0428 19:41:02.494766 30618 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0428 19:41:02.601850 30618 solver.cpp:237] Iteration 400, loss = 0.377688
I0428 19:41:02.601893 30618 solver.cpp:253]     Train net output #0: loss = 0.377688 (* 1 = 0.377688 loss)
I0428 19:41:02.601903 30618 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0428 19:41:02.692462 30618 solver.cpp:237] Iteration 500, loss = 0.429128
I0428 19:41:02.692541 30618 solver.cpp:253]     Train net output #0: loss = 0.429128 (* 1 = 0.429128 loss)
I0428 19:41:02.692564 30618 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I0428 19:41:02.764427 30618 solver.cpp:237] Iteration 600, loss = 0.346219
I0428 19:41:02.764550 30618 solver.cpp:253]     Train net output #0: loss = 0.346219 (* 1 = 0.346219 loss)
I0428 19:41:02.764612 30618 sgd_solver.cpp:106] Iteration 600, lr = 0.0095724
I0428 19:41:02.841295 30618 solver.cpp:237] Iteration 700, loss = 0.35604
I0428 19:41:02.841429 30618 solver.cpp:253]     Train net output #0: loss = 0.35604 (* 1 = 0.35604 loss)
I0428 19:41:02.841475 30618 sgd_solver.cpp:106] Iteration 700, lr = 0.00950522
I0428 19:41:02.919926 30618 solver.cpp:237] Iteration 800, loss = 0.463504
I0428 19:41:02.920047 30618 solver.cpp:253]     Train net output #0: loss = 0.463504 (* 1 = 0.463504 loss)
I0428 19:41:02.920086 30618 sgd_solver.cpp:106] Iteration 800, lr = 0.00943913
I0428 19:41:02.996768 30618 solver.cpp:237] Iteration 900, loss = 0.389798
I0428 19:41:02.996912 30618 solver.cpp:253]     Train net output #0: loss = 0.389798 (* 1 = 0.389798 loss)
I0428 19:41:02.996971 30618 sgd_solver.cpp:106] Iteration 900, lr = 0.00937411
I0428 19:41:03.067217 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:03.072502 30618 solver.cpp:237] Iteration 1000, loss = 0.319231
I0428 19:41:03.072648 30618 solver.cpp:253]     Train net output #0: loss = 0.319232 (* 1 = 0.319232 loss)
I0428 19:41:03.072715 30618 sgd_solver.cpp:106] Iteration 1000, lr = 0.00931012
I0428 19:41:03.149222 30618 solver.cpp:237] Iteration 1100, loss = 0.386093
I0428 19:41:03.149343 30618 solver.cpp:253]     Train net output #0: loss = 0.386093 (* 1 = 0.386093 loss)
I0428 19:41:03.149401 30618 sgd_solver.cpp:106] Iteration 1100, lr = 0.00924715
I0428 19:41:03.225149 30618 solver.cpp:237] Iteration 1200, loss = 0.298735
I0428 19:41:03.225276 30618 solver.cpp:253]     Train net output #0: loss = 0.298735 (* 1 = 0.298735 loss)
I0428 19:41:03.225327 30618 sgd_solver.cpp:106] Iteration 1200, lr = 0.00918515
I0428 19:41:03.302692 30618 solver.cpp:237] Iteration 1300, loss = 0.324926
I0428 19:41:03.302815 30618 solver.cpp:253]     Train net output #0: loss = 0.324926 (* 1 = 0.324926 loss)
I0428 19:41:03.302870 30618 sgd_solver.cpp:106] Iteration 1300, lr = 0.00912412
I0428 19:41:03.378779 30618 solver.cpp:237] Iteration 1400, loss = 0.409887
I0428 19:41:03.378923 30618 solver.cpp:253]     Train net output #0: loss = 0.409887 (* 1 = 0.409887 loss)
I0428 19:41:03.378971 30618 sgd_solver.cpp:106] Iteration 1400, lr = 0.00906403
I0428 19:41:03.454836 30618 solver.cpp:237] Iteration 1500, loss = 0.347053
I0428 19:41:03.454974 30618 solver.cpp:253]     Train net output #0: loss = 0.347053 (* 1 = 0.347053 loss)
I0428 19:41:03.455019 30618 sgd_solver.cpp:106] Iteration 1500, lr = 0.00900485
I0428 19:41:03.530768 30618 solver.cpp:237] Iteration 1600, loss = 0.300818
I0428 19:41:03.530915 30618 solver.cpp:253]     Train net output #0: loss = 0.300818 (* 1 = 0.300818 loss)
I0428 19:41:03.530958 30618 sgd_solver.cpp:106] Iteration 1600, lr = 0.00894657
I0428 19:41:03.606550 30618 solver.cpp:237] Iteration 1700, loss = 0.369481
I0428 19:41:03.606688 30618 solver.cpp:253]     Train net output #0: loss = 0.369481 (* 1 = 0.369481 loss)
I0428 19:41:03.606734 30618 sgd_solver.cpp:106] Iteration 1700, lr = 0.00888916
I0428 19:41:03.682477 30618 solver.cpp:237] Iteration 1800, loss = 0.276446
I0428 19:41:03.682615 30618 solver.cpp:253]     Train net output #0: loss = 0.276446 (* 1 = 0.276446 loss)
I0428 19:41:03.682669 30618 sgd_solver.cpp:106] Iteration 1800, lr = 0.0088326
I0428 19:41:03.758450 30618 solver.cpp:237] Iteration 1900, loss = 0.310368
I0428 19:41:03.758586 30618 solver.cpp:253]     Train net output #0: loss = 0.310368 (* 1 = 0.310368 loss)
I0428 19:41:03.758630 30618 sgd_solver.cpp:106] Iteration 1900, lr = 0.00877687
I0428 19:41:03.832801 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:03.834182 30618 solver.cpp:237] Iteration 2000, loss = 0.382348
I0428 19:41:03.834280 30618 solver.cpp:253]     Train net output #0: loss = 0.382348 (* 1 = 0.382348 loss)
I0428 19:41:03.834324 30618 sgd_solver.cpp:106] Iteration 2000, lr = 0.00872196
I0428 19:41:03.910097 30618 solver.cpp:237] Iteration 2100, loss = 0.325905
I0428 19:41:03.910235 30618 solver.cpp:253]     Train net output #0: loss = 0.325905 (* 1 = 0.325905 loss)
I0428 19:41:03.910279 30618 sgd_solver.cpp:106] Iteration 2100, lr = 0.00866784
I0428 19:41:03.985879 30618 solver.cpp:237] Iteration 2200, loss = 0.291453
I0428 19:41:03.986019 30618 solver.cpp:253]     Train net output #0: loss = 0.291453 (* 1 = 0.291453 loss)
I0428 19:41:03.986063 30618 sgd_solver.cpp:106] Iteration 2200, lr = 0.0086145
I0428 19:41:04.061660 30618 solver.cpp:237] Iteration 2300, loss = 0.359671
I0428 19:41:04.061796 30618 solver.cpp:253]     Train net output #0: loss = 0.359671 (* 1 = 0.359671 loss)
I0428 19:41:04.061851 30618 sgd_solver.cpp:106] Iteration 2300, lr = 0.00856192
I0428 19:41:04.132305 30618 solver.cpp:237] Iteration 2400, loss = 0.262909
I0428 19:41:04.132395 30618 solver.cpp:253]     Train net output #0: loss = 0.262909 (* 1 = 0.262909 loss)
I0428 19:41:04.132417 30618 sgd_solver.cpp:106] Iteration 2400, lr = 0.00851008
I0428 19:41:04.200695 30618 solver.cpp:237] Iteration 2500, loss = 0.301615
I0428 19:41:04.200780 30618 solver.cpp:253]     Train net output #0: loss = 0.301615 (* 1 = 0.301615 loss)
I0428 19:41:04.200834 30618 sgd_solver.cpp:106] Iteration 2500, lr = 0.00845897
I0428 19:41:04.269124 30618 solver.cpp:237] Iteration 2600, loss = 0.365036
I0428 19:41:04.269208 30618 solver.cpp:253]     Train net output #0: loss = 0.365036 (* 1 = 0.365036 loss)
I0428 19:41:04.269253 30618 sgd_solver.cpp:106] Iteration 2600, lr = 0.00840857
I0428 19:41:04.337424 30618 solver.cpp:237] Iteration 2700, loss = 0.313212
I0428 19:41:04.337510 30618 solver.cpp:253]     Train net output #0: loss = 0.313212 (* 1 = 0.313212 loss)
I0428 19:41:04.337538 30618 sgd_solver.cpp:106] Iteration 2700, lr = 0.00835886
I0428 19:41:04.405886 30618 solver.cpp:237] Iteration 2800, loss = 0.285692
I0428 19:41:04.405967 30618 solver.cpp:253]     Train net output #0: loss = 0.285692 (* 1 = 0.285692 loss)
I0428 19:41:04.405989 30618 sgd_solver.cpp:106] Iteration 2800, lr = 0.00830984
I0428 19:41:04.474354 30618 solver.cpp:237] Iteration 2900, loss = 0.35294
I0428 19:41:04.474432 30618 solver.cpp:253]     Train net output #0: loss = 0.35294 (* 1 = 0.35294 loss)
I0428 19:41:04.474453 30618 sgd_solver.cpp:106] Iteration 2900, lr = 0.00826148
I0428 19:41:04.542932 30618 solver.cpp:237] Iteration 3000, loss = 0.253725
I0428 19:41:04.543005 30618 solver.cpp:253]     Train net output #0: loss = 0.253725 (* 1 = 0.253725 loss)
I0428 19:41:04.543025 30618 sgd_solver.cpp:106] Iteration 3000, lr = 0.00821377
I0428 19:41:04.543555 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:04.611429 30618 solver.cpp:237] Iteration 3100, loss = 0.295657
I0428 19:41:04.611510 30618 solver.cpp:253]     Train net output #0: loss = 0.295657 (* 1 = 0.295657 loss)
I0428 19:41:04.611536 30618 sgd_solver.cpp:106] Iteration 3100, lr = 0.0081667
I0428 19:41:04.679590 30618 solver.cpp:237] Iteration 3200, loss = 0.352985
I0428 19:41:04.679671 30618 solver.cpp:253]     Train net output #0: loss = 0.352985 (* 1 = 0.352985 loss)
I0428 19:41:04.679692 30618 sgd_solver.cpp:106] Iteration 3200, lr = 0.00812025
I0428 19:41:04.748054 30618 solver.cpp:237] Iteration 3300, loss = 0.304708
I0428 19:41:04.748137 30618 solver.cpp:253]     Train net output #0: loss = 0.304708 (* 1 = 0.304708 loss)
I0428 19:41:04.748163 30618 sgd_solver.cpp:106] Iteration 3300, lr = 0.00807442
I0428 19:41:04.816579 30618 solver.cpp:237] Iteration 3400, loss = 0.281731
I0428 19:41:04.816663 30618 solver.cpp:253]     Train net output #0: loss = 0.281731 (* 1 = 0.281731 loss)
I0428 19:41:04.816681 30618 sgd_solver.cpp:106] Iteration 3400, lr = 0.00802918
I0428 19:41:04.885072 30618 solver.cpp:237] Iteration 3500, loss = 0.347967
I0428 19:41:04.885154 30618 solver.cpp:253]     Train net output #0: loss = 0.347967 (* 1 = 0.347967 loss)
I0428 19:41:04.885174 30618 sgd_solver.cpp:106] Iteration 3500, lr = 0.00798454
I0428 19:41:04.953907 30618 solver.cpp:237] Iteration 3600, loss = 0.247081
I0428 19:41:04.953985 30618 solver.cpp:253]     Train net output #0: loss = 0.247081 (* 1 = 0.247081 loss)
I0428 19:41:04.954006 30618 sgd_solver.cpp:106] Iteration 3600, lr = 0.00794046
I0428 19:41:05.022126 30618 solver.cpp:237] Iteration 3700, loss = 0.291288
I0428 19:41:05.022207 30618 solver.cpp:253]     Train net output #0: loss = 0.291288 (* 1 = 0.291288 loss)
I0428 19:41:05.022243 30618 sgd_solver.cpp:106] Iteration 3700, lr = 0.00789695
I0428 19:41:05.090432 30618 solver.cpp:237] Iteration 3800, loss = 0.344055
I0428 19:41:05.090518 30618 solver.cpp:253]     Train net output #0: loss = 0.344055 (* 1 = 0.344055 loss)
I0428 19:41:05.090544 30618 sgd_solver.cpp:106] Iteration 3800, lr = 0.007854
I0428 19:41:05.158530 30618 solver.cpp:237] Iteration 3900, loss = 0.298589
I0428 19:41:05.158610 30618 solver.cpp:253]     Train net output #0: loss = 0.298589 (* 1 = 0.298589 loss)
I0428 19:41:05.158629 30618 sgd_solver.cpp:106] Iteration 3900, lr = 0.00781158
I0428 19:41:05.226891 30618 solver.cpp:237] Iteration 4000, loss = 0.278794
I0428 19:41:05.226972 30618 solver.cpp:253]     Train net output #0: loss = 0.278794 (* 1 = 0.278794 loss)
I0428 19:41:05.227006 30618 sgd_solver.cpp:106] Iteration 4000, lr = 0.0077697
I0428 19:41:05.227614 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:05.295385 30618 solver.cpp:237] Iteration 4100, loss = 0.344122
I0428 19:41:05.295462 30618 solver.cpp:253]     Train net output #0: loss = 0.344122 (* 1 = 0.344122 loss)
I0428 19:41:05.295485 30618 sgd_solver.cpp:106] Iteration 4100, lr = 0.00772833
I0428 19:41:05.363571 30618 solver.cpp:237] Iteration 4200, loss = 0.242062
I0428 19:41:05.363654 30618 solver.cpp:253]     Train net output #0: loss = 0.242062 (* 1 = 0.242062 loss)
I0428 19:41:05.363677 30618 sgd_solver.cpp:106] Iteration 4200, lr = 0.00768748
I0428 19:41:05.431872 30618 solver.cpp:237] Iteration 4300, loss = 0.28792
I0428 19:41:05.431954 30618 solver.cpp:253]     Train net output #0: loss = 0.287919 (* 1 = 0.287919 loss)
I0428 19:41:05.431977 30618 sgd_solver.cpp:106] Iteration 4300, lr = 0.00764712
I0428 19:41:05.500166 30618 solver.cpp:237] Iteration 4400, loss = 0.337145
I0428 19:41:05.500248 30618 solver.cpp:253]     Train net output #0: loss = 0.337145 (* 1 = 0.337145 loss)
I0428 19:41:05.500267 30618 sgd_solver.cpp:106] Iteration 4400, lr = 0.00760726
I0428 19:41:05.568434 30618 solver.cpp:237] Iteration 4500, loss = 0.293967
I0428 19:41:05.568512 30618 solver.cpp:253]     Train net output #0: loss = 0.293967 (* 1 = 0.293967 loss)
I0428 19:41:05.568533 30618 sgd_solver.cpp:106] Iteration 4500, lr = 0.00756788
I0428 19:41:05.636709 30618 solver.cpp:237] Iteration 4600, loss = 0.276495
I0428 19:41:05.636793 30618 solver.cpp:253]     Train net output #0: loss = 0.276494 (* 1 = 0.276494 loss)
I0428 19:41:05.636821 30618 sgd_solver.cpp:106] Iteration 4600, lr = 0.00752897
I0428 19:41:05.705049 30618 solver.cpp:237] Iteration 4700, loss = 0.341052
I0428 19:41:05.705128 30618 solver.cpp:253]     Train net output #0: loss = 0.341052 (* 1 = 0.341052 loss)
I0428 19:41:05.705149 30618 sgd_solver.cpp:106] Iteration 4700, lr = 0.00749052
I0428 19:41:05.773437 30618 solver.cpp:237] Iteration 4800, loss = 0.238148
I0428 19:41:05.773522 30618 solver.cpp:253]     Train net output #0: loss = 0.238148 (* 1 = 0.238148 loss)
I0428 19:41:05.773547 30618 sgd_solver.cpp:106] Iteration 4800, lr = 0.00745253
I0428 19:41:05.841694 30618 solver.cpp:237] Iteration 4900, loss = 0.285228
I0428 19:41:05.841778 30618 solver.cpp:253]     Train net output #0: loss = 0.285228 (* 1 = 0.285228 loss)
I0428 19:41:05.841800 30618 sgd_solver.cpp:106] Iteration 4900, lr = 0.00741498
I0428 19:41:05.909518 30618 solver.cpp:459] Snapshotting to binary proto file examples/mlp/mlp_test_iter_5000.caffemodel
I0428 19:41:05.910039 30618 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mlp/mlp_test_iter_5000.solverstate
I0428 19:41:05.910202 30618 solver.cpp:341] Iteration 5000, Testing net (#0)
I0428 19:41:05.915858 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:06.052676 30618 solver.cpp:409]     Test net output #0: accuracy = 0.92
I0428 19:41:06.052757 30618 solver.cpp:409]     Test net output #1: loss = 0.289027 (* 1 = 0.289027 loss)
I0428 19:41:06.053078 30618 solver.cpp:237] Iteration 5000, loss = 0.331624
I0428 19:41:06.053113 30618 solver.cpp:253]     Train net output #0: loss = 0.331624 (* 1 = 0.331624 loss)
I0428 19:41:06.053133 30618 sgd_solver.cpp:106] Iteration 5000, lr = 0.00737788
I0428 19:41:06.165644 30618 solver.cpp:237] Iteration 5100, loss = 0.29035
I0428 19:41:06.165732 30618 solver.cpp:253]     Train net output #0: loss = 0.29035 (* 1 = 0.29035 loss)
I0428 19:41:06.165776 30618 sgd_solver.cpp:106] Iteration 5100, lr = 0.0073412
I0428 19:41:06.243787 30618 solver.cpp:237] Iteration 5200, loss = 0.27462
I0428 19:41:06.243870 30618 solver.cpp:253]     Train net output #0: loss = 0.27462 (* 1 = 0.27462 loss)
I0428 19:41:06.243892 30618 sgd_solver.cpp:106] Iteration 5200, lr = 0.00730495
I0428 19:41:06.316287 30618 solver.cpp:237] Iteration 5300, loss = 0.33854
I0428 19:41:06.316368 30618 solver.cpp:253]     Train net output #0: loss = 0.33854 (* 1 = 0.33854 loss)
I0428 19:41:06.316404 30618 sgd_solver.cpp:106] Iteration 5300, lr = 0.00726911
I0428 19:41:06.388589 30618 solver.cpp:237] Iteration 5400, loss = 0.23502
I0428 19:41:06.388669 30618 solver.cpp:253]     Train net output #0: loss = 0.235019 (* 1 = 0.235019 loss)
I0428 19:41:06.388689 30618 sgd_solver.cpp:106] Iteration 5400, lr = 0.00723368
I0428 19:41:06.461030 30618 solver.cpp:237] Iteration 5500, loss = 0.283018
I0428 19:41:06.461112 30618 solver.cpp:253]     Train net output #0: loss = 0.283018 (* 1 = 0.283018 loss)
I0428 19:41:06.461134 30618 sgd_solver.cpp:106] Iteration 5500, lr = 0.00719865
I0428 19:41:06.533109 30618 solver.cpp:237] Iteration 5600, loss = 0.327103
I0428 19:41:06.533190 30618 solver.cpp:253]     Train net output #0: loss = 0.327102 (* 1 = 0.327102 loss)
I0428 19:41:06.533215 30618 sgd_solver.cpp:106] Iteration 5600, lr = 0.00716402
I0428 19:41:06.605087 30618 solver.cpp:237] Iteration 5700, loss = 0.287444
I0428 19:41:06.605166 30618 solver.cpp:253]     Train net output #0: loss = 0.287444 (* 1 = 0.287444 loss)
I0428 19:41:06.605187 30618 sgd_solver.cpp:106] Iteration 5700, lr = 0.00712977
I0428 19:41:06.677274 30618 solver.cpp:237] Iteration 5800, loss = 0.273044
I0428 19:41:06.677356 30618 solver.cpp:253]     Train net output #0: loss = 0.273044 (* 1 = 0.273044 loss)
I0428 19:41:06.677376 30618 sgd_solver.cpp:106] Iteration 5800, lr = 0.0070959
I0428 19:41:06.749351 30618 solver.cpp:237] Iteration 5900, loss = 0.336443
I0428 19:41:06.749433 30618 solver.cpp:253]     Train net output #0: loss = 0.336443 (* 1 = 0.336443 loss)
I0428 19:41:06.749454 30618 sgd_solver.cpp:106] Iteration 5900, lr = 0.0070624
I0428 19:41:06.772332 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:06.820822 30618 solver.cpp:237] Iteration 6000, loss = 0.23247
I0428 19:41:06.820911 30618 solver.cpp:253]     Train net output #0: loss = 0.23247 (* 1 = 0.23247 loss)
I0428 19:41:06.820936 30618 sgd_solver.cpp:106] Iteration 6000, lr = 0.00702927
I0428 19:41:06.892254 30618 solver.cpp:237] Iteration 6100, loss = 0.281165
I0428 19:41:06.892333 30618 solver.cpp:253]     Train net output #0: loss = 0.281165 (* 1 = 0.281165 loss)
I0428 19:41:06.892354 30618 sgd_solver.cpp:106] Iteration 6100, lr = 0.0069965
I0428 19:41:06.964862 30618 solver.cpp:237] Iteration 6200, loss = 0.323327
I0428 19:41:06.964949 30618 solver.cpp:253]     Train net output #0: loss = 0.323326 (* 1 = 0.323326 loss)
I0428 19:41:06.964982 30618 sgd_solver.cpp:106] Iteration 6200, lr = 0.00696408
I0428 19:41:07.034711 30618 solver.cpp:237] Iteration 6300, loss = 0.285061
I0428 19:41:07.034798 30618 solver.cpp:253]     Train net output #0: loss = 0.285061 (* 1 = 0.285061 loss)
I0428 19:41:07.034826 30618 sgd_solver.cpp:106] Iteration 6300, lr = 0.00693201
I0428 19:41:07.106689 30618 solver.cpp:237] Iteration 6400, loss = 0.271689
I0428 19:41:07.106770 30618 solver.cpp:253]     Train net output #0: loss = 0.271689 (* 1 = 0.271689 loss)
I0428 19:41:07.106791 30618 sgd_solver.cpp:106] Iteration 6400, lr = 0.00690029
I0428 19:41:07.179056 30618 solver.cpp:237] Iteration 6500, loss = 0.334665
I0428 19:41:07.179138 30618 solver.cpp:253]     Train net output #0: loss = 0.334664 (* 1 = 0.334664 loss)
I0428 19:41:07.179157 30618 sgd_solver.cpp:106] Iteration 6500, lr = 0.0068689
I0428 19:41:07.250777 30618 solver.cpp:237] Iteration 6600, loss = 0.230359
I0428 19:41:07.250861 30618 solver.cpp:253]     Train net output #0: loss = 0.230358 (* 1 = 0.230358 loss)
I0428 19:41:07.250888 30618 sgd_solver.cpp:106] Iteration 6600, lr = 0.00683784
I0428 19:41:07.322350 30618 solver.cpp:237] Iteration 6700, loss = 0.279584
I0428 19:41:07.322428 30618 solver.cpp:253]     Train net output #0: loss = 0.279583 (* 1 = 0.279583 loss)
I0428 19:41:07.322450 30618 sgd_solver.cpp:106] Iteration 6700, lr = 0.00680711
I0428 19:41:07.393977 30618 solver.cpp:237] Iteration 6800, loss = 0.320123
I0428 19:41:07.394062 30618 solver.cpp:253]     Train net output #0: loss = 0.320122 (* 1 = 0.320122 loss)
I0428 19:41:07.394091 30618 sgd_solver.cpp:106] Iteration 6800, lr = 0.0067767
I0428 19:41:07.465229 30618 solver.cpp:237] Iteration 6900, loss = 0.283074
I0428 19:41:07.465335 30618 solver.cpp:253]     Train net output #0: loss = 0.283074 (* 1 = 0.283074 loss)
I0428 19:41:07.465356 30618 sgd_solver.cpp:106] Iteration 6900, lr = 0.0067466
I0428 19:41:07.488430 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:07.537299 30618 solver.cpp:237] Iteration 7000, loss = 0.270504
I0428 19:41:07.537380 30618 solver.cpp:253]     Train net output #0: loss = 0.270504 (* 1 = 0.270504 loss)
I0428 19:41:07.537400 30618 sgd_solver.cpp:106] Iteration 7000, lr = 0.00671681
I0428 19:41:07.609216 30618 solver.cpp:237] Iteration 7100, loss = 0.333135
I0428 19:41:07.609302 30618 solver.cpp:253]     Train net output #0: loss = 0.333135 (* 1 = 0.333135 loss)
I0428 19:41:07.609330 30618 sgd_solver.cpp:106] Iteration 7100, lr = 0.00668733
I0428 19:41:07.681507 30618 solver.cpp:237] Iteration 7200, loss = 0.228587
I0428 19:41:07.681591 30618 solver.cpp:253]     Train net output #0: loss = 0.228586 (* 1 = 0.228586 loss)
I0428 19:41:07.681612 30618 sgd_solver.cpp:106] Iteration 7200, lr = 0.00665815
I0428 19:41:07.753708 30618 solver.cpp:237] Iteration 7300, loss = 0.278215
I0428 19:41:07.753795 30618 solver.cpp:253]     Train net output #0: loss = 0.278215 (* 1 = 0.278215 loss)
I0428 19:41:07.753823 30618 sgd_solver.cpp:106] Iteration 7300, lr = 0.00662927
I0428 19:41:07.825647 30618 solver.cpp:237] Iteration 7400, loss = 0.317368
I0428 19:41:07.825729 30618 solver.cpp:253]     Train net output #0: loss = 0.317368 (* 1 = 0.317368 loss)
I0428 19:41:07.825750 30618 sgd_solver.cpp:106] Iteration 7400, lr = 0.00660067
I0428 19:41:07.898336 30618 solver.cpp:237] Iteration 7500, loss = 0.281395
I0428 19:41:07.898416 30618 solver.cpp:253]     Train net output #0: loss = 0.281395 (* 1 = 0.281395 loss)
I0428 19:41:07.898437 30618 sgd_solver.cpp:106] Iteration 7500, lr = 0.00657236
I0428 19:41:07.970476 30618 solver.cpp:237] Iteration 7600, loss = 0.269453
I0428 19:41:07.970558 30618 solver.cpp:253]     Train net output #0: loss = 0.269452 (* 1 = 0.269452 loss)
I0428 19:41:07.970579 30618 sgd_solver.cpp:106] Iteration 7600, lr = 0.00654433
I0428 19:41:08.039017 30618 solver.cpp:237] Iteration 7700, loss = 0.331805
I0428 19:41:08.039096 30618 solver.cpp:253]     Train net output #0: loss = 0.331805 (* 1 = 0.331805 loss)
I0428 19:41:08.039118 30618 sgd_solver.cpp:106] Iteration 7700, lr = 0.00651658
I0428 19:41:08.106971 30618 solver.cpp:237] Iteration 7800, loss = 0.227082
I0428 19:41:08.107049 30618 solver.cpp:253]     Train net output #0: loss = 0.227082 (* 1 = 0.227082 loss)
I0428 19:41:08.107069 30618 sgd_solver.cpp:106] Iteration 7800, lr = 0.00648911
I0428 19:41:08.174969 30618 solver.cpp:237] Iteration 7900, loss = 0.277016
I0428 19:41:08.175046 30618 solver.cpp:253]     Train net output #0: loss = 0.277016 (* 1 = 0.277016 loss)
I0428 19:41:08.175066 30618 sgd_solver.cpp:106] Iteration 7900, lr = 0.0064619
I0428 19:41:08.196805 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:08.242976 30618 solver.cpp:237] Iteration 8000, loss = 0.314974
I0428 19:41:08.243058 30618 solver.cpp:253]     Train net output #0: loss = 0.314974 (* 1 = 0.314974 loss)
I0428 19:41:08.243080 30618 sgd_solver.cpp:106] Iteration 8000, lr = 0.00643496
I0428 19:41:08.310912 30618 solver.cpp:237] Iteration 8100, loss = 0.27996
I0428 19:41:08.310995 30618 solver.cpp:253]     Train net output #0: loss = 0.27996 (* 1 = 0.27996 loss)
I0428 19:41:08.311022 30618 sgd_solver.cpp:106] Iteration 8100, lr = 0.00640827
I0428 19:41:08.379055 30618 solver.cpp:237] Iteration 8200, loss = 0.26851
I0428 19:41:08.379139 30618 solver.cpp:253]     Train net output #0: loss = 0.26851 (* 1 = 0.26851 loss)
I0428 19:41:08.379168 30618 sgd_solver.cpp:106] Iteration 8200, lr = 0.00638185
I0428 19:41:08.447006 30618 solver.cpp:237] Iteration 8300, loss = 0.330636
I0428 19:41:08.447084 30618 solver.cpp:253]     Train net output #0: loss = 0.330636 (* 1 = 0.330636 loss)
I0428 19:41:08.447103 30618 sgd_solver.cpp:106] Iteration 8300, lr = 0.00635567
I0428 19:41:08.514909 30618 solver.cpp:237] Iteration 8400, loss = 0.225793
I0428 19:41:08.515013 30618 solver.cpp:253]     Train net output #0: loss = 0.225793 (* 1 = 0.225793 loss)
I0428 19:41:08.515041 30618 sgd_solver.cpp:106] Iteration 8400, lr = 0.00632975
I0428 19:41:08.582769 30618 solver.cpp:237] Iteration 8500, loss = 0.275956
I0428 19:41:08.582851 30618 solver.cpp:253]     Train net output #0: loss = 0.275956 (* 1 = 0.275956 loss)
I0428 19:41:08.582872 30618 sgd_solver.cpp:106] Iteration 8500, lr = 0.00630407
I0428 19:41:08.650800 30618 solver.cpp:237] Iteration 8600, loss = 0.312874
I0428 19:41:08.650882 30618 solver.cpp:253]     Train net output #0: loss = 0.312874 (* 1 = 0.312874 loss)
I0428 19:41:08.650902 30618 sgd_solver.cpp:106] Iteration 8600, lr = 0.00627864
I0428 19:41:08.718816 30618 solver.cpp:237] Iteration 8700, loss = 0.278721
I0428 19:41:08.718896 30618 solver.cpp:253]     Train net output #0: loss = 0.278721 (* 1 = 0.278721 loss)
I0428 19:41:08.718916 30618 sgd_solver.cpp:106] Iteration 8700, lr = 0.00625344
I0428 19:41:08.786777 30618 solver.cpp:237] Iteration 8800, loss = 0.267658
I0428 19:41:08.786860 30618 solver.cpp:253]     Train net output #0: loss = 0.267657 (* 1 = 0.267657 loss)
I0428 19:41:08.786887 30618 sgd_solver.cpp:106] Iteration 8800, lr = 0.00622847
I0428 19:41:08.854638 30618 solver.cpp:237] Iteration 8900, loss = 0.3296
I0428 19:41:08.854718 30618 solver.cpp:253]     Train net output #0: loss = 0.329599 (* 1 = 0.329599 loss)
I0428 19:41:08.854750 30618 sgd_solver.cpp:106] Iteration 8900, lr = 0.00620374
I0428 19:41:08.876536 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:08.923326 30618 solver.cpp:237] Iteration 9000, loss = 0.224678
I0428 19:41:08.923404 30618 solver.cpp:253]     Train net output #0: loss = 0.224678 (* 1 = 0.224678 loss)
I0428 19:41:08.923424 30618 sgd_solver.cpp:106] Iteration 9000, lr = 0.00617924
I0428 19:41:08.991451 30618 solver.cpp:237] Iteration 9100, loss = 0.27501
I0428 19:41:08.991534 30618 solver.cpp:253]     Train net output #0: loss = 0.27501 (* 1 = 0.27501 loss)
I0428 19:41:08.991561 30618 sgd_solver.cpp:106] Iteration 9100, lr = 0.00615496
I0428 19:41:09.059649 30618 solver.cpp:237] Iteration 9200, loss = 0.311016
I0428 19:41:09.059725 30618 solver.cpp:253]     Train net output #0: loss = 0.311016 (* 1 = 0.311016 loss)
I0428 19:41:09.059744 30618 sgd_solver.cpp:106] Iteration 9200, lr = 0.0061309
I0428 19:41:09.127687 30618 solver.cpp:237] Iteration 9300, loss = 0.277642
I0428 19:41:09.127770 30618 solver.cpp:253]     Train net output #0: loss = 0.277642 (* 1 = 0.277642 loss)
I0428 19:41:09.127794 30618 sgd_solver.cpp:106] Iteration 9300, lr = 0.00610706
I0428 19:41:09.195680 30618 solver.cpp:237] Iteration 9400, loss = 0.266881
I0428 19:41:09.195760 30618 solver.cpp:253]     Train net output #0: loss = 0.266881 (* 1 = 0.266881 loss)
I0428 19:41:09.195780 30618 sgd_solver.cpp:106] Iteration 9400, lr = 0.00608343
I0428 19:41:09.263797 30618 solver.cpp:237] Iteration 9500, loss = 0.328674
I0428 19:41:09.263875 30618 solver.cpp:253]     Train net output #0: loss = 0.328674 (* 1 = 0.328674 loss)
I0428 19:41:09.263897 30618 sgd_solver.cpp:106] Iteration 9500, lr = 0.00606002
I0428 19:41:09.333205 30618 solver.cpp:237] Iteration 9600, loss = 0.223707
I0428 19:41:09.333287 30618 solver.cpp:253]     Train net output #0: loss = 0.223707 (* 1 = 0.223707 loss)
I0428 19:41:09.333307 30618 sgd_solver.cpp:106] Iteration 9600, lr = 0.00603682
I0428 19:41:09.403842 30618 solver.cpp:237] Iteration 9700, loss = 0.274159
I0428 19:41:09.403924 30618 solver.cpp:253]     Train net output #0: loss = 0.274159 (* 1 = 0.274159 loss)
I0428 19:41:09.403947 30618 sgd_solver.cpp:106] Iteration 9700, lr = 0.00601382
I0428 19:41:09.475797 30618 solver.cpp:237] Iteration 9800, loss = 0.309362
I0428 19:41:09.475878 30618 solver.cpp:253]     Train net output #0: loss = 0.309362 (* 1 = 0.309362 loss)
I0428 19:41:09.475908 30618 sgd_solver.cpp:106] Iteration 9800, lr = 0.00599102
I0428 19:41:09.547724 30618 solver.cpp:237] Iteration 9900, loss = 0.276695
I0428 19:41:09.547817 30618 solver.cpp:253]     Train net output #0: loss = 0.276695 (* 1 = 0.276695 loss)
I0428 19:41:09.547852 30618 sgd_solver.cpp:106] Iteration 9900, lr = 0.00596843
I0428 19:41:09.572276 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:09.618911 30618 solver.cpp:459] Snapshotting to binary proto file examples/mlp/mlp_test_iter_10000.caffemodel
I0428 19:41:09.619298 30618 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mlp/mlp_test_iter_10000.solverstate
I0428 19:41:09.619462 30618 solver.cpp:341] Iteration 10000, Testing net (#0)
I0428 19:41:09.748880 30618 solver.cpp:409]     Test net output #0: accuracy = 0.9223
I0428 19:41:09.748965 30618 solver.cpp:409]     Test net output #1: loss = 0.28038 (* 1 = 0.28038 loss)
I0428 19:41:09.749287 30618 solver.cpp:237] Iteration 10000, loss = 0.26617
I0428 19:41:09.749325 30618 solver.cpp:253]     Train net output #0: loss = 0.26617 (* 1 = 0.26617 loss)
I0428 19:41:09.749346 30618 sgd_solver.cpp:106] Iteration 10000, lr = 0.00594604
I0428 19:41:09.856256 30618 solver.cpp:237] Iteration 10100, loss = 0.327842
I0428 19:41:09.856338 30618 solver.cpp:253]     Train net output #0: loss = 0.327842 (* 1 = 0.327842 loss)
I0428 19:41:09.856356 30618 sgd_solver.cpp:106] Iteration 10100, lr = 0.00592384
I0428 19:41:09.936359 30618 solver.cpp:237] Iteration 10200, loss = 0.222856
I0428 19:41:09.936441 30618 solver.cpp:253]     Train net output #0: loss = 0.222855 (* 1 = 0.222855 loss)
I0428 19:41:09.936476 30618 sgd_solver.cpp:106] Iteration 10200, lr = 0.00590183
I0428 19:41:10.008558 30618 solver.cpp:237] Iteration 10300, loss = 0.273389
I0428 19:41:10.008642 30618 solver.cpp:253]     Train net output #0: loss = 0.273389 (* 1 = 0.273389 loss)
I0428 19:41:10.008676 30618 sgd_solver.cpp:106] Iteration 10300, lr = 0.00588001
I0428 19:41:10.080845 30618 solver.cpp:237] Iteration 10400, loss = 0.30788
I0428 19:41:10.080940 30618 solver.cpp:253]     Train net output #0: loss = 0.30788 (* 1 = 0.30788 loss)
I0428 19:41:10.080963 30618 sgd_solver.cpp:106] Iteration 10400, lr = 0.00585838
I0428 19:41:10.153265 30618 solver.cpp:237] Iteration 10500, loss = 0.275857
I0428 19:41:10.153344 30618 solver.cpp:253]     Train net output #0: loss = 0.275857 (* 1 = 0.275857 loss)
I0428 19:41:10.153363 30618 sgd_solver.cpp:106] Iteration 10500, lr = 0.00583693
I0428 19:41:10.226057 30618 solver.cpp:237] Iteration 10600, loss = 0.265516
I0428 19:41:10.226133 30618 solver.cpp:253]     Train net output #0: loss = 0.265516 (* 1 = 0.265516 loss)
I0428 19:41:10.226155 30618 sgd_solver.cpp:106] Iteration 10600, lr = 0.00581567
I0428 19:41:10.298588 30618 solver.cpp:237] Iteration 10700, loss = 0.327089
I0428 19:41:10.298668 30618 solver.cpp:253]     Train net output #0: loss = 0.327089 (* 1 = 0.327089 loss)
I0428 19:41:10.298688 30618 sgd_solver.cpp:106] Iteration 10700, lr = 0.00579458
I0428 19:41:10.370631 30618 solver.cpp:237] Iteration 10800, loss = 0.222105
I0428 19:41:10.370718 30618 solver.cpp:253]     Train net output #0: loss = 0.222104 (* 1 = 0.222104 loss)
I0428 19:41:10.370745 30618 sgd_solver.cpp:106] Iteration 10800, lr = 0.00577368
I0428 19:41:10.419136 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:10.442867 30618 solver.cpp:237] Iteration 10900, loss = 0.272688
I0428 19:41:10.442950 30618 solver.cpp:253]     Train net output #0: loss = 0.272687 (* 1 = 0.272687 loss)
I0428 19:41:10.442973 30618 sgd_solver.cpp:106] Iteration 10900, lr = 0.00575295
I0428 19:41:10.515123 30618 solver.cpp:237] Iteration 11000, loss = 0.306545
I0428 19:41:10.515209 30618 solver.cpp:253]     Train net output #0: loss = 0.306544 (* 1 = 0.306544 loss)
I0428 19:41:10.515244 30618 sgd_solver.cpp:106] Iteration 11000, lr = 0.00573239
I0428 19:41:10.587983 30618 solver.cpp:237] Iteration 11100, loss = 0.275112
I0428 19:41:10.588063 30618 solver.cpp:253]     Train net output #0: loss = 0.275112 (* 1 = 0.275112 loss)
I0428 19:41:10.588084 30618 sgd_solver.cpp:106] Iteration 11100, lr = 0.005712
I0428 19:41:10.660485 30618 solver.cpp:237] Iteration 11200, loss = 0.264911
I0428 19:41:10.660588 30618 solver.cpp:253]     Train net output #0: loss = 0.264911 (* 1 = 0.264911 loss)
I0428 19:41:10.660609 30618 sgd_solver.cpp:106] Iteration 11200, lr = 0.00569178
I0428 19:41:10.732853 30618 solver.cpp:237] Iteration 11300, loss = 0.326405
I0428 19:41:10.732944 30618 solver.cpp:253]     Train net output #0: loss = 0.326404 (* 1 = 0.326404 loss)
I0428 19:41:10.732980 30618 sgd_solver.cpp:106] Iteration 11300, lr = 0.00567173
I0428 19:41:10.804280 30618 solver.cpp:237] Iteration 11400, loss = 0.221438
I0428 19:41:10.804369 30618 solver.cpp:253]     Train net output #0: loss = 0.221438 (* 1 = 0.221438 loss)
I0428 19:41:10.804394 30618 sgd_solver.cpp:106] Iteration 11400, lr = 0.00565184
I0428 19:41:10.875335 30618 solver.cpp:237] Iteration 11500, loss = 0.272046
I0428 19:41:10.875417 30618 solver.cpp:253]     Train net output #0: loss = 0.272046 (* 1 = 0.272046 loss)
I0428 19:41:10.875437 30618 sgd_solver.cpp:106] Iteration 11500, lr = 0.00563211
I0428 19:41:10.946940 30618 solver.cpp:237] Iteration 11600, loss = 0.305336
I0428 19:41:10.947016 30618 solver.cpp:253]     Train net output #0: loss = 0.305336 (* 1 = 0.305336 loss)
I0428 19:41:10.947037 30618 sgd_solver.cpp:106] Iteration 11600, lr = 0.00561254
I0428 19:41:11.018028 30618 solver.cpp:237] Iteration 11700, loss = 0.274445
I0428 19:41:11.018111 30618 solver.cpp:253]     Train net output #0: loss = 0.274444 (* 1 = 0.274444 loss)
I0428 19:41:11.018139 30618 sgd_solver.cpp:106] Iteration 11700, lr = 0.00559313
I0428 19:41:11.086452 30618 solver.cpp:237] Iteration 11800, loss = 0.26435
I0428 19:41:11.086529 30618 solver.cpp:253]     Train net output #0: loss = 0.26435 (* 1 = 0.26435 loss)
I0428 19:41:11.086550 30618 sgd_solver.cpp:106] Iteration 11800, lr = 0.00557388
I0428 19:41:11.132454 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:11.154045 30618 solver.cpp:237] Iteration 11900, loss = 0.325779
I0428 19:41:11.154125 30618 solver.cpp:253]     Train net output #0: loss = 0.325779 (* 1 = 0.325779 loss)
I0428 19:41:11.154157 30618 sgd_solver.cpp:106] Iteration 11900, lr = 0.00555478
I0428 19:41:11.221819 30618 solver.cpp:237] Iteration 12000, loss = 0.220845
I0428 19:41:11.221900 30618 solver.cpp:253]     Train net output #0: loss = 0.220844 (* 1 = 0.220844 loss)
I0428 19:41:11.221920 30618 sgd_solver.cpp:106] Iteration 12000, lr = 0.00553583
I0428 19:41:11.289566 30618 solver.cpp:237] Iteration 12100, loss = 0.271457
I0428 19:41:11.289650 30618 solver.cpp:253]     Train net output #0: loss = 0.271456 (* 1 = 0.271456 loss)
I0428 19:41:11.289676 30618 sgd_solver.cpp:106] Iteration 12100, lr = 0.00551704
I0428 19:41:11.357424 30618 solver.cpp:237] Iteration 12200, loss = 0.304238
I0428 19:41:11.357507 30618 solver.cpp:253]     Train net output #0: loss = 0.304237 (* 1 = 0.304237 loss)
I0428 19:41:11.357530 30618 sgd_solver.cpp:106] Iteration 12200, lr = 0.00549839
I0428 19:41:11.425392 30618 solver.cpp:237] Iteration 12300, loss = 0.273844
I0428 19:41:11.425478 30618 solver.cpp:253]     Train net output #0: loss = 0.273843 (* 1 = 0.273843 loss)
I0428 19:41:11.425503 30618 sgd_solver.cpp:106] Iteration 12300, lr = 0.00547988
I0428 19:41:11.493288 30618 solver.cpp:237] Iteration 12400, loss = 0.263829
I0428 19:41:11.493367 30618 solver.cpp:253]     Train net output #0: loss = 0.263828 (* 1 = 0.263828 loss)
I0428 19:41:11.493391 30618 sgd_solver.cpp:106] Iteration 12400, lr = 0.00546153
I0428 19:41:11.561503 30618 solver.cpp:237] Iteration 12500, loss = 0.325205
I0428 19:41:11.561583 30618 solver.cpp:253]     Train net output #0: loss = 0.325205 (* 1 = 0.325205 loss)
I0428 19:41:11.561604 30618 sgd_solver.cpp:106] Iteration 12500, lr = 0.00544331
I0428 19:41:11.629474 30618 solver.cpp:237] Iteration 12600, loss = 0.220313
I0428 19:41:11.629555 30618 solver.cpp:253]     Train net output #0: loss = 0.220312 (* 1 = 0.220312 loss)
I0428 19:41:11.629575 30618 sgd_solver.cpp:106] Iteration 12600, lr = 0.00542524
I0428 19:41:11.697474 30618 solver.cpp:237] Iteration 12700, loss = 0.270913
I0428 19:41:11.697567 30618 solver.cpp:253]     Train net output #0: loss = 0.270912 (* 1 = 0.270912 loss)
I0428 19:41:11.697607 30618 sgd_solver.cpp:106] Iteration 12700, lr = 0.0054073
I0428 19:41:11.765483 30618 solver.cpp:237] Iteration 12800, loss = 0.303235
I0428 19:41:11.765563 30618 solver.cpp:253]     Train net output #0: loss = 0.303235 (* 1 = 0.303235 loss)
I0428 19:41:11.765586 30618 sgd_solver.cpp:106] Iteration 12800, lr = 0.0053895
I0428 19:41:11.811640 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:11.833295 30618 solver.cpp:237] Iteration 12900, loss = 0.2733
I0428 19:41:11.833377 30618 solver.cpp:253]     Train net output #0: loss = 0.273299 (* 1 = 0.273299 loss)
I0428 19:41:11.833400 30618 sgd_solver.cpp:106] Iteration 12900, lr = 0.00537184
I0428 19:41:11.901203 30618 solver.cpp:237] Iteration 13000, loss = 0.263342
I0428 19:41:11.901283 30618 solver.cpp:253]     Train net output #0: loss = 0.263341 (* 1 = 0.263341 loss)
I0428 19:41:11.901304 30618 sgd_solver.cpp:106] Iteration 13000, lr = 0.00535432
I0428 19:41:11.968999 30618 solver.cpp:237] Iteration 13100, loss = 0.324676
I0428 19:41:11.969079 30618 solver.cpp:253]     Train net output #0: loss = 0.324675 (* 1 = 0.324675 loss)
I0428 19:41:11.969110 30618 sgd_solver.cpp:106] Iteration 13100, lr = 0.00533692
I0428 19:41:12.036573 30618 solver.cpp:237] Iteration 13200, loss = 0.219834
I0428 19:41:12.036660 30618 solver.cpp:253]     Train net output #0: loss = 0.219833 (* 1 = 0.219833 loss)
I0428 19:41:12.036684 30618 sgd_solver.cpp:106] Iteration 13200, lr = 0.00531966
I0428 19:41:12.104291 30618 solver.cpp:237] Iteration 13300, loss = 0.270408
I0428 19:41:12.104372 30618 solver.cpp:253]     Train net output #0: loss = 0.270408 (* 1 = 0.270408 loss)
I0428 19:41:12.104395 30618 sgd_solver.cpp:106] Iteration 13300, lr = 0.00530253
I0428 19:41:12.172122 30618 solver.cpp:237] Iteration 13400, loss = 0.302318
I0428 19:41:12.172199 30618 solver.cpp:253]     Train net output #0: loss = 0.302317 (* 1 = 0.302317 loss)
I0428 19:41:12.172221 30618 sgd_solver.cpp:106] Iteration 13400, lr = 0.00528552
I0428 19:41:12.239526 30618 solver.cpp:237] Iteration 13500, loss = 0.272805
I0428 19:41:12.239608 30618 solver.cpp:253]     Train net output #0: loss = 0.272804 (* 1 = 0.272804 loss)
I0428 19:41:12.239632 30618 sgd_solver.cpp:106] Iteration 13500, lr = 0.00526865
I0428 19:41:12.307185 30618 solver.cpp:237] Iteration 13600, loss = 0.262886
I0428 19:41:12.307265 30618 solver.cpp:253]     Train net output #0: loss = 0.262885 (* 1 = 0.262885 loss)
I0428 19:41:12.307286 30618 sgd_solver.cpp:106] Iteration 13600, lr = 0.00525189
I0428 19:41:12.374913 30618 solver.cpp:237] Iteration 13700, loss = 0.324187
I0428 19:41:12.374995 30618 solver.cpp:253]     Train net output #0: loss = 0.324186 (* 1 = 0.324186 loss)
I0428 19:41:12.375021 30618 sgd_solver.cpp:106] Iteration 13700, lr = 0.00523527
I0428 19:41:12.442564 30618 solver.cpp:237] Iteration 13800, loss = 0.219402
I0428 19:41:12.442647 30618 solver.cpp:253]     Train net output #0: loss = 0.219401 (* 1 = 0.219401 loss)
I0428 19:41:12.442683 30618 sgd_solver.cpp:106] Iteration 13800, lr = 0.00521876
I0428 19:41:12.490005 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:12.510299 30618 solver.cpp:237] Iteration 13900, loss = 0.26994
I0428 19:41:12.510377 30618 solver.cpp:253]     Train net output #0: loss = 0.269939 (* 1 = 0.269939 loss)
I0428 19:41:12.510397 30618 sgd_solver.cpp:106] Iteration 13900, lr = 0.00520237
I0428 19:41:12.578433 30618 solver.cpp:237] Iteration 14000, loss = 0.301474
I0428 19:41:12.578513 30618 solver.cpp:253]     Train net output #0: loss = 0.301473 (* 1 = 0.301473 loss)
I0428 19:41:12.578534 30618 sgd_solver.cpp:106] Iteration 14000, lr = 0.00518611
I0428 19:41:12.646347 30618 solver.cpp:237] Iteration 14100, loss = 0.272353
I0428 19:41:12.646431 30618 solver.cpp:253]     Train net output #0: loss = 0.272352 (* 1 = 0.272352 loss)
I0428 19:41:12.646458 30618 sgd_solver.cpp:106] Iteration 14100, lr = 0.00516996
I0428 19:41:12.714223 30618 solver.cpp:237] Iteration 14200, loss = 0.262459
I0428 19:41:12.714321 30618 solver.cpp:253]     Train net output #0: loss = 0.262458 (* 1 = 0.262458 loss)
I0428 19:41:12.714344 30618 sgd_solver.cpp:106] Iteration 14200, lr = 0.00515393
I0428 19:41:12.782047 30618 solver.cpp:237] Iteration 14300, loss = 0.323732
I0428 19:41:12.782130 30618 solver.cpp:253]     Train net output #0: loss = 0.323732 (* 1 = 0.323732 loss)
I0428 19:41:12.782155 30618 sgd_solver.cpp:106] Iteration 14300, lr = 0.00513801
I0428 19:41:12.849905 30618 solver.cpp:237] Iteration 14400, loss = 0.21901
I0428 19:41:12.849987 30618 solver.cpp:253]     Train net output #0: loss = 0.21901 (* 1 = 0.21901 loss)
I0428 19:41:12.850026 30618 sgd_solver.cpp:106] Iteration 14400, lr = 0.00512221
I0428 19:41:12.918216 30618 solver.cpp:237] Iteration 14500, loss = 0.269503
I0428 19:41:12.918299 30618 solver.cpp:253]     Train net output #0: loss = 0.269502 (* 1 = 0.269502 loss)
I0428 19:41:12.918335 30618 sgd_solver.cpp:106] Iteration 14500, lr = 0.00510653
I0428 19:41:12.985819 30618 solver.cpp:237] Iteration 14600, loss = 0.300697
I0428 19:41:12.985898 30618 solver.cpp:253]     Train net output #0: loss = 0.300696 (* 1 = 0.300696 loss)
I0428 19:41:12.985918 30618 sgd_solver.cpp:106] Iteration 14600, lr = 0.00509095
I0428 19:41:13.053485 30618 solver.cpp:237] Iteration 14700, loss = 0.271938
I0428 19:41:13.053568 30618 solver.cpp:253]     Train net output #0: loss = 0.271937 (* 1 = 0.271937 loss)
I0428 19:41:13.053594 30618 sgd_solver.cpp:106] Iteration 14700, lr = 0.00507548
I0428 19:41:13.121186 30618 solver.cpp:237] Iteration 14800, loss = 0.262057
I0428 19:41:13.121269 30618 solver.cpp:253]     Train net output #0: loss = 0.262056 (* 1 = 0.262056 loss)
I0428 19:41:13.121291 30618 sgd_solver.cpp:106] Iteration 14800, lr = 0.00506012
I0428 19:41:13.168588 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:13.188848 30618 solver.cpp:237] Iteration 14900, loss = 0.32331
I0428 19:41:13.188935 30618 solver.cpp:253]     Train net output #0: loss = 0.323309 (* 1 = 0.323309 loss)
I0428 19:41:13.188958 30618 sgd_solver.cpp:106] Iteration 14900, lr = 0.00504488
I0428 19:41:13.256283 30618 solver.cpp:459] Snapshotting to binary proto file examples/mlp/mlp_test_iter_15000.caffemodel
I0428 19:41:13.256677 30618 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mlp/mlp_test_iter_15000.solverstate
I0428 19:41:13.256830 30618 solver.cpp:341] Iteration 15000, Testing net (#0)
I0428 19:41:13.347344 30618 solver.cpp:409]     Test net output #0: accuracy = 0.9228
I0428 19:41:13.347427 30618 solver.cpp:409]     Test net output #1: loss = 0.277655 (* 1 = 0.277655 loss)
I0428 19:41:13.347743 30618 solver.cpp:237] Iteration 15000, loss = 0.218654
I0428 19:41:13.347775 30618 solver.cpp:253]     Train net output #0: loss = 0.218653 (* 1 = 0.218653 loss)
I0428 19:41:13.347795 30618 sgd_solver.cpp:106] Iteration 15000, lr = 0.00502973
I0428 19:41:13.423982 30618 solver.cpp:237] Iteration 15100, loss = 0.269095
I0428 19:41:13.424075 30618 solver.cpp:253]     Train net output #0: loss = 0.269094 (* 1 = 0.269094 loss)
I0428 19:41:13.424101 30618 sgd_solver.cpp:106] Iteration 15100, lr = 0.0050147
I0428 19:41:13.503525 30618 solver.cpp:237] Iteration 15200, loss = 0.29998
I0428 19:41:13.503604 30618 solver.cpp:253]     Train net output #0: loss = 0.299979 (* 1 = 0.299979 loss)
I0428 19:41:13.503626 30618 sgd_solver.cpp:106] Iteration 15200, lr = 0.00499976
I0428 19:41:13.581625 30618 solver.cpp:237] Iteration 15300, loss = 0.271555
I0428 19:41:13.581704 30618 solver.cpp:253]     Train net output #0: loss = 0.271554 (* 1 = 0.271554 loss)
I0428 19:41:13.581723 30618 sgd_solver.cpp:106] Iteration 15300, lr = 0.00498494
I0428 19:41:13.658174 30618 solver.cpp:237] Iteration 15400, loss = 0.261679
I0428 19:41:13.658267 30618 solver.cpp:253]     Train net output #0: loss = 0.261678 (* 1 = 0.261678 loss)
I0428 19:41:13.658288 30618 sgd_solver.cpp:106] Iteration 15400, lr = 0.00497021
I0428 19:41:13.735296 30618 solver.cpp:237] Iteration 15500, loss = 0.322915
I0428 19:41:13.735404 30618 solver.cpp:253]     Train net output #0: loss = 0.322914 (* 1 = 0.322914 loss)
I0428 19:41:13.735437 30618 sgd_solver.cpp:106] Iteration 15500, lr = 0.00495558
I0428 19:41:13.814927 30618 solver.cpp:237] Iteration 15600, loss = 0.218329
I0428 19:41:13.815023 30618 solver.cpp:253]     Train net output #0: loss = 0.218328 (* 1 = 0.218328 loss)
I0428 19:41:13.815050 30618 sgd_solver.cpp:106] Iteration 15600, lr = 0.00494106
I0428 19:41:13.888953 30618 solver.cpp:237] Iteration 15700, loss = 0.268712
I0428 19:41:13.889045 30618 solver.cpp:253]     Train net output #0: loss = 0.268711 (* 1 = 0.268711 loss)
I0428 19:41:13.889065 30618 sgd_solver.cpp:106] Iteration 15700, lr = 0.00492663
I0428 19:41:13.957243 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:13.970552 30618 solver.cpp:237] Iteration 15800, loss = 0.299315
I0428 19:41:13.970633 30618 solver.cpp:253]     Train net output #0: loss = 0.299314 (* 1 = 0.299314 loss)
I0428 19:41:13.970676 30618 sgd_solver.cpp:106] Iteration 15800, lr = 0.0049123
I0428 19:41:14.047993 30618 solver.cpp:237] Iteration 15900, loss = 0.271201
I0428 19:41:14.048074 30618 solver.cpp:253]     Train net output #0: loss = 0.2712 (* 1 = 0.2712 loss)
I0428 19:41:14.048096 30618 sgd_solver.cpp:106] Iteration 15900, lr = 0.00489807
I0428 19:41:14.122381 30618 solver.cpp:237] Iteration 16000, loss = 0.261322
I0428 19:41:14.122469 30618 solver.cpp:253]     Train net output #0: loss = 0.261321 (* 1 = 0.261321 loss)
I0428 19:41:14.122494 30618 sgd_solver.cpp:106] Iteration 16000, lr = 0.00488394
I0428 19:41:14.196529 30618 solver.cpp:237] Iteration 16100, loss = 0.322546
I0428 19:41:14.196609 30618 solver.cpp:253]     Train net output #0: loss = 0.322545 (* 1 = 0.322545 loss)
I0428 19:41:14.196629 30618 sgd_solver.cpp:106] Iteration 16100, lr = 0.0048699
I0428 19:41:14.271165 30618 solver.cpp:237] Iteration 16200, loss = 0.218032
I0428 19:41:14.271247 30618 solver.cpp:253]     Train net output #0: loss = 0.218031 (* 1 = 0.218031 loss)
I0428 19:41:14.271272 30618 sgd_solver.cpp:106] Iteration 16200, lr = 0.00485595
I0428 19:41:14.345881 30618 solver.cpp:237] Iteration 16300, loss = 0.268353
I0428 19:41:14.345964 30618 solver.cpp:253]     Train net output #0: loss = 0.268352 (* 1 = 0.268352 loss)
I0428 19:41:14.345986 30618 sgd_solver.cpp:106] Iteration 16300, lr = 0.00484209
I0428 19:41:14.420768 30618 solver.cpp:237] Iteration 16400, loss = 0.298698
I0428 19:41:14.420852 30618 solver.cpp:253]     Train net output #0: loss = 0.298697 (* 1 = 0.298697 loss)
I0428 19:41:14.420876 30618 sgd_solver.cpp:106] Iteration 16400, lr = 0.00482833
I0428 19:41:14.494927 30618 solver.cpp:237] Iteration 16500, loss = 0.270872
I0428 19:41:14.495018 30618 solver.cpp:253]     Train net output #0: loss = 0.270871 (* 1 = 0.270871 loss)
I0428 19:41:14.495041 30618 sgd_solver.cpp:106] Iteration 16500, lr = 0.00481466
I0428 19:41:14.569660 30618 solver.cpp:237] Iteration 16600, loss = 0.260985
I0428 19:41:14.569741 30618 solver.cpp:253]     Train net output #0: loss = 0.260984 (* 1 = 0.260984 loss)
I0428 19:41:14.569761 30618 sgd_solver.cpp:106] Iteration 16600, lr = 0.00480108
I0428 19:41:14.640346 30618 solver.cpp:237] Iteration 16700, loss = 0.322199
I0428 19:41:14.640432 30618 solver.cpp:253]     Train net output #0: loss = 0.322198 (* 1 = 0.322198 loss)
I0428 19:41:14.640455 30618 sgd_solver.cpp:106] Iteration 16700, lr = 0.00478759
I0428 19:41:14.695761 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:14.707854 30618 solver.cpp:237] Iteration 16800, loss = 0.217759
I0428 19:41:14.707936 30618 solver.cpp:253]     Train net output #0: loss = 0.217758 (* 1 = 0.217758 loss)
I0428 19:41:14.707958 30618 sgd_solver.cpp:106] Iteration 16800, lr = 0.00477418
I0428 19:41:14.775414 30618 solver.cpp:237] Iteration 16900, loss = 0.268015
I0428 19:41:14.775499 30618 solver.cpp:253]     Train net output #0: loss = 0.268014 (* 1 = 0.268014 loss)
I0428 19:41:14.775521 30618 sgd_solver.cpp:106] Iteration 16900, lr = 0.00476086
I0428 19:41:14.842921 30618 solver.cpp:237] Iteration 17000, loss = 0.298124
I0428 19:41:14.843027 30618 solver.cpp:253]     Train net output #0: loss = 0.298123 (* 1 = 0.298123 loss)
I0428 19:41:14.843050 30618 sgd_solver.cpp:106] Iteration 17000, lr = 0.00474763
I0428 19:41:14.910498 30618 solver.cpp:237] Iteration 17100, loss = 0.270566
I0428 19:41:14.910579 30618 solver.cpp:253]     Train net output #0: loss = 0.270565 (* 1 = 0.270565 loss)
I0428 19:41:14.910612 30618 sgd_solver.cpp:106] Iteration 17100, lr = 0.00473449
I0428 19:41:14.978582 30618 solver.cpp:237] Iteration 17200, loss = 0.260667
I0428 19:41:14.978663 30618 solver.cpp:253]     Train net output #0: loss = 0.260665 (* 1 = 0.260665 loss)
I0428 19:41:14.978682 30618 sgd_solver.cpp:106] Iteration 17200, lr = 0.00472143
I0428 19:41:15.046124 30618 solver.cpp:237] Iteration 17300, loss = 0.321873
I0428 19:41:15.046208 30618 solver.cpp:253]     Train net output #0: loss = 0.321872 (* 1 = 0.321872 loss)
I0428 19:41:15.046237 30618 sgd_solver.cpp:106] Iteration 17300, lr = 0.00470845
I0428 19:41:15.113662 30618 solver.cpp:237] Iteration 17400, loss = 0.217507
I0428 19:41:15.113744 30618 solver.cpp:253]     Train net output #0: loss = 0.217506 (* 1 = 0.217506 loss)
I0428 19:41:15.113767 30618 sgd_solver.cpp:106] Iteration 17400, lr = 0.00469556
I0428 19:41:15.181367 30618 solver.cpp:237] Iteration 17500, loss = 0.267697
I0428 19:41:15.181447 30618 solver.cpp:253]     Train net output #0: loss = 0.267695 (* 1 = 0.267695 loss)
I0428 19:41:15.181468 30618 sgd_solver.cpp:106] Iteration 17500, lr = 0.00468274
I0428 19:41:15.249142 30618 solver.cpp:237] Iteration 17600, loss = 0.297589
I0428 19:41:15.249228 30618 solver.cpp:253]     Train net output #0: loss = 0.297588 (* 1 = 0.297588 loss)
I0428 19:41:15.249251 30618 sgd_solver.cpp:106] Iteration 17600, lr = 0.00467001
I0428 19:41:15.316933 30618 solver.cpp:237] Iteration 17700, loss = 0.270279
I0428 19:41:15.317013 30618 solver.cpp:253]     Train net output #0: loss = 0.270278 (* 1 = 0.270278 loss)
I0428 19:41:15.317031 30618 sgd_solver.cpp:106] Iteration 17700, lr = 0.00465736
I0428 19:41:15.372469 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:15.384621 30618 solver.cpp:237] Iteration 17800, loss = 0.260364
I0428 19:41:15.384701 30618 solver.cpp:253]     Train net output #0: loss = 0.260363 (* 1 = 0.260363 loss)
I0428 19:41:15.384726 30618 sgd_solver.cpp:106] Iteration 17800, lr = 0.00464479
I0428 19:41:15.452347 30618 solver.cpp:237] Iteration 17900, loss = 0.321566
I0428 19:41:15.452426 30618 solver.cpp:253]     Train net output #0: loss = 0.321565 (* 1 = 0.321565 loss)
I0428 19:41:15.452458 30618 sgd_solver.cpp:106] Iteration 17900, lr = 0.0046323
I0428 19:41:15.521735 30618 solver.cpp:237] Iteration 18000, loss = 0.217275
I0428 19:41:15.521818 30618 solver.cpp:253]     Train net output #0: loss = 0.217274 (* 1 = 0.217274 loss)
I0428 19:41:15.521842 30618 sgd_solver.cpp:106] Iteration 18000, lr = 0.00461989
I0428 19:41:15.590157 30618 solver.cpp:237] Iteration 18100, loss = 0.267395
I0428 19:41:15.590241 30618 solver.cpp:253]     Train net output #0: loss = 0.267394 (* 1 = 0.267394 loss)
I0428 19:41:15.590263 30618 sgd_solver.cpp:106] Iteration 18100, lr = 0.00460755
I0428 19:41:15.661197 30618 solver.cpp:237] Iteration 18200, loss = 0.29709
I0428 19:41:15.661279 30618 solver.cpp:253]     Train net output #0: loss = 0.297089 (* 1 = 0.297089 loss)
I0428 19:41:15.661303 30618 sgd_solver.cpp:106] Iteration 18200, lr = 0.00459529
I0428 19:41:15.731206 30618 solver.cpp:237] Iteration 18300, loss = 0.27001
I0428 19:41:15.731287 30618 solver.cpp:253]     Train net output #0: loss = 0.270009 (* 1 = 0.270009 loss)
I0428 19:41:15.731308 30618 sgd_solver.cpp:106] Iteration 18300, lr = 0.00458311
I0428 19:41:15.796936 30618 solver.cpp:237] Iteration 18400, loss = 0.260077
I0428 19:41:15.797015 30618 solver.cpp:253]     Train net output #0: loss = 0.260076 (* 1 = 0.260076 loss)
I0428 19:41:15.797039 30618 sgd_solver.cpp:106] Iteration 18400, lr = 0.004571
I0428 19:41:15.864254 30618 solver.cpp:237] Iteration 18500, loss = 0.321276
I0428 19:41:15.864348 30618 solver.cpp:253]     Train net output #0: loss = 0.321275 (* 1 = 0.321275 loss)
I0428 19:41:15.864389 30618 sgd_solver.cpp:106] Iteration 18500, lr = 0.00455897
I0428 19:41:15.931937 30618 solver.cpp:237] Iteration 18600, loss = 0.217061
I0428 19:41:15.932018 30618 solver.cpp:253]     Train net output #0: loss = 0.21706 (* 1 = 0.21706 loss)
I0428 19:41:15.932041 30618 sgd_solver.cpp:106] Iteration 18600, lr = 0.00454701
I0428 19:41:15.999681 30618 solver.cpp:237] Iteration 18700, loss = 0.26711
I0428 19:41:15.999763 30618 solver.cpp:253]     Train net output #0: loss = 0.267109 (* 1 = 0.267109 loss)
I0428 19:41:15.999788 30618 sgd_solver.cpp:106] Iteration 18700, lr = 0.00453512
I0428 19:41:16.067180 30618 solver.cpp:237] Iteration 18800, loss = 0.296622
I0428 19:41:16.067260 30618 solver.cpp:253]     Train net output #0: loss = 0.296621 (* 1 = 0.296621 loss)
I0428 19:41:16.067282 30618 sgd_solver.cpp:106] Iteration 18800, lr = 0.0045233
I0428 19:41:16.115697 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:16.134513 30618 solver.cpp:237] Iteration 18900, loss = 0.269756
I0428 19:41:16.134593 30618 solver.cpp:253]     Train net output #0: loss = 0.269755 (* 1 = 0.269755 loss)
I0428 19:41:16.134615 30618 sgd_solver.cpp:106] Iteration 18900, lr = 0.00451156
I0428 19:41:16.202190 30618 solver.cpp:237] Iteration 19000, loss = 0.259804
I0428 19:41:16.202271 30618 solver.cpp:253]     Train net output #0: loss = 0.259804 (* 1 = 0.259804 loss)
I0428 19:41:16.202299 30618 sgd_solver.cpp:106] Iteration 19000, lr = 0.00449989
I0428 19:41:16.231067 30624 blocking_queue.cpp:50] Waiting for data
I0428 19:41:16.286795 30618 solver.cpp:237] Iteration 19100, loss = 0.321002
I0428 19:41:16.286875 30618 solver.cpp:253]     Train net output #0: loss = 0.321001 (* 1 = 0.321001 loss)
I0428 19:41:16.286919 30618 sgd_solver.cpp:106] Iteration 19100, lr = 0.00448828
I0428 19:41:16.378129 30618 solver.cpp:237] Iteration 19200, loss = 0.216861
I0428 19:41:16.378216 30618 solver.cpp:253]     Train net output #0: loss = 0.21686 (* 1 = 0.21686 loss)
I0428 19:41:16.378239 30618 sgd_solver.cpp:106] Iteration 19200, lr = 0.00447675
I0428 19:41:16.462575 30618 solver.cpp:237] Iteration 19300, loss = 0.266841
I0428 19:41:16.462661 30618 solver.cpp:253]     Train net output #0: loss = 0.26684 (* 1 = 0.26684 loss)
I0428 19:41:16.462682 30618 sgd_solver.cpp:106] Iteration 19300, lr = 0.00446529
I0428 19:41:16.551079 30618 solver.cpp:237] Iteration 19400, loss = 0.296184
I0428 19:41:16.551162 30618 solver.cpp:253]     Train net output #0: loss = 0.296183 (* 1 = 0.296183 loss)
I0428 19:41:16.551187 30618 sgd_solver.cpp:106] Iteration 19400, lr = 0.00445389
I0428 19:41:16.640977 30618 solver.cpp:237] Iteration 19500, loss = 0.269517
I0428 19:41:16.641027 30618 solver.cpp:253]     Train net output #0: loss = 0.269516 (* 1 = 0.269516 loss)
I0428 19:41:16.641038 30618 sgd_solver.cpp:106] Iteration 19500, lr = 0.00444256
I0428 19:41:16.723995 30618 solver.cpp:237] Iteration 19600, loss = 0.259545
I0428 19:41:16.724045 30618 solver.cpp:253]     Train net output #0: loss = 0.259544 (* 1 = 0.259544 loss)
I0428 19:41:16.724056 30618 sgd_solver.cpp:106] Iteration 19600, lr = 0.0044313
I0428 19:41:16.806902 30618 solver.cpp:237] Iteration 19700, loss = 0.320742
I0428 19:41:16.806942 30618 solver.cpp:253]     Train net output #0: loss = 0.320741 (* 1 = 0.320741 loss)
I0428 19:41:16.806952 30618 sgd_solver.cpp:106] Iteration 19700, lr = 0.00442011
I0428 19:41:16.888725 30618 solver.cpp:237] Iteration 19800, loss = 0.216676
I0428 19:41:16.888805 30618 solver.cpp:253]     Train net output #0: loss = 0.216675 (* 1 = 0.216675 loss)
I0428 19:41:16.888828 30618 sgd_solver.cpp:106] Iteration 19800, lr = 0.00440898
I0428 19:41:16.951310 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:16.972036 30618 solver.cpp:237] Iteration 19900, loss = 0.266585
I0428 19:41:16.972106 30618 solver.cpp:253]     Train net output #0: loss = 0.266584 (* 1 = 0.266584 loss)
I0428 19:41:16.972127 30618 sgd_solver.cpp:106] Iteration 19900, lr = 0.00439791
I0428 19:41:17.053926 30618 solver.cpp:459] Snapshotting to binary proto file examples/mlp/mlp_test_iter_20000.caffemodel
I0428 19:41:17.054309 30618 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mlp/mlp_test_iter_20000.solverstate
I0428 19:41:17.054463 30618 solver.cpp:341] Iteration 20000, Testing net (#0)
I0428 19:41:17.188446 30618 solver.cpp:409]     Test net output #0: accuracy = 0.9217
I0428 19:41:17.188526 30618 solver.cpp:409]     Test net output #1: loss = 0.276565 (* 1 = 0.276565 loss)
I0428 19:41:17.188868 30618 solver.cpp:237] Iteration 20000, loss = 0.295773
I0428 19:41:17.188910 30618 solver.cpp:253]     Train net output #0: loss = 0.295772 (* 1 = 0.295772 loss)
I0428 19:41:17.188936 30618 sgd_solver.cpp:106] Iteration 20000, lr = 0.00438691
I0428 19:41:17.298873 30618 solver.cpp:237] Iteration 20100, loss = 0.269291
I0428 19:41:17.298959 30618 solver.cpp:253]     Train net output #0: loss = 0.26929 (* 1 = 0.26929 loss)
I0428 19:41:17.298984 30618 sgd_solver.cpp:106] Iteration 20100, lr = 0.00437598
I0428 19:41:17.382071 30618 solver.cpp:237] Iteration 20200, loss = 0.259298
I0428 19:41:17.382153 30618 solver.cpp:253]     Train net output #0: loss = 0.259297 (* 1 = 0.259297 loss)
I0428 19:41:17.382175 30618 sgd_solver.cpp:106] Iteration 20200, lr = 0.00436511
I0428 19:41:17.464552 30618 solver.cpp:237] Iteration 20300, loss = 0.320495
I0428 19:41:17.464634 30618 solver.cpp:253]     Train net output #0: loss = 0.320494 (* 1 = 0.320494 loss)
I0428 19:41:17.464658 30618 sgd_solver.cpp:106] Iteration 20300, lr = 0.0043543
I0428 19:41:17.547143 30618 solver.cpp:237] Iteration 20400, loss = 0.216504
I0428 19:41:17.547226 30618 solver.cpp:253]     Train net output #0: loss = 0.216503 (* 1 = 0.216503 loss)
I0428 19:41:17.547252 30618 sgd_solver.cpp:106] Iteration 20400, lr = 0.00434355
I0428 19:41:17.629389 30618 solver.cpp:237] Iteration 20500, loss = 0.266342
I0428 19:41:17.629468 30618 solver.cpp:253]     Train net output #0: loss = 0.26634 (* 1 = 0.26634 loss)
I0428 19:41:17.629488 30618 sgd_solver.cpp:106] Iteration 20500, lr = 0.00433286
I0428 19:41:17.712200 30618 solver.cpp:237] Iteration 20600, loss = 0.295387
I0428 19:41:17.712283 30618 solver.cpp:253]     Train net output #0: loss = 0.295386 (* 1 = 0.295386 loss)
I0428 19:41:17.712306 30618 sgd_solver.cpp:106] Iteration 20600, lr = 0.00432224
I0428 19:41:17.795462 30618 solver.cpp:237] Iteration 20700, loss = 0.269076
I0428 19:41:17.795545 30618 solver.cpp:253]     Train net output #0: loss = 0.269075 (* 1 = 0.269075 loss)
I0428 19:41:17.795569 30618 sgd_solver.cpp:106] Iteration 20700, lr = 0.00431168
I0428 19:41:17.878676 30618 solver.cpp:237] Iteration 20800, loss = 0.259062
I0428 19:41:17.878759 30618 solver.cpp:253]     Train net output #0: loss = 0.259061 (* 1 = 0.259061 loss)
I0428 19:41:17.878787 30618 sgd_solver.cpp:106] Iteration 20800, lr = 0.00430117
I0428 19:41:17.902999 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:17.964189 30618 solver.cpp:237] Iteration 20900, loss = 0.32026
I0428 19:41:17.964270 30618 solver.cpp:253]     Train net output #0: loss = 0.32026 (* 1 = 0.32026 loss)
I0428 19:41:17.964368 30618 sgd_solver.cpp:106] Iteration 20900, lr = 0.00429073
I0428 19:41:18.048043 30618 solver.cpp:237] Iteration 21000, loss = 0.216343
I0428 19:41:18.048128 30618 solver.cpp:253]     Train net output #0: loss = 0.216342 (* 1 = 0.216342 loss)
I0428 19:41:18.048148 30618 sgd_solver.cpp:106] Iteration 21000, lr = 0.00428034
I0428 19:41:18.131399 30618 solver.cpp:237] Iteration 21100, loss = 0.26611
I0428 19:41:18.131484 30618 solver.cpp:253]     Train net output #0: loss = 0.266109 (* 1 = 0.266109 loss)
I0428 19:41:18.131508 30618 sgd_solver.cpp:106] Iteration 21100, lr = 0.00427002
I0428 19:41:18.214841 30618 solver.cpp:237] Iteration 21200, loss = 0.295024
I0428 19:41:18.214923 30618 solver.cpp:253]     Train net output #0: loss = 0.295023 (* 1 = 0.295023 loss)
I0428 19:41:18.214946 30618 sgd_solver.cpp:106] Iteration 21200, lr = 0.00425975
I0428 19:41:18.298671 30618 solver.cpp:237] Iteration 21300, loss = 0.268872
I0428 19:41:18.298773 30618 solver.cpp:253]     Train net output #0: loss = 0.268871 (* 1 = 0.268871 loss)
I0428 19:41:18.298795 30618 sgd_solver.cpp:106] Iteration 21300, lr = 0.00424954
I0428 19:41:18.382251 30618 solver.cpp:237] Iteration 21400, loss = 0.258837
I0428 19:41:18.382338 30618 solver.cpp:253]     Train net output #0: loss = 0.258836 (* 1 = 0.258836 loss)
I0428 19:41:18.382429 30618 sgd_solver.cpp:106] Iteration 21400, lr = 0.00423938
I0428 19:41:18.466231 30618 solver.cpp:237] Iteration 21500, loss = 0.320037
I0428 19:41:18.466315 30618 solver.cpp:253]     Train net output #0: loss = 0.320036 (* 1 = 0.320036 loss)
I0428 19:41:18.466339 30618 sgd_solver.cpp:106] Iteration 21500, lr = 0.00422929
I0428 19:41:18.550067 30618 solver.cpp:237] Iteration 21600, loss = 0.216192
I0428 19:41:18.550150 30618 solver.cpp:253]     Train net output #0: loss = 0.216191 (* 1 = 0.216191 loss)
I0428 19:41:18.550173 30618 sgd_solver.cpp:106] Iteration 21600, lr = 0.00421924
I0428 19:41:18.633420 30618 solver.cpp:237] Iteration 21700, loss = 0.26589
I0428 19:41:18.633504 30618 solver.cpp:253]     Train net output #0: loss = 0.265889 (* 1 = 0.265889 loss)
I0428 19:41:18.633592 30618 sgd_solver.cpp:106] Iteration 21700, lr = 0.00420926
I0428 19:41:18.717043 30618 solver.cpp:237] Iteration 21800, loss = 0.294681
I0428 19:41:18.717126 30618 solver.cpp:253]     Train net output #0: loss = 0.29468 (* 1 = 0.29468 loss)
I0428 19:41:18.717149 30618 sgd_solver.cpp:106] Iteration 21800, lr = 0.00419933
I0428 19:41:18.741441 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:18.800335 30618 solver.cpp:237] Iteration 21900, loss = 0.268677
I0428 19:41:18.800420 30618 solver.cpp:253]     Train net output #0: loss = 0.268676 (* 1 = 0.268676 loss)
I0428 19:41:18.800442 30618 sgd_solver.cpp:106] Iteration 21900, lr = 0.00418945
I0428 19:41:18.883337 30618 solver.cpp:237] Iteration 22000, loss = 0.258622
I0428 19:41:18.883419 30618 solver.cpp:253]     Train net output #0: loss = 0.258621 (* 1 = 0.258621 loss)
I0428 19:41:18.883443 30618 sgd_solver.cpp:106] Iteration 22000, lr = 0.00417963
I0428 19:41:18.966951 30618 solver.cpp:237] Iteration 22100, loss = 0.319824
I0428 19:41:18.967034 30618 solver.cpp:253]     Train net output #0: loss = 0.319823 (* 1 = 0.319823 loss)
I0428 19:41:18.967125 30618 sgd_solver.cpp:106] Iteration 22100, lr = 0.00416986
I0428 19:41:19.050510 30618 solver.cpp:237] Iteration 22200, loss = 0.216051
I0428 19:41:19.050590 30618 solver.cpp:253]     Train net output #0: loss = 0.21605 (* 1 = 0.21605 loss)
I0428 19:41:19.050614 30618 sgd_solver.cpp:106] Iteration 22200, lr = 0.00416014
I0428 19:41:19.133729 30618 solver.cpp:237] Iteration 22300, loss = 0.26568
I0428 19:41:19.133815 30618 solver.cpp:253]     Train net output #0: loss = 0.265679 (* 1 = 0.265679 loss)
I0428 19:41:19.133960 30618 sgd_solver.cpp:106] Iteration 22300, lr = 0.00415048
I0428 19:41:19.217270 30618 solver.cpp:237] Iteration 22400, loss = 0.294357
I0428 19:41:19.217352 30618 solver.cpp:253]     Train net output #0: loss = 0.294356 (* 1 = 0.294356 loss)
I0428 19:41:19.217375 30618 sgd_solver.cpp:106] Iteration 22400, lr = 0.00414087
I0428 19:41:19.300642 30618 solver.cpp:237] Iteration 22500, loss = 0.268491
I0428 19:41:19.300724 30618 solver.cpp:253]     Train net output #0: loss = 0.26849 (* 1 = 0.26849 loss)
I0428 19:41:19.300745 30618 sgd_solver.cpp:106] Iteration 22500, lr = 0.00413131
I0428 19:41:19.383827 30618 solver.cpp:237] Iteration 22600, loss = 0.258417
I0428 19:41:19.383909 30618 solver.cpp:253]     Train net output #0: loss = 0.258416 (* 1 = 0.258416 loss)
I0428 19:41:19.384001 30618 sgd_solver.cpp:106] Iteration 22600, lr = 0.0041218
I0428 19:41:19.467025 30618 solver.cpp:237] Iteration 22700, loss = 0.319621
I0428 19:41:19.467108 30618 solver.cpp:253]     Train net output #0: loss = 0.31962 (* 1 = 0.31962 loss)
I0428 19:41:19.467211 30618 sgd_solver.cpp:106] Iteration 22700, lr = 0.00411234
I0428 19:41:19.550159 30618 solver.cpp:237] Iteration 22800, loss = 0.215918
I0428 19:41:19.550257 30618 solver.cpp:253]     Train net output #0: loss = 0.215917 (* 1 = 0.215917 loss)
I0428 19:41:19.550292 30618 sgd_solver.cpp:106] Iteration 22800, lr = 0.00410293
I0428 19:41:19.574525 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:19.633412 30618 solver.cpp:237] Iteration 22900, loss = 0.26548
I0428 19:41:19.633496 30618 solver.cpp:253]     Train net output #0: loss = 0.265479 (* 1 = 0.265479 loss)
I0428 19:41:19.633522 30618 sgd_solver.cpp:106] Iteration 22900, lr = 0.00409358
I0428 19:41:19.717320 30618 solver.cpp:237] Iteration 23000, loss = 0.294051
I0428 19:41:19.717401 30618 solver.cpp:253]     Train net output #0: loss = 0.294051 (* 1 = 0.294051 loss)
I0428 19:41:19.717424 30618 sgd_solver.cpp:106] Iteration 23000, lr = 0.00408427
I0428 19:41:19.800837 30618 solver.cpp:237] Iteration 23100, loss = 0.268313
I0428 19:41:19.800930 30618 solver.cpp:253]     Train net output #0: loss = 0.268312 (* 1 = 0.268312 loss)
I0428 19:41:19.800956 30618 sgd_solver.cpp:106] Iteration 23100, lr = 0.00407501
I0428 19:41:19.884011 30618 solver.cpp:237] Iteration 23200, loss = 0.258219
I0428 19:41:19.884090 30618 solver.cpp:253]     Train net output #0: loss = 0.258219 (* 1 = 0.258219 loss)
I0428 19:41:19.884114 30618 sgd_solver.cpp:106] Iteration 23200, lr = 0.0040658
I0428 19:41:19.967300 30618 solver.cpp:237] Iteration 23300, loss = 0.319427
I0428 19:41:19.967381 30618 solver.cpp:253]     Train net output #0: loss = 0.319426 (* 1 = 0.319426 loss)
I0428 19:41:19.967476 30618 sgd_solver.cpp:106] Iteration 23300, lr = 0.00405664
I0428 19:41:20.051005 30618 solver.cpp:237] Iteration 23400, loss = 0.215793
I0428 19:41:20.051086 30618 solver.cpp:253]     Train net output #0: loss = 0.215793 (* 1 = 0.215793 loss)
I0428 19:41:20.051107 30618 sgd_solver.cpp:106] Iteration 23400, lr = 0.00404753
I0428 19:41:20.134776 30618 solver.cpp:237] Iteration 23500, loss = 0.265288
I0428 19:41:20.134866 30618 solver.cpp:253]     Train net output #0: loss = 0.265287 (* 1 = 0.265287 loss)
I0428 19:41:20.134888 30618 sgd_solver.cpp:106] Iteration 23500, lr = 0.00403847
I0428 19:41:20.218467 30618 solver.cpp:237] Iteration 23600, loss = 0.293762
I0428 19:41:20.218547 30618 solver.cpp:253]     Train net output #0: loss = 0.293761 (* 1 = 0.293761 loss)
I0428 19:41:20.218569 30618 sgd_solver.cpp:106] Iteration 23600, lr = 0.00402945
I0428 19:41:20.289216 30618 solver.cpp:237] Iteration 23700, loss = 0.268142
I0428 19:41:20.289304 30618 solver.cpp:253]     Train net output #0: loss = 0.268141 (* 1 = 0.268141 loss)
I0428 19:41:20.289329 30618 sgd_solver.cpp:106] Iteration 23700, lr = 0.00402048
I0428 19:41:20.371554 30618 solver.cpp:237] Iteration 23800, loss = 0.258031
I0428 19:41:20.371644 30618 solver.cpp:253]     Train net output #0: loss = 0.25803 (* 1 = 0.25803 loss)
I0428 19:41:20.371739 30618 sgd_solver.cpp:106] Iteration 23800, lr = 0.00401155
I0428 19:41:20.396334 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:20.453866 30618 solver.cpp:237] Iteration 23900, loss = 0.319241
I0428 19:41:20.453945 30618 solver.cpp:253]     Train net output #0: loss = 0.319241 (* 1 = 0.319241 loss)
I0428 19:41:20.453971 30618 sgd_solver.cpp:106] Iteration 23900, lr = 0.00400267
I0428 19:41:20.536214 30618 solver.cpp:237] Iteration 24000, loss = 0.215676
I0428 19:41:20.536300 30618 solver.cpp:253]     Train net output #0: loss = 0.215675 (* 1 = 0.215675 loss)
I0428 19:41:20.536322 30618 sgd_solver.cpp:106] Iteration 24000, lr = 0.00399384
I0428 19:41:20.618626 30618 solver.cpp:237] Iteration 24100, loss = 0.265105
I0428 19:41:20.618708 30618 solver.cpp:253]     Train net output #0: loss = 0.265105 (* 1 = 0.265105 loss)
I0428 19:41:20.618732 30618 sgd_solver.cpp:106] Iteration 24100, lr = 0.00398505
I0428 19:41:20.701625 30618 solver.cpp:237] Iteration 24200, loss = 0.293488
I0428 19:41:20.701707 30618 solver.cpp:253]     Train net output #0: loss = 0.293488 (* 1 = 0.293488 loss)
I0428 19:41:20.701730 30618 sgd_solver.cpp:106] Iteration 24200, lr = 0.00397631
I0428 19:41:20.784881 30618 solver.cpp:237] Iteration 24300, loss = 0.267978
I0428 19:41:20.784993 30618 solver.cpp:253]     Train net output #0: loss = 0.267977 (* 1 = 0.267977 loss)
I0428 19:41:20.785017 30618 sgd_solver.cpp:106] Iteration 24300, lr = 0.00396761
I0428 19:41:20.867801 30618 solver.cpp:237] Iteration 24400, loss = 0.257849
I0428 19:41:20.867883 30618 solver.cpp:253]     Train net output #0: loss = 0.257849 (* 1 = 0.257849 loss)
I0428 19:41:20.867904 30618 sgd_solver.cpp:106] Iteration 24400, lr = 0.00395896
I0428 19:41:20.951325 30618 solver.cpp:237] Iteration 24500, loss = 0.319063
I0428 19:41:20.951406 30618 solver.cpp:253]     Train net output #0: loss = 0.319063 (* 1 = 0.319063 loss)
I0428 19:41:20.951427 30618 sgd_solver.cpp:106] Iteration 24500, lr = 0.00395035
I0428 19:41:21.036478 30618 solver.cpp:237] Iteration 24600, loss = 0.215565
I0428 19:41:21.036558 30618 solver.cpp:253]     Train net output #0: loss = 0.215564 (* 1 = 0.215564 loss)
I0428 19:41:21.036581 30618 sgd_solver.cpp:106] Iteration 24600, lr = 0.00394178
I0428 19:41:21.119729 30618 solver.cpp:237] Iteration 24700, loss = 0.26493
I0428 19:41:21.119812 30618 solver.cpp:253]     Train net output #0: loss = 0.26493 (* 1 = 0.26493 loss)
I0428 19:41:21.119838 30618 sgd_solver.cpp:106] Iteration 24700, lr = 0.00393326
I0428 19:41:21.203095 30618 solver.cpp:237] Iteration 24800, loss = 0.293228
I0428 19:41:21.203178 30618 solver.cpp:253]     Train net output #0: loss = 0.293228 (* 1 = 0.293228 loss)
I0428 19:41:21.203202 30618 sgd_solver.cpp:106] Iteration 24800, lr = 0.00392478
I0428 19:41:21.228432 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:21.287225 30618 solver.cpp:237] Iteration 24900, loss = 0.26782
I0428 19:41:21.287307 30618 solver.cpp:253]     Train net output #0: loss = 0.267819 (* 1 = 0.267819 loss)
I0428 19:41:21.287328 30618 sgd_solver.cpp:106] Iteration 24900, lr = 0.00391634
I0428 19:41:21.369760 30618 solver.cpp:459] Snapshotting to binary proto file examples/mlp/mlp_test_iter_25000.caffemodel
I0428 19:41:21.370137 30618 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mlp/mlp_test_iter_25000.solverstate
I0428 19:41:21.370290 30618 solver.cpp:341] Iteration 25000, Testing net (#0)
I0428 19:41:21.468780 30618 solver.cpp:409]     Test net output #0: accuracy = 0.923
I0428 19:41:21.468868 30618 solver.cpp:409]     Test net output #1: loss = 0.274973 (* 1 = 0.274973 loss)
I0428 19:41:21.469211 30618 solver.cpp:237] Iteration 25000, loss = 0.257675
I0428 19:41:21.469249 30618 solver.cpp:253]     Train net output #0: loss = 0.257675 (* 1 = 0.257675 loss)
I0428 19:41:21.469272 30618 sgd_solver.cpp:106] Iteration 25000, lr = 0.00390795
I0428 19:41:21.589269 30618 solver.cpp:237] Iteration 25100, loss = 0.318892
I0428 19:41:21.589357 30618 solver.cpp:253]     Train net output #0: loss = 0.318892 (* 1 = 0.318892 loss)
I0428 19:41:21.589381 30618 sgd_solver.cpp:106] Iteration 25100, lr = 0.0038996
I0428 19:41:21.673267 30618 solver.cpp:237] Iteration 25200, loss = 0.21546
I0428 19:41:21.673351 30618 solver.cpp:253]     Train net output #0: loss = 0.215459 (* 1 = 0.215459 loss)
I0428 19:41:21.673452 30618 sgd_solver.cpp:106] Iteration 25200, lr = 0.00389128
I0428 19:41:21.755504 30618 solver.cpp:237] Iteration 25300, loss = 0.264763
I0428 19:41:21.755583 30618 solver.cpp:253]     Train net output #0: loss = 0.264762 (* 1 = 0.264762 loss)
I0428 19:41:21.755604 30618 sgd_solver.cpp:106] Iteration 25300, lr = 0.00388301
I0428 19:41:21.838114 30618 solver.cpp:237] Iteration 25400, loss = 0.292982
I0428 19:41:21.838199 30618 solver.cpp:253]     Train net output #0: loss = 0.292981 (* 1 = 0.292981 loss)
I0428 19:41:21.838225 30618 sgd_solver.cpp:106] Iteration 25400, lr = 0.00387478
I0428 19:41:21.921371 30618 solver.cpp:237] Iteration 25500, loss = 0.267667
I0428 19:41:21.921452 30618 solver.cpp:253]     Train net output #0: loss = 0.267667 (* 1 = 0.267667 loss)
I0428 19:41:21.921473 30618 sgd_solver.cpp:106] Iteration 25500, lr = 0.0038666
I0428 19:41:22.004425 30618 solver.cpp:237] Iteration 25600, loss = 0.257508
I0428 19:41:22.004520 30618 solver.cpp:253]     Train net output #0: loss = 0.257508 (* 1 = 0.257508 loss)
I0428 19:41:22.004561 30618 sgd_solver.cpp:106] Iteration 25600, lr = 0.00385845
I0428 19:41:22.087875 30618 solver.cpp:237] Iteration 25700, loss = 0.318729
I0428 19:41:22.087955 30618 solver.cpp:253]     Train net output #0: loss = 0.318728 (* 1 = 0.318728 loss)
I0428 19:41:22.088047 30618 sgd_solver.cpp:106] Iteration 25700, lr = 0.00385034
I0428 19:41:22.147150 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:22.171218 30618 solver.cpp:237] Iteration 25800, loss = 0.21536
I0428 19:41:22.171301 30618 solver.cpp:253]     Train net output #0: loss = 0.21536 (* 1 = 0.21536 loss)
I0428 19:41:22.171324 30618 sgd_solver.cpp:106] Iteration 25800, lr = 0.00384227
I0428 19:41:22.254506 30618 solver.cpp:237] Iteration 25900, loss = 0.264602
I0428 19:41:22.254590 30618 solver.cpp:253]     Train net output #0: loss = 0.264602 (* 1 = 0.264602 loss)
I0428 19:41:22.254617 30618 sgd_solver.cpp:106] Iteration 25900, lr = 0.00383424
I0428 19:41:22.337369 30618 solver.cpp:237] Iteration 26000, loss = 0.292747
I0428 19:41:22.337450 30618 solver.cpp:253]     Train net output #0: loss = 0.292747 (* 1 = 0.292747 loss)
I0428 19:41:22.337472 30618 sgd_solver.cpp:106] Iteration 26000, lr = 0.00382625
I0428 19:41:22.420404 30618 solver.cpp:237] Iteration 26100, loss = 0.26752
I0428 19:41:22.420483 30618 solver.cpp:253]     Train net output #0: loss = 0.26752 (* 1 = 0.26752 loss)
I0428 19:41:22.420572 30618 sgd_solver.cpp:106] Iteration 26100, lr = 0.0038183
I0428 19:41:22.503720 30618 solver.cpp:237] Iteration 26200, loss = 0.257348
I0428 19:41:22.503798 30618 solver.cpp:253]     Train net output #0: loss = 0.257347 (* 1 = 0.257347 loss)
I0428 19:41:22.503883 30618 sgd_solver.cpp:106] Iteration 26200, lr = 0.00381038
I0428 19:41:22.586683 30618 solver.cpp:237] Iteration 26300, loss = 0.318571
I0428 19:41:22.586766 30618 solver.cpp:253]     Train net output #0: loss = 0.318571 (* 1 = 0.318571 loss)
I0428 19:41:22.586859 30618 sgd_solver.cpp:106] Iteration 26300, lr = 0.00380251
I0428 19:41:22.669970 30618 solver.cpp:237] Iteration 26400, loss = 0.215266
I0428 19:41:22.670049 30618 solver.cpp:253]     Train net output #0: loss = 0.215265 (* 1 = 0.215265 loss)
I0428 19:41:22.670073 30618 sgd_solver.cpp:106] Iteration 26400, lr = 0.00379467
I0428 19:41:22.753685 30618 solver.cpp:237] Iteration 26500, loss = 0.264448
I0428 19:41:22.753767 30618 solver.cpp:253]     Train net output #0: loss = 0.264448 (* 1 = 0.264448 loss)
I0428 19:41:22.753795 30618 sgd_solver.cpp:106] Iteration 26500, lr = 0.00378687
I0428 19:41:22.837098 30618 solver.cpp:237] Iteration 26600, loss = 0.292524
I0428 19:41:22.837183 30618 solver.cpp:253]     Train net output #0: loss = 0.292524 (* 1 = 0.292524 loss)
I0428 19:41:22.837211 30618 sgd_solver.cpp:106] Iteration 26600, lr = 0.00377911
I0428 19:41:22.921008 30618 solver.cpp:237] Iteration 26700, loss = 0.267378
I0428 19:41:22.921088 30618 solver.cpp:253]     Train net output #0: loss = 0.267377 (* 1 = 0.267377 loss)
I0428 19:41:22.921111 30618 sgd_solver.cpp:106] Iteration 26700, lr = 0.00377138
I0428 19:41:22.980120 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:23.004325 30618 solver.cpp:237] Iteration 26800, loss = 0.257193
I0428 19:41:23.004406 30618 solver.cpp:253]     Train net output #0: loss = 0.257193 (* 1 = 0.257193 loss)
I0428 19:41:23.004429 30618 sgd_solver.cpp:106] Iteration 26800, lr = 0.00376369
I0428 19:41:23.087517 30618 solver.cpp:237] Iteration 26900, loss = 0.31842
I0428 19:41:23.087597 30618 solver.cpp:253]     Train net output #0: loss = 0.31842 (* 1 = 0.31842 loss)
I0428 19:41:23.087616 30618 sgd_solver.cpp:106] Iteration 26900, lr = 0.00375604
I0428 19:41:23.171041 30618 solver.cpp:237] Iteration 27000, loss = 0.215176
I0428 19:41:23.171126 30618 solver.cpp:253]     Train net output #0: loss = 0.215176 (* 1 = 0.215176 loss)
I0428 19:41:23.171154 30618 sgd_solver.cpp:106] Iteration 27000, lr = 0.00374842
I0428 19:41:23.254739 30618 solver.cpp:237] Iteration 27100, loss = 0.2643
I0428 19:41:23.254838 30618 solver.cpp:253]     Train net output #0: loss = 0.2643 (* 1 = 0.2643 loss)
I0428 19:41:23.254861 30618 sgd_solver.cpp:106] Iteration 27100, lr = 0.00374084
I0428 19:41:23.332263 30618 solver.cpp:237] Iteration 27200, loss = 0.292312
I0428 19:41:23.332342 30618 solver.cpp:253]     Train net output #0: loss = 0.292312 (* 1 = 0.292312 loss)
I0428 19:41:23.332362 30618 sgd_solver.cpp:106] Iteration 27200, lr = 0.0037333
I0428 19:41:23.414510 30618 solver.cpp:237] Iteration 27300, loss = 0.26724
I0428 19:41:23.414599 30618 solver.cpp:253]     Train net output #0: loss = 0.26724 (* 1 = 0.26724 loss)
I0428 19:41:23.414623 30618 sgd_solver.cpp:106] Iteration 27300, lr = 0.00372579
I0428 19:41:23.496441 30618 solver.cpp:237] Iteration 27400, loss = 0.257044
I0428 19:41:23.496523 30618 solver.cpp:253]     Train net output #0: loss = 0.257044 (* 1 = 0.257044 loss)
I0428 19:41:23.496543 30618 sgd_solver.cpp:106] Iteration 27400, lr = 0.00371832
I0428 19:41:23.578289 30618 solver.cpp:237] Iteration 27500, loss = 0.318274
I0428 19:41:23.578373 30618 solver.cpp:253]     Train net output #0: loss = 0.318274 (* 1 = 0.318274 loss)
I0428 19:41:23.578397 30618 sgd_solver.cpp:106] Iteration 27500, lr = 0.00371088
I0428 19:41:23.660249 30618 solver.cpp:237] Iteration 27600, loss = 0.215091
I0428 19:41:23.660331 30618 solver.cpp:253]     Train net output #0: loss = 0.215091 (* 1 = 0.215091 loss)
I0428 19:41:23.660353 30618 sgd_solver.cpp:106] Iteration 27600, lr = 0.00370347
I0428 19:41:23.742090 30618 solver.cpp:237] Iteration 27700, loss = 0.264159
I0428 19:41:23.742174 30618 solver.cpp:253]     Train net output #0: loss = 0.264158 (* 1 = 0.264158 loss)
I0428 19:41:23.742208 30618 sgd_solver.cpp:106] Iteration 27700, lr = 0.0036961
I0428 19:41:23.804479 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:23.824098 30618 solver.cpp:237] Iteration 27800, loss = 0.29211
I0428 19:41:23.824178 30618 solver.cpp:253]     Train net output #0: loss = 0.29211 (* 1 = 0.29211 loss)
I0428 19:41:23.824198 30618 sgd_solver.cpp:106] Iteration 27800, lr = 0.00368877
I0428 19:41:23.906327 30618 solver.cpp:237] Iteration 27900, loss = 0.267106
I0428 19:41:23.906404 30618 solver.cpp:253]     Train net output #0: loss = 0.267106 (* 1 = 0.267106 loss)
I0428 19:41:23.906424 30618 sgd_solver.cpp:106] Iteration 27900, lr = 0.00368146
I0428 19:41:23.991102 30618 solver.cpp:237] Iteration 28000, loss = 0.256901
I0428 19:41:23.991183 30618 solver.cpp:253]     Train net output #0: loss = 0.2569 (* 1 = 0.2569 loss)
I0428 19:41:23.991204 30618 sgd_solver.cpp:106] Iteration 28000, lr = 0.0036742
I0428 19:41:24.075006 30618 solver.cpp:237] Iteration 28100, loss = 0.318134
I0428 19:41:24.075088 30618 solver.cpp:253]     Train net output #0: loss = 0.318133 (* 1 = 0.318133 loss)
I0428 19:41:24.075111 30618 sgd_solver.cpp:106] Iteration 28100, lr = 0.00366696
I0428 19:41:24.136245 30618 solver.cpp:237] Iteration 28200, loss = 0.21501
I0428 19:41:24.136328 30618 solver.cpp:253]     Train net output #0: loss = 0.215009 (* 1 = 0.215009 loss)
I0428 19:41:24.136350 30618 sgd_solver.cpp:106] Iteration 28200, lr = 0.00365976
I0428 19:41:24.212139 30618 solver.cpp:237] Iteration 28300, loss = 0.264022
I0428 19:41:24.212224 30618 solver.cpp:253]     Train net output #0: loss = 0.264022 (* 1 = 0.264022 loss)
I0428 19:41:24.212245 30618 sgd_solver.cpp:106] Iteration 28300, lr = 0.00365259
I0428 19:41:24.288661 30618 solver.cpp:237] Iteration 28400, loss = 0.291917
I0428 19:41:24.288741 30618 solver.cpp:253]     Train net output #0: loss = 0.291917 (* 1 = 0.291917 loss)
I0428 19:41:24.288763 30618 sgd_solver.cpp:106] Iteration 28400, lr = 0.00364545
I0428 19:41:24.364729 30618 solver.cpp:237] Iteration 28500, loss = 0.266977
I0428 19:41:24.364810 30618 solver.cpp:253]     Train net output #0: loss = 0.266976 (* 1 = 0.266976 loss)
I0428 19:41:24.364830 30618 sgd_solver.cpp:106] Iteration 28500, lr = 0.00363835
I0428 19:41:24.441332 30618 solver.cpp:237] Iteration 28600, loss = 0.256762
I0428 19:41:24.441424 30618 solver.cpp:253]     Train net output #0: loss = 0.256762 (* 1 = 0.256762 loss)
I0428 19:41:24.441457 30618 sgd_solver.cpp:106] Iteration 28600, lr = 0.00363128
I0428 19:41:24.517518 30618 solver.cpp:237] Iteration 28700, loss = 0.317998
I0428 19:41:24.517606 30618 solver.cpp:253]     Train net output #0: loss = 0.317998 (* 1 = 0.317998 loss)
I0428 19:41:24.517629 30618 sgd_solver.cpp:106] Iteration 28700, lr = 0.00362424
I0428 19:41:24.575428 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:24.593727 30618 solver.cpp:237] Iteration 28800, loss = 0.214933
I0428 19:41:24.593807 30618 solver.cpp:253]     Train net output #0: loss = 0.214932 (* 1 = 0.214932 loss)
I0428 19:41:24.593825 30618 sgd_solver.cpp:106] Iteration 28800, lr = 0.00361723
I0428 19:41:24.670137 30618 solver.cpp:237] Iteration 28900, loss = 0.263892
I0428 19:41:24.670218 30618 solver.cpp:253]     Train net output #0: loss = 0.263891 (* 1 = 0.263891 loss)
I0428 19:41:24.670239 30618 sgd_solver.cpp:106] Iteration 28900, lr = 0.00361025
I0428 19:41:24.745795 30618 solver.cpp:237] Iteration 29000, loss = 0.291733
I0428 19:41:24.745879 30618 solver.cpp:253]     Train net output #0: loss = 0.291733 (* 1 = 0.291733 loss)
I0428 19:41:24.745976 30618 sgd_solver.cpp:106] Iteration 29000, lr = 0.00360331
I0428 19:41:24.822180 30618 solver.cpp:237] Iteration 29100, loss = 0.266851
I0428 19:41:24.822257 30618 solver.cpp:253]     Train net output #0: loss = 0.26685 (* 1 = 0.26685 loss)
I0428 19:41:24.822279 30618 sgd_solver.cpp:106] Iteration 29100, lr = 0.0035964
I0428 19:41:24.898054 30618 solver.cpp:237] Iteration 29200, loss = 0.256629
I0428 19:41:24.898138 30618 solver.cpp:253]     Train net output #0: loss = 0.256628 (* 1 = 0.256628 loss)
I0428 19:41:24.898164 30618 sgd_solver.cpp:106] Iteration 29200, lr = 0.00358951
I0428 19:41:24.974531 30618 solver.cpp:237] Iteration 29300, loss = 0.317868
I0428 19:41:24.974612 30618 solver.cpp:253]     Train net output #0: loss = 0.317867 (* 1 = 0.317867 loss)
I0428 19:41:24.974634 30618 sgd_solver.cpp:106] Iteration 29300, lr = 0.00358266
I0428 19:41:25.050427 30618 solver.cpp:237] Iteration 29400, loss = 0.214859
I0428 19:41:25.050509 30618 solver.cpp:253]     Train net output #0: loss = 0.214858 (* 1 = 0.214858 loss)
I0428 19:41:25.050531 30618 sgd_solver.cpp:106] Iteration 29400, lr = 0.00357584
I0428 19:41:25.121924 30618 solver.cpp:237] Iteration 29500, loss = 0.263765
I0428 19:41:25.122007 30618 solver.cpp:253]     Train net output #0: loss = 0.263765 (* 1 = 0.263765 loss)
I0428 19:41:25.122028 30618 sgd_solver.cpp:106] Iteration 29500, lr = 0.00356905
I0428 19:41:25.198071 30618 solver.cpp:237] Iteration 29600, loss = 0.291557
I0428 19:41:25.198153 30618 solver.cpp:253]     Train net output #0: loss = 0.291557 (* 1 = 0.291557 loss)
I0428 19:41:25.198179 30618 sgd_solver.cpp:106] Iteration 29600, lr = 0.00356228
I0428 19:41:25.274215 30618 solver.cpp:237] Iteration 29700, loss = 0.266728
I0428 19:41:25.274294 30618 solver.cpp:253]     Train net output #0: loss = 0.266728 (* 1 = 0.266728 loss)
I0428 19:41:25.274313 30618 sgd_solver.cpp:106] Iteration 29700, lr = 0.00355555
I0428 19:41:25.331634 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:25.349860 30618 solver.cpp:237] Iteration 29800, loss = 0.2565
I0428 19:41:25.349942 30618 solver.cpp:253]     Train net output #0: loss = 0.256499 (* 1 = 0.256499 loss)
I0428 19:41:25.349962 30618 sgd_solver.cpp:106] Iteration 29800, lr = 0.00354885
I0428 19:41:25.425899 30618 solver.cpp:237] Iteration 29900, loss = 0.317742
I0428 19:41:25.425977 30618 solver.cpp:253]     Train net output #0: loss = 0.317741 (* 1 = 0.317741 loss)
I0428 19:41:25.425999 30618 sgd_solver.cpp:106] Iteration 29900, lr = 0.00354218
I0428 19:41:25.501255 30618 solver.cpp:459] Snapshotting to binary proto file examples/mlp/mlp_test_iter_30000.caffemodel
I0428 19:41:25.501638 30618 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mlp/mlp_test_iter_30000.solverstate
I0428 19:41:25.501787 30618 solver.cpp:341] Iteration 30000, Testing net (#0)
I0428 19:41:25.601070 30618 solver.cpp:409]     Test net output #0: accuracy = 0.9235
I0428 19:41:25.601173 30618 solver.cpp:409]     Test net output #1: loss = 0.274554 (* 1 = 0.274554 loss)
I0428 19:41:25.601572 30618 solver.cpp:237] Iteration 30000, loss = 0.214788
I0428 19:41:25.601613 30618 solver.cpp:253]     Train net output #0: loss = 0.214787 (* 1 = 0.214787 loss)
I0428 19:41:25.601639 30618 sgd_solver.cpp:106] Iteration 30000, lr = 0.00353553
I0428 19:41:25.725257 30618 solver.cpp:237] Iteration 30100, loss = 0.263644
I0428 19:41:25.725337 30618 solver.cpp:253]     Train net output #0: loss = 0.263644 (* 1 = 0.263644 loss)
I0428 19:41:25.725361 30618 sgd_solver.cpp:106] Iteration 30100, lr = 0.00352892
I0428 19:41:25.841425 30618 solver.cpp:237] Iteration 30200, loss = 0.291389
I0428 19:41:25.841516 30618 solver.cpp:253]     Train net output #0: loss = 0.291389 (* 1 = 0.291389 loss)
I0428 19:41:25.841550 30618 sgd_solver.cpp:106] Iteration 30200, lr = 0.00352233
I0428 19:41:25.922161 30618 solver.cpp:237] Iteration 30300, loss = 0.266609
I0428 19:41:25.922237 30618 solver.cpp:253]     Train net output #0: loss = 0.266608 (* 1 = 0.266608 loss)
I0428 19:41:25.922260 30618 sgd_solver.cpp:106] Iteration 30300, lr = 0.00351578
I0428 19:41:26.001574 30618 solver.cpp:237] Iteration 30400, loss = 0.256375
I0428 19:41:26.001657 30618 solver.cpp:253]     Train net output #0: loss = 0.256374 (* 1 = 0.256374 loss)
I0428 19:41:26.001678 30618 sgd_solver.cpp:106] Iteration 30400, lr = 0.00350925
I0428 19:41:26.078493 30618 solver.cpp:237] Iteration 30500, loss = 0.317619
I0428 19:41:26.078577 30618 solver.cpp:253]     Train net output #0: loss = 0.317619 (* 1 = 0.317619 loss)
I0428 19:41:26.078598 30618 sgd_solver.cpp:106] Iteration 30500, lr = 0.00350275
I0428 19:41:26.156462 30618 solver.cpp:237] Iteration 30600, loss = 0.21472
I0428 19:41:26.156544 30618 solver.cpp:253]     Train net output #0: loss = 0.21472 (* 1 = 0.21472 loss)
I0428 19:41:26.156565 30618 sgd_solver.cpp:106] Iteration 30600, lr = 0.00349627
I0428 19:41:26.233284 30618 solver.cpp:237] Iteration 30700, loss = 0.263527
I0428 19:41:26.233369 30618 solver.cpp:253]     Train net output #0: loss = 0.263527 (* 1 = 0.263527 loss)
I0428 19:41:26.233395 30618 sgd_solver.cpp:106] Iteration 30700, lr = 0.00348983
I0428 19:41:26.291074 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:26.311401 30618 solver.cpp:237] Iteration 30800, loss = 0.291228
I0428 19:41:26.311482 30618 solver.cpp:253]     Train net output #0: loss = 0.291228 (* 1 = 0.291228 loss)
I0428 19:41:26.311506 30618 sgd_solver.cpp:106] Iteration 30800, lr = 0.00348341
I0428 19:41:26.388608 30618 solver.cpp:237] Iteration 30900, loss = 0.266492
I0428 19:41:26.388691 30618 solver.cpp:253]     Train net output #0: loss = 0.266492 (* 1 = 0.266492 loss)
I0428 19:41:26.388715 30618 sgd_solver.cpp:106] Iteration 30900, lr = 0.00347702
I0428 19:41:26.463387 30618 solver.cpp:237] Iteration 31000, loss = 0.256254
I0428 19:41:26.463467 30618 solver.cpp:253]     Train net output #0: loss = 0.256254 (* 1 = 0.256254 loss)
I0428 19:41:26.463488 30618 sgd_solver.cpp:106] Iteration 31000, lr = 0.00347066
I0428 19:41:26.539465 30618 solver.cpp:237] Iteration 31100, loss = 0.317501
I0428 19:41:26.539546 30618 solver.cpp:253]     Train net output #0: loss = 0.317501 (* 1 = 0.317501 loss)
I0428 19:41:26.539569 30618 sgd_solver.cpp:106] Iteration 31100, lr = 0.00346433
I0428 19:41:26.615341 30618 solver.cpp:237] Iteration 31200, loss = 0.214655
I0428 19:41:26.615422 30618 solver.cpp:253]     Train net output #0: loss = 0.214655 (* 1 = 0.214655 loss)
I0428 19:41:26.615442 30618 sgd_solver.cpp:106] Iteration 31200, lr = 0.00345802
I0428 19:41:26.691279 30618 solver.cpp:237] Iteration 31300, loss = 0.263415
I0428 19:41:26.691361 30618 solver.cpp:253]     Train net output #0: loss = 0.263414 (* 1 = 0.263414 loss)
I0428 19:41:26.691393 30618 sgd_solver.cpp:106] Iteration 31300, lr = 0.00345174
I0428 19:41:26.767374 30618 solver.cpp:237] Iteration 31400, loss = 0.291074
I0428 19:41:26.767467 30618 solver.cpp:253]     Train net output #0: loss = 0.291073 (* 1 = 0.291073 loss)
I0428 19:41:26.767503 30618 sgd_solver.cpp:106] Iteration 31400, lr = 0.00344548
I0428 19:41:26.843453 30618 solver.cpp:237] Iteration 31500, loss = 0.266379
I0428 19:41:26.843539 30618 solver.cpp:253]     Train net output #0: loss = 0.266378 (* 1 = 0.266378 loss)
I0428 19:41:26.843565 30618 sgd_solver.cpp:106] Iteration 31500, lr = 0.00343925
I0428 19:41:26.920449 30618 solver.cpp:237] Iteration 31600, loss = 0.256138
I0428 19:41:26.920526 30618 solver.cpp:253]     Train net output #0: loss = 0.256137 (* 1 = 0.256137 loss)
I0428 19:41:26.920545 30618 sgd_solver.cpp:106] Iteration 31600, lr = 0.00343305
I0428 19:41:26.999961 30618 solver.cpp:237] Iteration 31700, loss = 0.317387
I0428 19:41:27.000043 30618 solver.cpp:253]     Train net output #0: loss = 0.317387 (* 1 = 0.317387 loss)
I0428 19:41:27.000063 30618 sgd_solver.cpp:106] Iteration 31700, lr = 0.00342687
I0428 19:41:27.060811 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:27.082659 30618 solver.cpp:237] Iteration 31800, loss = 0.214593
I0428 19:41:27.082736 30618 solver.cpp:253]     Train net output #0: loss = 0.214593 (* 1 = 0.214593 loss)
I0428 19:41:27.082762 30618 sgd_solver.cpp:106] Iteration 31800, lr = 0.00342072
I0428 19:41:27.159903 30618 solver.cpp:237] Iteration 31900, loss = 0.263306
I0428 19:41:27.159984 30618 solver.cpp:253]     Train net output #0: loss = 0.263306 (* 1 = 0.263306 loss)
I0428 19:41:27.160006 30618 sgd_solver.cpp:106] Iteration 31900, lr = 0.0034146
I0428 19:41:27.236008 30618 solver.cpp:237] Iteration 32000, loss = 0.290926
I0428 19:41:27.236085 30618 solver.cpp:253]     Train net output #0: loss = 0.290926 (* 1 = 0.290926 loss)
I0428 19:41:27.236106 30618 sgd_solver.cpp:106] Iteration 32000, lr = 0.0034085
I0428 19:41:27.312592 30618 solver.cpp:237] Iteration 32100, loss = 0.266268
I0428 19:41:27.312672 30618 solver.cpp:253]     Train net output #0: loss = 0.266268 (* 1 = 0.266268 loss)
I0428 19:41:27.312693 30618 sgd_solver.cpp:106] Iteration 32100, lr = 0.00340242
I0428 19:41:27.389503 30618 solver.cpp:237] Iteration 32200, loss = 0.256025
I0428 19:41:27.389590 30618 solver.cpp:253]     Train net output #0: loss = 0.256024 (* 1 = 0.256024 loss)
I0428 19:41:27.389617 30618 sgd_solver.cpp:106] Iteration 32200, lr = 0.00339637
I0428 19:41:27.466130 30618 solver.cpp:237] Iteration 32300, loss = 0.317276
I0428 19:41:27.466209 30618 solver.cpp:253]     Train net output #0: loss = 0.317276 (* 1 = 0.317276 loss)
I0428 19:41:27.466230 30618 sgd_solver.cpp:106] Iteration 32300, lr = 0.00339035
I0428 19:41:27.542640 30618 solver.cpp:237] Iteration 32400, loss = 0.214533
I0428 19:41:27.542722 30618 solver.cpp:253]     Train net output #0: loss = 0.214533 (* 1 = 0.214533 loss)
I0428 19:41:27.542744 30618 sgd_solver.cpp:106] Iteration 32400, lr = 0.00338435
I0428 19:41:27.619504 30618 solver.cpp:237] Iteration 32500, loss = 0.263202
I0428 19:41:27.619583 30618 solver.cpp:253]     Train net output #0: loss = 0.263202 (* 1 = 0.263202 loss)
I0428 19:41:27.619602 30618 sgd_solver.cpp:106] Iteration 32500, lr = 0.00337838
I0428 19:41:27.696411 30618 solver.cpp:237] Iteration 32600, loss = 0.290784
I0428 19:41:27.696493 30618 solver.cpp:253]     Train net output #0: loss = 0.290784 (* 1 = 0.290784 loss)
I0428 19:41:27.696514 30618 sgd_solver.cpp:106] Iteration 32600, lr = 0.00337243
I0428 19:41:27.772505 30618 solver.cpp:237] Iteration 32700, loss = 0.26616
I0428 19:41:27.772589 30618 solver.cpp:253]     Train net output #0: loss = 0.26616 (* 1 = 0.26616 loss)
I0428 19:41:27.772616 30618 sgd_solver.cpp:106] Iteration 32700, lr = 0.0033665
I0428 19:41:27.831035 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:27.848480 30618 solver.cpp:237] Iteration 32800, loss = 0.255915
I0428 19:41:27.848559 30618 solver.cpp:253]     Train net output #0: loss = 0.255915 (* 1 = 0.255915 loss)
I0428 19:41:27.848582 30618 sgd_solver.cpp:106] Iteration 32800, lr = 0.0033606
I0428 19:41:27.924469 30618 solver.cpp:237] Iteration 32900, loss = 0.317169
I0428 19:41:27.924571 30618 solver.cpp:253]     Train net output #0: loss = 0.317169 (* 1 = 0.317169 loss)
I0428 19:41:27.924615 30618 sgd_solver.cpp:106] Iteration 32900, lr = 0.00335473
I0428 19:41:28.000537 30618 solver.cpp:237] Iteration 33000, loss = 0.214475
I0428 19:41:28.000617 30618 solver.cpp:253]     Train net output #0: loss = 0.214475 (* 1 = 0.214475 loss)
I0428 19:41:28.000638 30618 sgd_solver.cpp:106] Iteration 33000, lr = 0.00334887
I0428 19:41:28.077004 30618 solver.cpp:237] Iteration 33100, loss = 0.263101
I0428 19:41:28.077085 30618 solver.cpp:253]     Train net output #0: loss = 0.263101 (* 1 = 0.263101 loss)
I0428 19:41:28.077106 30618 sgd_solver.cpp:106] Iteration 33100, lr = 0.00334304
I0428 19:41:28.154479 30618 solver.cpp:237] Iteration 33200, loss = 0.290648
I0428 19:41:28.154563 30618 solver.cpp:253]     Train net output #0: loss = 0.290648 (* 1 = 0.290648 loss)
I0428 19:41:28.154587 30618 sgd_solver.cpp:106] Iteration 33200, lr = 0.00333724
I0428 19:41:28.221842 30618 solver.cpp:237] Iteration 33300, loss = 0.266054
I0428 19:41:28.221923 30618 solver.cpp:253]     Train net output #0: loss = 0.266054 (* 1 = 0.266054 loss)
I0428 19:41:28.221946 30618 sgd_solver.cpp:106] Iteration 33300, lr = 0.00333146
I0428 19:41:28.299294 30618 solver.cpp:237] Iteration 33400, loss = 0.255809
I0428 19:41:28.299376 30618 solver.cpp:253]     Train net output #0: loss = 0.255809 (* 1 = 0.255809 loss)
I0428 19:41:28.299397 30618 sgd_solver.cpp:106] Iteration 33400, lr = 0.0033257
I0428 19:41:28.376781 30618 solver.cpp:237] Iteration 33500, loss = 0.317065
I0428 19:41:28.376863 30618 solver.cpp:253]     Train net output #0: loss = 0.317065 (* 1 = 0.317065 loss)
I0428 19:41:28.376884 30618 sgd_solver.cpp:106] Iteration 33500, lr = 0.00331996
I0428 19:41:28.454319 30618 solver.cpp:237] Iteration 33600, loss = 0.21442
I0428 19:41:28.454401 30618 solver.cpp:253]     Train net output #0: loss = 0.21442 (* 1 = 0.21442 loss)
I0428 19:41:28.454422 30618 sgd_solver.cpp:106] Iteration 33600, lr = 0.00331425
I0428 19:41:28.531471 30618 solver.cpp:237] Iteration 33700, loss = 0.263003
I0428 19:41:28.531549 30618 solver.cpp:253]     Train net output #0: loss = 0.263003 (* 1 = 0.263003 loss)
I0428 19:41:28.531570 30618 sgd_solver.cpp:106] Iteration 33700, lr = 0.00330856
I0428 19:41:28.591809 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:28.608835 30618 solver.cpp:237] Iteration 33800, loss = 0.290517
I0428 19:41:28.608927 30618 solver.cpp:253]     Train net output #0: loss = 0.290517 (* 1 = 0.290517 loss)
I0428 19:41:28.608952 30618 sgd_solver.cpp:106] Iteration 33800, lr = 0.00330289
I0428 19:41:28.685992 30618 solver.cpp:237] Iteration 33900, loss = 0.26595
I0428 19:41:28.686072 30618 solver.cpp:253]     Train net output #0: loss = 0.26595 (* 1 = 0.26595 loss)
I0428 19:41:28.686092 30618 sgd_solver.cpp:106] Iteration 33900, lr = 0.00329725
I0428 19:41:28.763556 30618 solver.cpp:237] Iteration 34000, loss = 0.255706
I0428 19:41:28.763638 30618 solver.cpp:253]     Train net output #0: loss = 0.255706 (* 1 = 0.255706 loss)
I0428 19:41:28.763669 30618 sgd_solver.cpp:106] Iteration 34000, lr = 0.00329163
I0428 19:41:28.840924 30618 solver.cpp:237] Iteration 34100, loss = 0.316964
I0428 19:41:28.841008 30618 solver.cpp:253]     Train net output #0: loss = 0.316964 (* 1 = 0.316964 loss)
I0428 19:41:28.841040 30618 sgd_solver.cpp:106] Iteration 34100, lr = 0.00328603
I0428 19:41:28.918830 30618 solver.cpp:237] Iteration 34200, loss = 0.214367
I0428 19:41:28.918911 30618 solver.cpp:253]     Train net output #0: loss = 0.214367 (* 1 = 0.214367 loss)
I0428 19:41:28.918936 30618 sgd_solver.cpp:106] Iteration 34200, lr = 0.00328045
I0428 19:41:28.994149 30618 solver.cpp:237] Iteration 34300, loss = 0.262909
I0428 19:41:28.994232 30618 solver.cpp:253]     Train net output #0: loss = 0.262909 (* 1 = 0.262909 loss)
I0428 19:41:28.994259 30618 sgd_solver.cpp:106] Iteration 34300, lr = 0.00327489
I0428 19:41:29.071868 30618 solver.cpp:237] Iteration 34400, loss = 0.290391
I0428 19:41:29.071948 30618 solver.cpp:253]     Train net output #0: loss = 0.290391 (* 1 = 0.290391 loss)
I0428 19:41:29.071993 30618 sgd_solver.cpp:106] Iteration 34400, lr = 0.00326936
I0428 19:41:29.152163 30618 solver.cpp:237] Iteration 34500, loss = 0.265849
I0428 19:41:29.152251 30618 solver.cpp:253]     Train net output #0: loss = 0.265849 (* 1 = 0.265849 loss)
I0428 19:41:29.152271 30618 sgd_solver.cpp:106] Iteration 34500, lr = 0.00326385
I0428 19:41:29.229656 30618 solver.cpp:237] Iteration 34600, loss = 0.255607
I0428 19:41:29.229743 30618 solver.cpp:253]     Train net output #0: loss = 0.255607 (* 1 = 0.255607 loss)
I0428 19:41:29.229764 30618 sgd_solver.cpp:106] Iteration 34600, lr = 0.00325836
I0428 19:41:29.307059 30618 solver.cpp:237] Iteration 34700, loss = 0.316866
I0428 19:41:29.307143 30618 solver.cpp:253]     Train net output #0: loss = 0.316866 (* 1 = 0.316866 loss)
I0428 19:41:29.307178 30618 sgd_solver.cpp:106] Iteration 34700, lr = 0.00325289
I0428 19:41:29.367449 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:29.384496 30618 solver.cpp:237] Iteration 34800, loss = 0.214315
I0428 19:41:29.384575 30618 solver.cpp:253]     Train net output #0: loss = 0.214315 (* 1 = 0.214315 loss)
I0428 19:41:29.384595 30618 sgd_solver.cpp:106] Iteration 34800, lr = 0.00324744
I0428 19:41:29.461552 30618 solver.cpp:237] Iteration 34900, loss = 0.262819
I0428 19:41:29.461632 30618 solver.cpp:253]     Train net output #0: loss = 0.262819 (* 1 = 0.262819 loss)
I0428 19:41:29.461652 30618 sgd_solver.cpp:106] Iteration 34900, lr = 0.00324202
I0428 19:41:29.538275 30618 solver.cpp:459] Snapshotting to binary proto file examples/mlp/mlp_test_iter_35000.caffemodel
I0428 19:41:29.538653 30618 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mlp/mlp_test_iter_35000.solverstate
I0428 19:41:29.538808 30618 solver.cpp:341] Iteration 35000, Testing net (#0)
I0428 19:41:29.646001 30618 solver.cpp:409]     Test net output #0: accuracy = 0.9223
I0428 19:41:29.646082 30618 solver.cpp:409]     Test net output #1: loss = 0.274335 (* 1 = 0.274335 loss)
I0428 19:41:29.646409 30618 solver.cpp:237] Iteration 35000, loss = 0.29027
I0428 19:41:29.646445 30618 solver.cpp:253]     Train net output #0: loss = 0.29027 (* 1 = 0.29027 loss)
I0428 19:41:29.646474 30618 sgd_solver.cpp:106] Iteration 35000, lr = 0.00323661
I0428 19:41:29.746026 30618 solver.cpp:237] Iteration 35100, loss = 0.265749
I0428 19:41:29.746112 30618 solver.cpp:253]     Train net output #0: loss = 0.265749 (* 1 = 0.265749 loss)
I0428 19:41:29.746140 30618 sgd_solver.cpp:106] Iteration 35100, lr = 0.00323123
I0428 19:41:29.826611 30618 solver.cpp:237] Iteration 35200, loss = 0.25551
I0428 19:41:29.826694 30618 solver.cpp:253]     Train net output #0: loss = 0.25551 (* 1 = 0.25551 loss)
I0428 19:41:29.826716 30618 sgd_solver.cpp:106] Iteration 35200, lr = 0.00322586
I0428 19:41:29.907482 30618 solver.cpp:237] Iteration 35300, loss = 0.31677
I0428 19:41:29.907567 30618 solver.cpp:253]     Train net output #0: loss = 0.31677 (* 1 = 0.31677 loss)
I0428 19:41:29.907588 30618 sgd_solver.cpp:106] Iteration 35300, lr = 0.00322052
I0428 19:41:29.993957 30618 solver.cpp:237] Iteration 35400, loss = 0.214266
I0428 19:41:29.994037 30618 solver.cpp:253]     Train net output #0: loss = 0.214266 (* 1 = 0.214266 loss)
I0428 19:41:29.994057 30618 sgd_solver.cpp:106] Iteration 35400, lr = 0.0032152
I0428 19:41:30.074851 30618 solver.cpp:237] Iteration 35500, loss = 0.262731
I0428 19:41:30.074928 30618 solver.cpp:253]     Train net output #0: loss = 0.262731 (* 1 = 0.262731 loss)
I0428 19:41:30.074947 30618 sgd_solver.cpp:106] Iteration 35500, lr = 0.0032099
I0428 19:41:30.153074 30618 solver.cpp:237] Iteration 35600, loss = 0.290154
I0428 19:41:30.153157 30618 solver.cpp:253]     Train net output #0: loss = 0.290153 (* 1 = 0.290153 loss)
I0428 19:41:30.153177 30618 sgd_solver.cpp:106] Iteration 35600, lr = 0.00320462
I0428 19:41:30.229423 30618 solver.cpp:237] Iteration 35700, loss = 0.265652
I0428 19:41:30.229501 30618 solver.cpp:253]     Train net output #0: loss = 0.265652 (* 1 = 0.265652 loss)
I0428 19:41:30.229547 30618 sgd_solver.cpp:106] Iteration 35700, lr = 0.00319936
I0428 19:41:30.239713 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:30.297365 30618 solver.cpp:237] Iteration 35800, loss = 0.255416
I0428 19:41:30.297451 30618 solver.cpp:253]     Train net output #0: loss = 0.255416 (* 1 = 0.255416 loss)
I0428 19:41:30.297484 30618 sgd_solver.cpp:106] Iteration 35800, lr = 0.00319412
I0428 19:41:30.375027 30618 solver.cpp:237] Iteration 35900, loss = 0.316678
I0428 19:41:30.375109 30618 solver.cpp:253]     Train net output #0: loss = 0.316678 (* 1 = 0.316678 loss)
I0428 19:41:30.375133 30618 sgd_solver.cpp:106] Iteration 35900, lr = 0.0031889
I0428 19:41:30.452819 30618 solver.cpp:237] Iteration 36000, loss = 0.214218
I0428 19:41:30.452908 30618 solver.cpp:253]     Train net output #0: loss = 0.214218 (* 1 = 0.214218 loss)
I0428 19:41:30.452929 30618 sgd_solver.cpp:106] Iteration 36000, lr = 0.0031837
I0428 19:41:30.522691 30618 solver.cpp:237] Iteration 36100, loss = 0.262646
I0428 19:41:30.522778 30618 solver.cpp:253]     Train net output #0: loss = 0.262646 (* 1 = 0.262646 loss)
I0428 19:41:30.522799 30618 sgd_solver.cpp:106] Iteration 36100, lr = 0.00317852
I0428 19:41:30.601467 30618 solver.cpp:237] Iteration 36200, loss = 0.290041
I0428 19:41:30.601549 30618 solver.cpp:253]     Train net output #0: loss = 0.290041 (* 1 = 0.290041 loss)
I0428 19:41:30.601572 30618 sgd_solver.cpp:106] Iteration 36200, lr = 0.00317335
I0428 19:41:30.682366 30618 solver.cpp:237] Iteration 36300, loss = 0.265557
I0428 19:41:30.682448 30618 solver.cpp:253]     Train net output #0: loss = 0.265557 (* 1 = 0.265557 loss)
I0428 19:41:30.682474 30618 sgd_solver.cpp:106] Iteration 36300, lr = 0.00316821
I0428 19:41:30.761518 30618 solver.cpp:237] Iteration 36400, loss = 0.255325
I0428 19:41:30.761600 30618 solver.cpp:253]     Train net output #0: loss = 0.255325 (* 1 = 0.255325 loss)
I0428 19:41:30.761620 30618 sgd_solver.cpp:106] Iteration 36400, lr = 0.00316309
I0428 19:41:30.840389 30618 solver.cpp:237] Iteration 36500, loss = 0.316588
I0428 19:41:30.840472 30618 solver.cpp:253]     Train net output #0: loss = 0.316588 (* 1 = 0.316588 loss)
I0428 19:41:30.840492 30618 sgd_solver.cpp:106] Iteration 36500, lr = 0.00315799
I0428 19:41:30.918993 30618 solver.cpp:237] Iteration 36600, loss = 0.214171
I0428 19:41:30.919071 30618 solver.cpp:253]     Train net output #0: loss = 0.214171 (* 1 = 0.214171 loss)
I0428 19:41:30.919090 30618 sgd_solver.cpp:106] Iteration 36600, lr = 0.0031529
I0428 19:41:30.997022 30618 solver.cpp:237] Iteration 36700, loss = 0.262564
I0428 19:41:30.997107 30618 solver.cpp:253]     Train net output #0: loss = 0.262564 (* 1 = 0.262564 loss)
I0428 19:41:30.997131 30618 sgd_solver.cpp:106] Iteration 36700, lr = 0.00314784
I0428 19:41:31.012235 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:31.075884 30618 solver.cpp:237] Iteration 36800, loss = 0.289933
I0428 19:41:31.075964 30618 solver.cpp:253]     Train net output #0: loss = 0.289933 (* 1 = 0.289933 loss)
I0428 19:41:31.075983 30618 sgd_solver.cpp:106] Iteration 36800, lr = 0.00314279
I0428 19:41:31.153036 30618 solver.cpp:237] Iteration 36900, loss = 0.265463
I0428 19:41:31.153115 30618 solver.cpp:253]     Train net output #0: loss = 0.265463 (* 1 = 0.265463 loss)
I0428 19:41:31.153147 30618 sgd_solver.cpp:106] Iteration 36900, lr = 0.00313776
I0428 19:41:31.229992 30618 solver.cpp:237] Iteration 37000, loss = 0.255237
I0428 19:41:31.230078 30618 solver.cpp:253]     Train net output #0: loss = 0.255237 (* 1 = 0.255237 loss)
I0428 19:41:31.230104 30618 sgd_solver.cpp:106] Iteration 37000, lr = 0.00313276
I0428 19:41:31.307611 30618 solver.cpp:237] Iteration 37100, loss = 0.3165
I0428 19:41:31.307698 30618 solver.cpp:253]     Train net output #0: loss = 0.3165 (* 1 = 0.3165 loss)
I0428 19:41:31.307723 30618 sgd_solver.cpp:106] Iteration 37100, lr = 0.00312777
I0428 19:41:31.378111 30618 solver.cpp:237] Iteration 37200, loss = 0.214126
I0428 19:41:31.378192 30618 solver.cpp:253]     Train net output #0: loss = 0.214126 (* 1 = 0.214126 loss)
I0428 19:41:31.378234 30618 sgd_solver.cpp:106] Iteration 37200, lr = 0.0031228
I0428 19:41:31.459481 30618 solver.cpp:237] Iteration 37300, loss = 0.262485
I0428 19:41:31.459559 30618 solver.cpp:253]     Train net output #0: loss = 0.262485 (* 1 = 0.262485 loss)
I0428 19:41:31.459594 30618 sgd_solver.cpp:106] Iteration 37300, lr = 0.00311784
I0428 19:41:31.540935 30618 solver.cpp:237] Iteration 37400, loss = 0.289829
I0428 19:41:31.541013 30618 solver.cpp:253]     Train net output #0: loss = 0.289829 (* 1 = 0.289829 loss)
I0428 19:41:31.541038 30618 sgd_solver.cpp:106] Iteration 37400, lr = 0.00311291
I0428 19:41:31.621764 30618 solver.cpp:237] Iteration 37500, loss = 0.265371
I0428 19:41:31.621973 30618 solver.cpp:253]     Train net output #0: loss = 0.265371 (* 1 = 0.265371 loss)
I0428 19:41:31.621999 30618 sgd_solver.cpp:106] Iteration 37500, lr = 0.00310799
I0428 19:41:31.702301 30618 solver.cpp:237] Iteration 37600, loss = 0.25515
I0428 19:41:31.702384 30618 solver.cpp:253]     Train net output #0: loss = 0.25515 (* 1 = 0.25515 loss)
I0428 19:41:31.702405 30618 sgd_solver.cpp:106] Iteration 37600, lr = 0.00310309
I0428 19:41:31.776084 30618 solver.cpp:237] Iteration 37700, loss = 0.316415
I0428 19:41:31.776165 30618 solver.cpp:253]     Train net output #0: loss = 0.316415 (* 1 = 0.316415 loss)
I0428 19:41:31.776186 30618 sgd_solver.cpp:106] Iteration 37700, lr = 0.00309821
I0428 19:41:31.794703 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:31.854516 30618 solver.cpp:237] Iteration 37800, loss = 0.214083
I0428 19:41:31.854598 30618 solver.cpp:253]     Train net output #0: loss = 0.214083 (* 1 = 0.214083 loss)
I0428 19:41:31.854617 30618 sgd_solver.cpp:106] Iteration 37800, lr = 0.00309335
I0428 19:41:31.934206 30618 solver.cpp:237] Iteration 37900, loss = 0.262408
I0428 19:41:31.934284 30618 solver.cpp:253]     Train net output #0: loss = 0.262408 (* 1 = 0.262408 loss)
I0428 19:41:31.934303 30618 sgd_solver.cpp:106] Iteration 37900, lr = 0.00308851
I0428 19:41:32.012480 30618 solver.cpp:237] Iteration 38000, loss = 0.289728
I0428 19:41:32.012558 30618 solver.cpp:253]     Train net output #0: loss = 0.289728 (* 1 = 0.289728 loss)
I0428 19:41:32.012590 30618 sgd_solver.cpp:106] Iteration 38000, lr = 0.00308368
I0428 19:41:32.092849 30618 solver.cpp:237] Iteration 38100, loss = 0.265281
I0428 19:41:32.092941 30618 solver.cpp:253]     Train net output #0: loss = 0.265281 (* 1 = 0.265281 loss)
I0428 19:41:32.092965 30618 sgd_solver.cpp:106] Iteration 38100, lr = 0.00307887
I0428 19:41:32.170006 30618 solver.cpp:237] Iteration 38200, loss = 0.255067
I0428 19:41:32.170086 30618 solver.cpp:253]     Train net output #0: loss = 0.255067 (* 1 = 0.255067 loss)
I0428 19:41:32.170107 30618 sgd_solver.cpp:106] Iteration 38200, lr = 0.00307408
I0428 19:41:32.247370 30618 solver.cpp:237] Iteration 38300, loss = 0.316331
I0428 19:41:32.247452 30618 solver.cpp:253]     Train net output #0: loss = 0.316331 (* 1 = 0.316331 loss)
I0428 19:41:32.247474 30618 sgd_solver.cpp:106] Iteration 38300, lr = 0.0030693
I0428 19:41:32.318037 30618 solver.cpp:237] Iteration 38400, loss = 0.214041
I0428 19:41:32.318121 30618 solver.cpp:253]     Train net output #0: loss = 0.214041 (* 1 = 0.214041 loss)
I0428 19:41:32.318150 30618 sgd_solver.cpp:106] Iteration 38400, lr = 0.00306454
I0428 19:41:32.395876 30618 solver.cpp:237] Iteration 38500, loss = 0.262334
I0428 19:41:32.395961 30618 solver.cpp:253]     Train net output #0: loss = 0.262334 (* 1 = 0.262334 loss)
I0428 19:41:32.395988 30618 sgd_solver.cpp:106] Iteration 38500, lr = 0.0030598
I0428 19:41:32.473675 30618 solver.cpp:237] Iteration 38600, loss = 0.289631
I0428 19:41:32.473755 30618 solver.cpp:253]     Train net output #0: loss = 0.289631 (* 1 = 0.289631 loss)
I0428 19:41:32.473776 30618 sgd_solver.cpp:106] Iteration 38600, lr = 0.00305508
I0428 19:41:32.550207 30618 solver.cpp:237] Iteration 38700, loss = 0.265192
I0428 19:41:32.550288 30618 solver.cpp:253]     Train net output #0: loss = 0.265192 (* 1 = 0.265192 loss)
I0428 19:41:32.550329 30618 sgd_solver.cpp:106] Iteration 38700, lr = 0.00305038
I0428 19:41:32.569319 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:32.619916 30618 solver.cpp:237] Iteration 38800, loss = 0.254985
I0428 19:41:32.619997 30618 solver.cpp:253]     Train net output #0: loss = 0.254985 (* 1 = 0.254985 loss)
I0428 19:41:32.620018 30618 sgd_solver.cpp:106] Iteration 38800, lr = 0.00304569
I0428 19:41:32.688036 30618 solver.cpp:237] Iteration 38900, loss = 0.31625
I0428 19:41:32.688113 30618 solver.cpp:253]     Train net output #0: loss = 0.31625 (* 1 = 0.31625 loss)
I0428 19:41:32.688134 30618 sgd_solver.cpp:106] Iteration 38900, lr = 0.00304101
I0428 19:41:32.754812 30618 solver.cpp:237] Iteration 39000, loss = 0.214
I0428 19:41:32.754914 30618 solver.cpp:253]     Train net output #0: loss = 0.214 (* 1 = 0.214 loss)
I0428 19:41:32.754935 30618 sgd_solver.cpp:106] Iteration 39000, lr = 0.00303636
I0428 19:41:32.832594 30618 solver.cpp:237] Iteration 39100, loss = 0.262262
I0428 19:41:32.832675 30618 solver.cpp:253]     Train net output #0: loss = 0.262262 (* 1 = 0.262262 loss)
I0428 19:41:32.832696 30618 sgd_solver.cpp:106] Iteration 39100, lr = 0.00303172
I0428 19:41:32.911236 30618 solver.cpp:237] Iteration 39200, loss = 0.289537
I0428 19:41:32.911316 30618 solver.cpp:253]     Train net output #0: loss = 0.289537 (* 1 = 0.289537 loss)
I0428 19:41:32.911335 30618 sgd_solver.cpp:106] Iteration 39200, lr = 0.0030271
I0428 19:41:32.989508 30618 solver.cpp:237] Iteration 39300, loss = 0.265105
I0428 19:41:32.989594 30618 solver.cpp:253]     Train net output #0: loss = 0.265105 (* 1 = 0.265105 loss)
I0428 19:41:32.989622 30618 sgd_solver.cpp:106] Iteration 39300, lr = 0.00302249
I0428 19:41:33.068114 30618 solver.cpp:237] Iteration 39400, loss = 0.254906
I0428 19:41:33.068194 30618 solver.cpp:253]     Train net output #0: loss = 0.254906 (* 1 = 0.254906 loss)
I0428 19:41:33.068215 30618 sgd_solver.cpp:106] Iteration 39400, lr = 0.0030179
I0428 19:41:33.146646 30618 solver.cpp:237] Iteration 39500, loss = 0.316171
I0428 19:41:33.146726 30618 solver.cpp:253]     Train net output #0: loss = 0.316171 (* 1 = 0.316171 loss)
I0428 19:41:33.146761 30618 sgd_solver.cpp:106] Iteration 39500, lr = 0.00301333
I0428 19:41:33.224630 30618 solver.cpp:237] Iteration 39600, loss = 0.21396
I0428 19:41:33.224712 30618 solver.cpp:253]     Train net output #0: loss = 0.21396 (* 1 = 0.21396 loss)
I0428 19:41:33.224733 30618 sgd_solver.cpp:106] Iteration 39600, lr = 0.00300877
I0428 19:41:33.303362 30618 solver.cpp:237] Iteration 39700, loss = 0.262192
I0428 19:41:33.303445 30618 solver.cpp:253]     Train net output #0: loss = 0.262192 (* 1 = 0.262192 loss)
I0428 19:41:33.303467 30618 sgd_solver.cpp:106] Iteration 39700, lr = 0.00300423
I0428 19:41:33.323864 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:33.381410 30618 solver.cpp:237] Iteration 39800, loss = 0.289446
I0428 19:41:33.381486 30618 solver.cpp:253]     Train net output #0: loss = 0.289446 (* 1 = 0.289446 loss)
I0428 19:41:33.381506 30618 sgd_solver.cpp:106] Iteration 39800, lr = 0.0029997
I0428 19:41:33.462318 30618 solver.cpp:237] Iteration 39900, loss = 0.265019
I0428 19:41:33.462401 30618 solver.cpp:253]     Train net output #0: loss = 0.265019 (* 1 = 0.265019 loss)
I0428 19:41:33.462424 30618 sgd_solver.cpp:106] Iteration 39900, lr = 0.00299519
I0428 19:41:33.539659 30618 solver.cpp:459] Snapshotting to binary proto file examples/mlp/mlp_test_iter_40000.caffemodel
I0428 19:41:33.540042 30618 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mlp/mlp_test_iter_40000.solverstate
I0428 19:41:33.540195 30618 solver.cpp:341] Iteration 40000, Testing net (#0)
I0428 19:41:33.647063 30618 solver.cpp:409]     Test net output #0: accuracy = 0.9227
I0428 19:41:33.647150 30618 solver.cpp:409]     Test net output #1: loss = 0.273548 (* 1 = 0.273548 loss)
I0428 19:41:33.647816 30618 solver.cpp:237] Iteration 40000, loss = 0.254829
I0428 19:41:33.647855 30618 solver.cpp:253]     Train net output #0: loss = 0.254828 (* 1 = 0.254828 loss)
I0428 19:41:33.647886 30618 sgd_solver.cpp:106] Iteration 40000, lr = 0.0029907
I0428 19:41:33.753011 30618 solver.cpp:237] Iteration 40100, loss = 0.316094
I0428 19:41:33.753093 30618 solver.cpp:253]     Train net output #0: loss = 0.316094 (* 1 = 0.316094 loss)
I0428 19:41:33.753113 30618 sgd_solver.cpp:106] Iteration 40100, lr = 0.00298622
I0428 19:41:33.832214 30618 solver.cpp:237] Iteration 40200, loss = 0.213922
I0428 19:41:33.832294 30618 solver.cpp:253]     Train net output #0: loss = 0.213922 (* 1 = 0.213922 loss)
I0428 19:41:33.832315 30618 sgd_solver.cpp:106] Iteration 40200, lr = 0.00298176
I0428 19:41:33.911084 30618 solver.cpp:237] Iteration 40300, loss = 0.262125
I0428 19:41:33.911169 30618 solver.cpp:253]     Train net output #0: loss = 0.262125 (* 1 = 0.262125 loss)
I0428 19:41:33.911212 30618 sgd_solver.cpp:106] Iteration 40300, lr = 0.00297731
I0428 19:41:33.992012 30618 solver.cpp:237] Iteration 40400, loss = 0.289359
I0428 19:41:33.992094 30618 solver.cpp:253]     Train net output #0: loss = 0.289358 (* 1 = 0.289358 loss)
I0428 19:41:33.992116 30618 sgd_solver.cpp:106] Iteration 40400, lr = 0.00297288
I0428 19:41:34.070870 30618 solver.cpp:237] Iteration 40500, loss = 0.264935
I0428 19:41:34.070953 30618 solver.cpp:253]     Train net output #0: loss = 0.264935 (* 1 = 0.264935 loss)
I0428 19:41:34.070977 30618 sgd_solver.cpp:106] Iteration 40500, lr = 0.00296846
I0428 19:41:34.149744 30618 solver.cpp:237] Iteration 40600, loss = 0.254753
I0428 19:41:34.149822 30618 solver.cpp:253]     Train net output #0: loss = 0.254753 (* 1 = 0.254753 loss)
I0428 19:41:34.149843 30618 sgd_solver.cpp:106] Iteration 40600, lr = 0.00296406
I0428 19:41:34.179559 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:34.229523 30618 solver.cpp:237] Iteration 40700, loss = 0.316019
I0428 19:41:34.229604 30618 solver.cpp:253]     Train net output #0: loss = 0.316019 (* 1 = 0.316019 loss)
I0428 19:41:34.229626 30618 sgd_solver.cpp:106] Iteration 40700, lr = 0.00295968
I0428 19:41:34.307055 30618 solver.cpp:237] Iteration 40800, loss = 0.213884
I0428 19:41:34.307137 30618 solver.cpp:253]     Train net output #0: loss = 0.213884 (* 1 = 0.213884 loss)
I0428 19:41:34.307158 30618 sgd_solver.cpp:106] Iteration 40800, lr = 0.0029553
I0428 19:41:34.384146 30618 solver.cpp:237] Iteration 40900, loss = 0.26206
I0428 19:41:34.384219 30618 solver.cpp:253]     Train net output #0: loss = 0.262059 (* 1 = 0.262059 loss)
I0428 19:41:34.384243 30618 sgd_solver.cpp:106] Iteration 40900, lr = 0.00295095
I0428 19:41:34.461454 30618 solver.cpp:237] Iteration 41000, loss = 0.289274
I0428 19:41:34.461539 30618 solver.cpp:253]     Train net output #0: loss = 0.289273 (* 1 = 0.289273 loss)
I0428 19:41:34.461566 30618 sgd_solver.cpp:106] Iteration 41000, lr = 0.00294661
I0428 19:41:34.537665 30618 solver.cpp:237] Iteration 41100, loss = 0.264852
I0428 19:41:34.537747 30618 solver.cpp:253]     Train net output #0: loss = 0.264852 (* 1 = 0.264852 loss)
I0428 19:41:34.537766 30618 sgd_solver.cpp:106] Iteration 41100, lr = 0.00294228
I0428 19:41:34.625224 30618 solver.cpp:237] Iteration 41200, loss = 0.25468
I0428 19:41:34.625304 30618 solver.cpp:253]     Train net output #0: loss = 0.25468 (* 1 = 0.25468 loss)
I0428 19:41:34.625329 30618 sgd_solver.cpp:106] Iteration 41200, lr = 0.00293797
I0428 19:41:34.739560 30618 solver.cpp:237] Iteration 41300, loss = 0.315945
I0428 19:41:34.739641 30618 solver.cpp:253]     Train net output #0: loss = 0.315945 (* 1 = 0.315945 loss)
I0428 19:41:34.739681 30618 sgd_solver.cpp:106] Iteration 41300, lr = 0.00293367
I0428 19:41:34.811445 30618 solver.cpp:237] Iteration 41400, loss = 0.213848
I0428 19:41:34.811525 30618 solver.cpp:253]     Train net output #0: loss = 0.213848 (* 1 = 0.213848 loss)
I0428 19:41:34.811547 30618 sgd_solver.cpp:106] Iteration 41400, lr = 0.00292939
I0428 19:41:34.882694 30618 solver.cpp:237] Iteration 41500, loss = 0.261996
I0428 19:41:34.882777 30618 solver.cpp:253]     Train net output #0: loss = 0.261996 (* 1 = 0.261996 loss)
I0428 19:41:34.882812 30618 sgd_solver.cpp:106] Iteration 41500, lr = 0.00292513
I0428 19:41:34.953346 30618 solver.cpp:237] Iteration 41600, loss = 0.289191
I0428 19:41:34.953423 30618 solver.cpp:253]     Train net output #0: loss = 0.289191 (* 1 = 0.289191 loss)
I0428 19:41:34.953443 30618 sgd_solver.cpp:106] Iteration 41600, lr = 0.00292087
I0428 19:41:34.999460 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:35.025928 30618 solver.cpp:237] Iteration 41700, loss = 0.264771
I0428 19:41:35.025996 30618 solver.cpp:253]     Train net output #0: loss = 0.26477 (* 1 = 0.26477 loss)
I0428 19:41:35.026027 30618 sgd_solver.cpp:106] Iteration 41700, lr = 0.00291664
I0428 19:41:35.096534 30618 solver.cpp:237] Iteration 41800, loss = 0.254609
I0428 19:41:35.096635 30618 solver.cpp:253]     Train net output #0: loss = 0.254608 (* 1 = 0.254608 loss)
I0428 19:41:35.096657 30618 sgd_solver.cpp:106] Iteration 41800, lr = 0.00291241
I0428 19:41:35.164245 30618 solver.cpp:237] Iteration 41900, loss = 0.315874
I0428 19:41:35.164327 30618 solver.cpp:253]     Train net output #0: loss = 0.315873 (* 1 = 0.315873 loss)
I0428 19:41:35.164348 30618 sgd_solver.cpp:106] Iteration 41900, lr = 0.0029082
I0428 19:41:35.231768 30618 solver.cpp:237] Iteration 42000, loss = 0.213813
I0428 19:41:35.231848 30618 solver.cpp:253]     Train net output #0: loss = 0.213813 (* 1 = 0.213813 loss)
I0428 19:41:35.231869 30618 sgd_solver.cpp:106] Iteration 42000, lr = 0.00290401
I0428 19:41:35.299306 30618 solver.cpp:237] Iteration 42100, loss = 0.261935
I0428 19:41:35.299386 30618 solver.cpp:253]     Train net output #0: loss = 0.261935 (* 1 = 0.261935 loss)
I0428 19:41:35.299407 30618 sgd_solver.cpp:106] Iteration 42100, lr = 0.00289982
I0428 19:41:35.366924 30618 solver.cpp:237] Iteration 42200, loss = 0.289112
I0428 19:41:35.367013 30618 solver.cpp:253]     Train net output #0: loss = 0.289111 (* 1 = 0.289111 loss)
I0428 19:41:35.367041 30618 sgd_solver.cpp:106] Iteration 42200, lr = 0.00289566
I0428 19:41:35.434664 30618 solver.cpp:237] Iteration 42300, loss = 0.26469
I0428 19:41:35.434747 30618 solver.cpp:253]     Train net output #0: loss = 0.26469 (* 1 = 0.26469 loss)
I0428 19:41:35.434767 30618 sgd_solver.cpp:106] Iteration 42300, lr = 0.0028915
I0428 19:41:35.502311 30618 solver.cpp:237] Iteration 42400, loss = 0.254539
I0428 19:41:35.502391 30618 solver.cpp:253]     Train net output #0: loss = 0.254539 (* 1 = 0.254539 loss)
I0428 19:41:35.502416 30618 sgd_solver.cpp:106] Iteration 42400, lr = 0.00288736
I0428 19:41:35.569952 30618 solver.cpp:237] Iteration 42500, loss = 0.315804
I0428 19:41:35.570036 30618 solver.cpp:253]     Train net output #0: loss = 0.315803 (* 1 = 0.315803 loss)
I0428 19:41:35.570065 30618 sgd_solver.cpp:106] Iteration 42500, lr = 0.00288324
I0428 19:41:35.637593 30618 solver.cpp:237] Iteration 42600, loss = 0.213779
I0428 19:41:35.637676 30618 solver.cpp:253]     Train net output #0: loss = 0.213778 (* 1 = 0.213778 loss)
I0428 19:41:35.637696 30618 sgd_solver.cpp:106] Iteration 42600, lr = 0.00287913
I0428 19:41:35.680222 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:35.705188 30618 solver.cpp:237] Iteration 42700, loss = 0.261876
I0428 19:41:35.705272 30618 solver.cpp:253]     Train net output #0: loss = 0.261875 (* 1 = 0.261875 loss)
I0428 19:41:35.705297 30618 sgd_solver.cpp:106] Iteration 42700, lr = 0.00287503
I0428 19:41:35.772740 30618 solver.cpp:237] Iteration 42800, loss = 0.289034
I0428 19:41:35.772822 30618 solver.cpp:253]     Train net output #0: loss = 0.289034 (* 1 = 0.289034 loss)
I0428 19:41:35.772842 30618 sgd_solver.cpp:106] Iteration 42800, lr = 0.00287094
I0428 19:41:35.840188 30618 solver.cpp:237] Iteration 42900, loss = 0.264611
I0428 19:41:35.840270 30618 solver.cpp:253]     Train net output #0: loss = 0.264611 (* 1 = 0.264611 loss)
I0428 19:41:35.840291 30618 sgd_solver.cpp:106] Iteration 42900, lr = 0.00286687
I0428 19:41:35.907883 30618 solver.cpp:237] Iteration 43000, loss = 0.254471
I0428 19:41:35.907968 30618 solver.cpp:253]     Train net output #0: loss = 0.25447 (* 1 = 0.25447 loss)
I0428 19:41:35.908010 30618 sgd_solver.cpp:106] Iteration 43000, lr = 0.00286281
I0428 19:41:35.975455 30618 solver.cpp:237] Iteration 43100, loss = 0.315735
I0428 19:41:35.975549 30618 solver.cpp:253]     Train net output #0: loss = 0.315734 (* 1 = 0.315734 loss)
I0428 19:41:35.975577 30618 sgd_solver.cpp:106] Iteration 43100, lr = 0.00285877
I0428 19:41:36.043175 30618 solver.cpp:237] Iteration 43200, loss = 0.213745
I0428 19:41:36.043254 30618 solver.cpp:253]     Train net output #0: loss = 0.213744 (* 1 = 0.213744 loss)
I0428 19:41:36.043287 30618 sgd_solver.cpp:106] Iteration 43200, lr = 0.00285474
I0428 19:41:36.110646 30618 solver.cpp:237] Iteration 43300, loss = 0.261818
I0428 19:41:36.110720 30618 solver.cpp:253]     Train net output #0: loss = 0.261817 (* 1 = 0.261817 loss)
I0428 19:41:36.110772 30618 sgd_solver.cpp:106] Iteration 43300, lr = 0.00285072
I0428 19:41:36.178102 30618 solver.cpp:237] Iteration 43400, loss = 0.28896
I0428 19:41:36.178184 30618 solver.cpp:253]     Train net output #0: loss = 0.288959 (* 1 = 0.288959 loss)
I0428 19:41:36.178205 30618 sgd_solver.cpp:106] Iteration 43400, lr = 0.00284672
I0428 19:41:36.245795 30618 solver.cpp:237] Iteration 43500, loss = 0.264533
I0428 19:41:36.245875 30618 solver.cpp:253]     Train net output #0: loss = 0.264533 (* 1 = 0.264533 loss)
I0428 19:41:36.245896 30618 sgd_solver.cpp:106] Iteration 43500, lr = 0.00284272
I0428 19:41:36.313426 30618 solver.cpp:237] Iteration 43600, loss = 0.254405
I0428 19:41:36.313505 30618 solver.cpp:253]     Train net output #0: loss = 0.254404 (* 1 = 0.254404 loss)
I0428 19:41:36.313524 30618 sgd_solver.cpp:106] Iteration 43600, lr = 0.00283875
I0428 19:41:36.356286 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:36.381120 30618 solver.cpp:237] Iteration 43700, loss = 0.315668
I0428 19:41:36.381204 30618 solver.cpp:253]     Train net output #0: loss = 0.315667 (* 1 = 0.315667 loss)
I0428 19:41:36.381228 30618 sgd_solver.cpp:106] Iteration 43700, lr = 0.00283478
I0428 19:41:36.448930 30618 solver.cpp:237] Iteration 43800, loss = 0.213712
I0428 19:41:36.449012 30618 solver.cpp:253]     Train net output #0: loss = 0.213712 (* 1 = 0.213712 loss)
I0428 19:41:36.449034 30618 sgd_solver.cpp:106] Iteration 43800, lr = 0.00283083
I0428 19:41:36.516496 30618 solver.cpp:237] Iteration 43900, loss = 0.261762
I0428 19:41:36.516577 30618 solver.cpp:253]     Train net output #0: loss = 0.261761 (* 1 = 0.261761 loss)
I0428 19:41:36.516602 30618 sgd_solver.cpp:106] Iteration 43900, lr = 0.00282689
I0428 19:41:36.584082 30618 solver.cpp:237] Iteration 44000, loss = 0.288887
I0428 19:41:36.584161 30618 solver.cpp:253]     Train net output #0: loss = 0.288886 (* 1 = 0.288886 loss)
I0428 19:41:36.584180 30618 sgd_solver.cpp:106] Iteration 44000, lr = 0.00282296
I0428 19:41:36.651896 30618 solver.cpp:237] Iteration 44100, loss = 0.264456
I0428 19:41:36.651981 30618 solver.cpp:253]     Train net output #0: loss = 0.264456 (* 1 = 0.264456 loss)
I0428 19:41:36.652006 30618 sgd_solver.cpp:106] Iteration 44100, lr = 0.00281905
I0428 19:41:36.719516 30618 solver.cpp:237] Iteration 44200, loss = 0.25434
I0428 19:41:36.719601 30618 solver.cpp:253]     Train net output #0: loss = 0.254339 (* 1 = 0.254339 loss)
I0428 19:41:36.719629 30618 sgd_solver.cpp:106] Iteration 44200, lr = 0.00281514
I0428 19:41:36.787111 30618 solver.cpp:237] Iteration 44300, loss = 0.315602
I0428 19:41:36.787192 30618 solver.cpp:253]     Train net output #0: loss = 0.315602 (* 1 = 0.315602 loss)
I0428 19:41:36.787214 30618 sgd_solver.cpp:106] Iteration 44300, lr = 0.00281125
I0428 19:41:36.854830 30618 solver.cpp:237] Iteration 44400, loss = 0.21368
I0428 19:41:36.854912 30618 solver.cpp:253]     Train net output #0: loss = 0.21368 (* 1 = 0.21368 loss)
I0428 19:41:36.854933 30618 sgd_solver.cpp:106] Iteration 44400, lr = 0.00280738
I0428 19:41:36.922394 30618 solver.cpp:237] Iteration 44500, loss = 0.261707
I0428 19:41:36.922478 30618 solver.cpp:253]     Train net output #0: loss = 0.261707 (* 1 = 0.261707 loss)
I0428 19:41:36.922519 30618 sgd_solver.cpp:106] Iteration 44500, lr = 0.00280351
I0428 19:41:36.989812 30618 solver.cpp:237] Iteration 44600, loss = 0.288816
I0428 19:41:36.989898 30618 solver.cpp:253]     Train net output #0: loss = 0.288815 (* 1 = 0.288815 loss)
I0428 19:41:36.989919 30618 sgd_solver.cpp:106] Iteration 44600, lr = 0.00279966
I0428 19:41:37.032560 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:37.057656 30618 solver.cpp:237] Iteration 44700, loss = 0.264381
I0428 19:41:37.057732 30618 solver.cpp:253]     Train net output #0: loss = 0.26438 (* 1 = 0.26438 loss)
I0428 19:41:37.057751 30618 sgd_solver.cpp:106] Iteration 44700, lr = 0.00279582
I0428 19:41:37.124982 30618 solver.cpp:237] Iteration 44800, loss = 0.254276
I0428 19:41:37.125082 30618 solver.cpp:253]     Train net output #0: loss = 0.254276 (* 1 = 0.254276 loss)
I0428 19:41:37.125103 30618 sgd_solver.cpp:106] Iteration 44800, lr = 0.00279199
I0428 19:41:37.192370 30618 solver.cpp:237] Iteration 44900, loss = 0.315538
I0428 19:41:37.192456 30618 solver.cpp:253]     Train net output #0: loss = 0.315537 (* 1 = 0.315537 loss)
I0428 19:41:37.192476 30618 sgd_solver.cpp:106] Iteration 44900, lr = 0.00278818
I0428 19:41:37.259333 30618 solver.cpp:459] Snapshotting to binary proto file examples/mlp/mlp_test_iter_45000.caffemodel
I0428 19:41:37.259711 30618 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mlp/mlp_test_iter_45000.solverstate
I0428 19:41:37.259865 30618 solver.cpp:341] Iteration 45000, Testing net (#0)
I0428 19:41:37.380782 30618 solver.cpp:409]     Test net output #0: accuracy = 0.9239
I0428 19:41:37.380831 30618 solver.cpp:409]     Test net output #1: loss = 0.273504 (* 1 = 0.273504 loss)
I0428 19:41:37.381315 30618 solver.cpp:237] Iteration 45000, loss = 0.213649
I0428 19:41:37.381355 30618 solver.cpp:253]     Train net output #0: loss = 0.213648 (* 1 = 0.213648 loss)
I0428 19:41:37.381387 30618 sgd_solver.cpp:106] Iteration 45000, lr = 0.00278438
I0428 19:41:37.477260 30618 solver.cpp:237] Iteration 45100, loss = 0.261654
I0428 19:41:37.477305 30618 solver.cpp:253]     Train net output #0: loss = 0.261654 (* 1 = 0.261654 loss)
I0428 19:41:37.477314 30618 sgd_solver.cpp:106] Iteration 45100, lr = 0.00278059
I0428 19:41:37.568575 30618 solver.cpp:237] Iteration 45200, loss = 0.288747
I0428 19:41:37.568656 30618 solver.cpp:253]     Train net output #0: loss = 0.288747 (* 1 = 0.288747 loss)
I0428 19:41:37.568671 30618 sgd_solver.cpp:106] Iteration 45200, lr = 0.00277681
I0428 19:41:37.655864 30618 solver.cpp:237] Iteration 45300, loss = 0.264306
I0428 19:41:37.655946 30618 solver.cpp:253]     Train net output #0: loss = 0.264305 (* 1 = 0.264305 loss)
I0428 19:41:37.655969 30618 sgd_solver.cpp:106] Iteration 45300, lr = 0.00277304
I0428 19:41:37.731741 30618 solver.cpp:237] Iteration 45400, loss = 0.254214
I0428 19:41:37.731823 30618 solver.cpp:253]     Train net output #0: loss = 0.254214 (* 1 = 0.254214 loss)
I0428 19:41:37.731848 30618 sgd_solver.cpp:106] Iteration 45400, lr = 0.00276929
I0428 19:41:37.810576 30618 solver.cpp:237] Iteration 45500, loss = 0.315475
I0428 19:41:37.810657 30618 solver.cpp:253]     Train net output #0: loss = 0.315474 (* 1 = 0.315474 loss)
I0428 19:41:37.810683 30618 sgd_solver.cpp:106] Iteration 45500, lr = 0.00276554
I0428 19:41:37.871610 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:37.889441 30618 solver.cpp:237] Iteration 45600, loss = 0.213618
I0428 19:41:37.889521 30618 solver.cpp:253]     Train net output #0: loss = 0.213618 (* 1 = 0.213618 loss)
I0428 19:41:37.889536 30618 sgd_solver.cpp:106] Iteration 45600, lr = 0.00276181
I0428 19:41:37.967984 30618 solver.cpp:237] Iteration 45700, loss = 0.261603
I0428 19:41:37.968065 30618 solver.cpp:253]     Train net output #0: loss = 0.261602 (* 1 = 0.261602 loss)
I0428 19:41:37.968086 30618 sgd_solver.cpp:106] Iteration 45700, lr = 0.00275809
I0428 19:41:38.046463 30618 solver.cpp:237] Iteration 45800, loss = 0.288681
I0428 19:41:38.046543 30618 solver.cpp:253]     Train net output #0: loss = 0.28868 (* 1 = 0.28868 loss)
I0428 19:41:38.046566 30618 sgd_solver.cpp:106] Iteration 45800, lr = 0.00275438
I0428 19:41:38.125069 30618 solver.cpp:237] Iteration 45900, loss = 0.264232
I0428 19:41:38.125191 30618 solver.cpp:253]     Train net output #0: loss = 0.264232 (* 1 = 0.264232 loss)
I0428 19:41:38.125231 30618 sgd_solver.cpp:106] Iteration 45900, lr = 0.00275069
I0428 19:41:38.204257 30618 solver.cpp:237] Iteration 46000, loss = 0.254154
I0428 19:41:38.204378 30618 solver.cpp:253]     Train net output #0: loss = 0.254153 (* 1 = 0.254153 loss)
I0428 19:41:38.204416 30618 sgd_solver.cpp:106] Iteration 46000, lr = 0.002747
I0428 19:41:38.282673 30618 solver.cpp:237] Iteration 46100, loss = 0.315413
I0428 19:41:38.282799 30618 solver.cpp:253]     Train net output #0: loss = 0.315413 (* 1 = 0.315413 loss)
I0428 19:41:38.282904 30618 sgd_solver.cpp:106] Iteration 46100, lr = 0.00274333
I0428 19:41:38.361337 30618 solver.cpp:237] Iteration 46200, loss = 0.213589
I0428 19:41:38.361461 30618 solver.cpp:253]     Train net output #0: loss = 0.213588 (* 1 = 0.213588 loss)
I0428 19:41:38.361512 30618 sgd_solver.cpp:106] Iteration 46200, lr = 0.00273967
I0428 19:41:38.448437 30618 solver.cpp:237] Iteration 46300, loss = 0.261553
I0428 19:41:38.448573 30618 solver.cpp:253]     Train net output #0: loss = 0.261553 (* 1 = 0.261553 loss)
I0428 19:41:38.448616 30618 sgd_solver.cpp:106] Iteration 46300, lr = 0.00273602
I0428 19:41:38.524571 30618 solver.cpp:237] Iteration 46400, loss = 0.288616
I0428 19:41:38.524710 30618 solver.cpp:253]     Train net output #0: loss = 0.288615 (* 1 = 0.288615 loss)
I0428 19:41:38.524766 30618 sgd_solver.cpp:106] Iteration 46400, lr = 0.00273238
I0428 19:41:38.601111 30618 solver.cpp:237] Iteration 46500, loss = 0.264159
I0428 19:41:38.601253 30618 solver.cpp:253]     Train net output #0: loss = 0.264159 (* 1 = 0.264159 loss)
I0428 19:41:38.601296 30618 sgd_solver.cpp:106] Iteration 46500, lr = 0.00272875
I0428 19:41:38.663003 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:38.677362 30618 solver.cpp:237] Iteration 46600, loss = 0.254095
I0428 19:41:38.677503 30618 solver.cpp:253]     Train net output #0: loss = 0.254094 (* 1 = 0.254094 loss)
I0428 19:41:38.677547 30618 sgd_solver.cpp:106] Iteration 46600, lr = 0.00272513
I0428 19:41:38.753643 30618 solver.cpp:237] Iteration 46700, loss = 0.315353
I0428 19:41:38.753774 30618 solver.cpp:253]     Train net output #0: loss = 0.315352 (* 1 = 0.315352 loss)
I0428 19:41:38.753836 30618 sgd_solver.cpp:106] Iteration 46700, lr = 0.00272153
I0428 19:41:38.830097 30618 solver.cpp:237] Iteration 46800, loss = 0.213559
I0428 19:41:38.830237 30618 solver.cpp:253]     Train net output #0: loss = 0.213559 (* 1 = 0.213559 loss)
I0428 19:41:38.830281 30618 sgd_solver.cpp:106] Iteration 46800, lr = 0.00271793
I0428 19:41:38.906394 30618 solver.cpp:237] Iteration 46900, loss = 0.261505
I0428 19:41:38.906529 30618 solver.cpp:253]     Train net output #0: loss = 0.261504 (* 1 = 0.261504 loss)
I0428 19:41:38.906576 30618 sgd_solver.cpp:106] Iteration 46900, lr = 0.00271435
I0428 19:41:38.982769 30618 solver.cpp:237] Iteration 47000, loss = 0.288553
I0428 19:41:38.982914 30618 solver.cpp:253]     Train net output #0: loss = 0.288552 (* 1 = 0.288552 loss)
I0428 19:41:38.982964 30618 sgd_solver.cpp:106] Iteration 47000, lr = 0.00271078
I0428 19:41:39.059128 30618 solver.cpp:237] Iteration 47100, loss = 0.264088
I0428 19:41:39.059255 30618 solver.cpp:253]     Train net output #0: loss = 0.264087 (* 1 = 0.264087 loss)
I0428 19:41:39.059295 30618 sgd_solver.cpp:106] Iteration 47100, lr = 0.00270722
I0428 19:41:39.135519 30618 solver.cpp:237] Iteration 47200, loss = 0.254037
I0428 19:41:39.135671 30618 solver.cpp:253]     Train net output #0: loss = 0.254037 (* 1 = 0.254037 loss)
I0428 19:41:39.135718 30618 sgd_solver.cpp:106] Iteration 47200, lr = 0.00270367
I0428 19:41:39.210109 30618 solver.cpp:237] Iteration 47300, loss = 0.315293
I0428 19:41:39.210197 30618 solver.cpp:253]     Train net output #0: loss = 0.315293 (* 1 = 0.315293 loss)
I0428 19:41:39.210232 30618 sgd_solver.cpp:106] Iteration 47300, lr = 0.00270013
I0428 19:41:39.281484 30618 solver.cpp:237] Iteration 47400, loss = 0.213531
I0428 19:41:39.281572 30618 solver.cpp:253]     Train net output #0: loss = 0.213531 (* 1 = 0.213531 loss)
I0428 19:41:39.281599 30618 sgd_solver.cpp:106] Iteration 47400, lr = 0.0026966
I0428 19:41:39.352331 30618 solver.cpp:237] Iteration 47500, loss = 0.261458
I0428 19:41:39.352416 30618 solver.cpp:253]     Train net output #0: loss = 0.261457 (* 1 = 0.261457 loss)
I0428 19:41:39.352447 30618 sgd_solver.cpp:106] Iteration 47500, lr = 0.00269308
I0428 19:41:39.414572 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:39.420610 30618 solver.cpp:237] Iteration 47600, loss = 0.288492
I0428 19:41:39.420718 30618 solver.cpp:253]     Train net output #0: loss = 0.288491 (* 1 = 0.288491 loss)
I0428 19:41:39.420744 30618 sgd_solver.cpp:106] Iteration 47600, lr = 0.00268957
I0428 19:41:39.488204 30618 solver.cpp:237] Iteration 47700, loss = 0.264017
I0428 19:41:39.488294 30618 solver.cpp:253]     Train net output #0: loss = 0.264016 (* 1 = 0.264016 loss)
I0428 19:41:39.488323 30618 sgd_solver.cpp:106] Iteration 47700, lr = 0.00268607
I0428 19:41:39.555930 30618 solver.cpp:237] Iteration 47800, loss = 0.253981
I0428 19:41:39.556010 30618 solver.cpp:253]     Train net output #0: loss = 0.25398 (* 1 = 0.25398 loss)
I0428 19:41:39.556030 30618 sgd_solver.cpp:106] Iteration 47800, lr = 0.00268259
I0428 19:41:39.623669 30618 solver.cpp:237] Iteration 47900, loss = 0.315236
I0428 19:41:39.623751 30618 solver.cpp:253]     Train net output #0: loss = 0.315235 (* 1 = 0.315235 loss)
I0428 19:41:39.623782 30618 sgd_solver.cpp:106] Iteration 47900, lr = 0.00267911
I0428 19:41:39.691318 30618 solver.cpp:237] Iteration 48000, loss = 0.213503
I0428 19:41:39.691401 30618 solver.cpp:253]     Train net output #0: loss = 0.213503 (* 1 = 0.213503 loss)
I0428 19:41:39.691429 30618 sgd_solver.cpp:106] Iteration 48000, lr = 0.00267565
I0428 19:41:39.758978 30618 solver.cpp:237] Iteration 48100, loss = 0.261412
I0428 19:41:39.759060 30618 solver.cpp:253]     Train net output #0: loss = 0.261412 (* 1 = 0.261412 loss)
I0428 19:41:39.759083 30618 sgd_solver.cpp:106] Iteration 48100, lr = 0.00267219
I0428 19:41:39.826725 30618 solver.cpp:237] Iteration 48200, loss = 0.288432
I0428 19:41:39.826804 30618 solver.cpp:253]     Train net output #0: loss = 0.288431 (* 1 = 0.288431 loss)
I0428 19:41:39.826822 30618 sgd_solver.cpp:106] Iteration 48200, lr = 0.00266875
I0428 19:41:39.894371 30618 solver.cpp:237] Iteration 48300, loss = 0.263947
I0428 19:41:39.894450 30618 solver.cpp:253]     Train net output #0: loss = 0.263947 (* 1 = 0.263947 loss)
I0428 19:41:39.894469 30618 sgd_solver.cpp:106] Iteration 48300, lr = 0.00266532
I0428 19:41:39.962116 30618 solver.cpp:237] Iteration 48400, loss = 0.253925
I0428 19:41:39.962198 30618 solver.cpp:253]     Train net output #0: loss = 0.253925 (* 1 = 0.253925 loss)
I0428 19:41:39.962223 30618 sgd_solver.cpp:106] Iteration 48400, lr = 0.00266189
I0428 19:41:40.029917 30618 solver.cpp:237] Iteration 48500, loss = 0.315179
I0428 19:41:40.029994 30618 solver.cpp:253]     Train net output #0: loss = 0.315178 (* 1 = 0.315178 loss)
I0428 19:41:40.030014 30618 sgd_solver.cpp:106] Iteration 48500, lr = 0.00265848
I0428 19:41:40.091538 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:40.097573 30618 solver.cpp:237] Iteration 48600, loss = 0.213476
I0428 19:41:40.097652 30618 solver.cpp:253]     Train net output #0: loss = 0.213476 (* 1 = 0.213476 loss)
I0428 19:41:40.097681 30618 sgd_solver.cpp:106] Iteration 48600, lr = 0.00265507
I0428 19:41:40.165232 30618 solver.cpp:237] Iteration 48700, loss = 0.261368
I0428 19:41:40.165314 30618 solver.cpp:253]     Train net output #0: loss = 0.261367 (* 1 = 0.261367 loss)
I0428 19:41:40.165349 30618 sgd_solver.cpp:106] Iteration 48700, lr = 0.00265168
I0428 19:41:40.232936 30618 solver.cpp:237] Iteration 48800, loss = 0.288374
I0428 19:41:40.233017 30618 solver.cpp:253]     Train net output #0: loss = 0.288373 (* 1 = 0.288373 loss)
I0428 19:41:40.233050 30618 sgd_solver.cpp:106] Iteration 48800, lr = 0.0026483
I0428 19:41:40.300576 30618 solver.cpp:237] Iteration 48900, loss = 0.263878
I0428 19:41:40.300657 30618 solver.cpp:253]     Train net output #0: loss = 0.263878 (* 1 = 0.263878 loss)
I0428 19:41:40.300674 30618 sgd_solver.cpp:106] Iteration 48900, lr = 0.00264493
I0428 19:41:40.368178 30618 solver.cpp:237] Iteration 49000, loss = 0.253871
I0428 19:41:40.368257 30618 solver.cpp:253]     Train net output #0: loss = 0.253871 (* 1 = 0.253871 loss)
I0428 19:41:40.368278 30618 sgd_solver.cpp:106] Iteration 49000, lr = 0.00264156
I0428 19:41:40.435917 30618 solver.cpp:237] Iteration 49100, loss = 0.315123
I0428 19:41:40.436002 30618 solver.cpp:253]     Train net output #0: loss = 0.315122 (* 1 = 0.315122 loss)
I0428 19:41:40.436051 30618 sgd_solver.cpp:106] Iteration 49100, lr = 0.00263821
I0428 19:41:40.503674 30618 solver.cpp:237] Iteration 49200, loss = 0.21345
I0428 19:41:40.503756 30618 solver.cpp:253]     Train net output #0: loss = 0.213449 (* 1 = 0.213449 loss)
I0428 19:41:40.503782 30618 sgd_solver.cpp:106] Iteration 49200, lr = 0.00263487
I0428 19:41:40.571439 30618 solver.cpp:237] Iteration 49300, loss = 0.261324
I0428 19:41:40.571519 30618 solver.cpp:253]     Train net output #0: loss = 0.261324 (* 1 = 0.261324 loss)
I0428 19:41:40.571539 30618 sgd_solver.cpp:106] Iteration 49300, lr = 0.00263153
I0428 19:41:40.639226 30618 solver.cpp:237] Iteration 49400, loss = 0.288317
I0428 19:41:40.639305 30618 solver.cpp:253]     Train net output #0: loss = 0.288316 (* 1 = 0.288316 loss)
I0428 19:41:40.639327 30618 sgd_solver.cpp:106] Iteration 49400, lr = 0.00262821
I0428 19:41:40.706920 30618 solver.cpp:237] Iteration 49500, loss = 0.26381
I0428 19:41:40.707001 30618 solver.cpp:253]     Train net output #0: loss = 0.26381 (* 1 = 0.26381 loss)
I0428 19:41:40.707022 30618 sgd_solver.cpp:106] Iteration 49500, lr = 0.0026249
I0428 19:41:40.768496 30618 blocking_queue.cpp:50] Data layer prefetch queue empty
I0428 19:41:40.774555 30618 solver.cpp:237] Iteration 49600, loss = 0.253818
I0428 19:41:40.774633 30618 solver.cpp:253]     Train net output #0: loss = 0.253818 (* 1 = 0.253818 loss)
I0428 19:41:40.774654 30618 sgd_solver.cpp:106] Iteration 49600, lr = 0.00262159
I0428 19:41:40.842411 30618 solver.cpp:237] Iteration 49700, loss = 0.315068
I0428 19:41:40.842497 30618 solver.cpp:253]     Train net output #0: loss = 0.315068 (* 1 = 0.315068 loss)
I0428 19:41:40.842524 30618 sgd_solver.cpp:106] Iteration 49700, lr = 0.0026183
I0428 19:41:40.909967 30618 solver.cpp:237] Iteration 49800, loss = 0.213423
I0428 19:41:40.910046 30618 solver.cpp:253]     Train net output #0: loss = 0.213423 (* 1 = 0.213423 loss)
I0428 19:41:40.910065 30618 sgd_solver.cpp:106] Iteration 49800, lr = 0.00261501
I0428 19:41:40.977439 30618 solver.cpp:237] Iteration 49900, loss = 0.261282
I0428 19:41:40.977519 30618 solver.cpp:253]     Train net output #0: loss = 0.261282 (* 1 = 0.261282 loss)
I0428 19:41:40.977555 30618 sgd_solver.cpp:106] Iteration 49900, lr = 0.00261174
I0428 19:41:41.044378 30618 solver.cpp:459] Snapshotting to binary proto file examples/mlp/mlp_test_iter_50000.caffemodel
I0428 19:41:41.044761 30618 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mlp/mlp_test_iter_50000.solverstate
I0428 19:41:41.045166 30618 solver.cpp:321] Iteration 50000, loss = 0.288261
I0428 19:41:41.045203 30618 solver.cpp:341] Iteration 50000, Testing net (#0)
I0428 19:41:41.139859 30618 solver.cpp:409]     Test net output #0: accuracy = 0.9227
I0428 19:41:41.139940 30618 solver.cpp:409]     Test net output #1: loss = 0.273426 (* 1 = 0.273426 loss)
I0428 19:41:41.139957 30618 solver.cpp:326] Optimization Done.
I0428 19:41:41.139984 30618 caffe.cpp:215] Optimization Done.
